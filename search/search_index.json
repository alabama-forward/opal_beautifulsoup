{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OPAL - Oppositional Positions in ALabama","text":"<p>Welcome to OPAL's documentation! OPAL is a web scraping tool that extracts content from websites like Alabama news sites and court records.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udcf0 Multiple News Sources - Parse articles from 1819news.com and Alabama Daily News</li> <li>\u2696\ufe0f Court Records - Extract data from Alabama Appeals Court Public Portal</li> <li>\ud83d\udd27 Extensible Architecture - Easy to add new parsers</li> <li>\ud83d\udcca Structured Output - Clean JSON format for analysis</li> <li>\ud83d\ude80 CLI Tool - Simple command-line interface</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>```bash</p>"},{"location":"#install-opal","title":"Install OPAL","text":"<p>pip install -e .</p>"},{"location":"#scrape-news-articles","title":"Scrape news articles","text":"<p>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5</p>"},{"location":"#scrape-court-cases","title":"Scrape court cases","text":"<p>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court</p>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li>Getting Started - Installation and setup</li> <li>User Guide - How to use OPAL</li> <li>Developer Guide - Extend Opal with new parsers</li> </ul>"},{"location":"#built-by-alabama-forward","title":"Built by Alabama Forward","text":"<p>This project was created by Gabriel Cab\u00e1n Cubero, Data Director at Alabama Forward.</p>"},{"location":"configurable_court_extractor_design/","title":"Configurable Court Extractor Design","text":""},{"location":"configurable_court_extractor_design/#problem-statement","title":"Problem Statement","text":"<p>The current <code>extract_all_court_cases.py</code> has hardcoded search parameters in the URL, making it inflexible for different search criteria. Users cannot easily change: - Date ranges - Case number filters - Case title filters - Whether to include/exclude closed cases</p>"},{"location":"configurable_court_extractor_design/#solution-overview","title":"Solution Overview","text":"<p>I designed a configurable court extractor that separates URL construction from data extraction, allowing users to specify search parameters via command line arguments or function parameters.</p>"},{"location":"configurable_court_extractor_design/#architecture","title":"Architecture","text":""},{"location":"configurable_court_extractor_design/#1-courtsearchbuilder-class","title":"1. CourtSearchBuilder Class","text":"<p>Purpose: Encapsulates the complex URL building logic for Alabama Appeals Court searches.</p> <p>Why I designed it this way: - Separation of Concerns: URL building is separate from data extraction - Maintainability: Changes to URL structure only affect one class - Reusability: Can be used by different scripts or tools - Readability: Clear methods for each search parameter</p> <pre><code>class CourtSearchBuilder:\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n        self.court_id = \"68f021c4-6a44-4735-9a76-5360b2e8af13\"\n        self.reset_params()\n</code></pre>"},{"location":"configurable_court_extractor_design/#2-key-methods-explained","title":"2. Key Methods Explained","text":""},{"location":"configurable_court_extractor_design/#set_date_range","title":"<code>set_date_range()</code>","text":"<p>Purpose: Handle different date range options Design rationale: - Supports both predefined periods (<code>-1y</code>, <code>-6m</code>) and custom date ranges - Automatically converts dates to the portal's expected format (<code>*2f</code> encoding) - Provides sensible defaults</p>"},{"location":"configurable_court_extractor_design/#build_criteria_string","title":"<code>build_criteria_string()</code>","text":"<p>Purpose: Construct the complex URL-encoded criteria parameter Design rationale: - Handles the intricate URL encoding required by the portal - Builds the nested parameter structure programmatically - Reduces human error in URL construction</p>"},{"location":"configurable_court_extractor_design/#build_url","title":"<code>build_url()</code>","text":"<p>Purpose: Create complete search URLs with pagination Design rationale: - Updates page numbers dynamically - Maintains other search parameters across pages - Returns ready-to-use URLs</p>"},{"location":"configurable_court_extractor_design/#configuration-options","title":"Configuration Options","text":""},{"location":"configurable_court_extractor_design/#court-selection","title":"Court Selection","text":"<pre><code># Available courts\ncourts = {\n    'civil': 'Alabama Civil Court of Appeals',\n    'criminal': 'Alabama Court of Criminal Appeals', \n    'supreme': 'Alabama Supreme Court'\n}\n\n# Select court\nsearch_builder.set_court('civil')  # or 'criminal', 'supreme'\n</code></pre>"},{"location":"configurable_court_extractor_design/#case-number-formats","title":"Case Number Formats","text":"<pre><code># Open-ended search\nsearch_builder.set_case_number_filter('2024-001')\n\n# Court-specific formats\nsearch_builder.set_case_number_filter('CL-2024-0001')  # Civil Appeals\nsearch_builder.set_case_number_filter('CR-2024-0001')  # Criminal Appeals  \nsearch_builder.set_case_number_filter('SC-2024-0001')  # Supreme Court\n</code></pre>"},{"location":"configurable_court_extractor_design/#case-categories","title":"Case Categories","text":"<pre><code># For Civil Appeals and Criminal Appeals\ncategories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n\n# For Supreme Court (includes additional option)\nsupreme_categories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n\n# Set category\nsearch_builder.set_case_category('Appeal')\n</code></pre>"},{"location":"configurable_court_extractor_design/#date-filters","title":"Date Filters","text":"<pre><code># Predefined periods (matching portal options)\nsearch_builder.set_date_range(period='7d')   # Last 7 days\nsearch_builder.set_date_range(period='1m')   # Last month\nsearch_builder.set_date_range(period='3m')   # Last 3 months\nsearch_builder.set_date_range(period='6m')   # Last 6 months\nsearch_builder.set_date_range(period='1y')   # Last year\n\n# Custom date range\nsearch_builder.set_date_range('2024-01-01', '2024-12-31', 'custom')\n</code></pre>"},{"location":"configurable_court_extractor_design/#case-title-and-status-filters","title":"Case Title and Status Filters","text":"<pre><code># Filter by case title (partial match)\nsearch_builder.set_case_title_filter('Smith v Jones')\n\n# Exclude closed cases\nsearch_builder.set_exclude_closed(True)\n</code></pre>"},{"location":"configurable_court_extractor_design/#command-line-interface","title":"Command Line Interface","text":"<p>Why I included CLI arguments: - User-friendly: No need to modify code for different searches - Scriptable: Can be integrated into automated workflows - Documented: Built-in help shows all options</p>"},{"location":"configurable_court_extractor_design/#usage-examples","title":"Usage Examples","text":""},{"location":"configurable_court_extractor_design/#option-1-use-built-in-search-parameters-recommended","title":"Option 1: Use Built-in Search Parameters (Recommended)","text":"<pre><code># Extract all cases from last year (default from all courts)\npython configurable_court_extractor.py\n\n# Extract cases from Alabama Supreme Court only\npython configurable_court_extractor.py --court supreme\n\n# Extract cases from last 7 days from Criminal Appeals\npython configurable_court_extractor.py --court criminal --date-period 7d\n\n# Extract Appeal cases from Civil Court\npython configurable_court_extractor.py --court civil --case-category Appeal\n\n# Extract cases with custom date range from Supreme Court\npython configurable_court_extractor.py --court supreme --date-period custom --start-date 2024-01-01 --end-date 2024-06-30\n\n# Filter by specific case number format\npython configurable_court_extractor.py --court civil --case-number \"CL-2024-\"\n\n# Filter by case title in Criminal Appeals\npython configurable_court_extractor.py --court criminal --case-title \"State v\"\n\n# Exclude closed cases from Supreme Court\npython configurable_court_extractor.py --court supreme --exclude-closed\n\n# Extract Certified Questions from Supreme Court (unique to Supreme Court)\npython configurable_court_extractor.py --court supreme --case-category \"Certified Question\"\n\n# Comprehensive search with multiple filters\npython configurable_court_extractor.py --court civil --case-category Appeal --date-period 3m --exclude-closed --output-prefix \"civil_appeals_q1\"\n</code></pre>"},{"location":"configurable_court_extractor_design/#option-2-use-pre-built-url-with-embedded-search-terms","title":"Option 2: Use Pre-built URL with Embedded Search Terms","text":"<p>\u26a0\ufe0f  WARNING: Custom URLs are temporary and session-based. They may stop working when the website session expires.</p> <pre><code># Use your existing URL with search terms already embedded\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29\"\n\n# Use custom URL with limited pages and custom output prefix\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\" --max-pages 5 --output-prefix \"my_custom_search\"\n\n# Any URL from the portal search interface works\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=YOUR_CUSTOM_SEARCH_CRITERIA\"\n</code></pre>"},{"location":"configurable_court_extractor_design/#hybrid-approach","title":"Hybrid Approach","text":"<pre><code># You can also programmatically call the function with a custom URL\nfrom configurable_court_extractor import extract_court_cases_with_params\n\n# Use your existing URL\nyour_url = \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\nresult = extract_court_cases_with_params(custom_url=your_url, max_pages=10)\n</code></pre>"},{"location":"configurable_court_extractor_design/#dynamic-court-id-discovery","title":"Dynamic Court ID Discovery","text":""},{"location":"configurable_court_extractor_design/#the-problem-with-dynamic-ids","title":"The Problem with Dynamic IDs","text":"<p>Modern web applications often generate session-specific or dynamic identifiers that change between visits. The Alabama Appeals Court portal appears to use dynamic court IDs that are assigned during the user's session rather than being static, predictable values.</p>"},{"location":"configurable_court_extractor_design/#solution-approach","title":"Solution Approach","text":"<p>Option 1: Automatic Discovery (Recommended) The <code>discover_court_ids()</code> method navigates to the court's search interface and programmatically extracts the current court IDs by:</p> <ol> <li>Loading the search page - Navigates to the main case search interface</li> <li>Inspecting form elements - Locates the court selection dropdown or form elements</li> <li>Extracting ID mappings - Parses the HTML to find court names and their corresponding dynamic IDs</li> <li>Caching for session - Stores the discovered IDs for the duration of the session</li> </ol> <p>Option 2: Manual Discovery If automatic discovery fails, users can:</p> <ol> <li>Inspect browser network traffic - Use browser developer tools to monitor the search requests</li> <li>Extract court ID from URL - Copy a working search URL and extract the court ID parameter</li> <li>Set manually - Use <code>set_court_id_manually()</code> to override the discovered ID</li> </ol> <p>Option 3: URL Bypass (Fallback) When court ID discovery completely fails, users can:</p> <ol> <li>Use browser to build URL - Manually configure search on the website</li> <li>Copy complete URL - Get the full URL with embedded parameters</li> <li>Use --url option - Pass the pre-built URL directly, bypassing all parameter building</li> </ol>"},{"location":"configurable_court_extractor_design/#implementation-benefits","title":"Implementation Benefits","text":"<ol> <li>Resilient to changes - Automatically adapts to new court ID schemes</li> <li>Fallback options - Multiple strategies when automatic discovery fails</li> <li>User-friendly - Handles complexity behind the scenes</li> <li>Transparent - Shows discovered IDs to user for verification</li> </ol>"},{"location":"configurable_court_extractor_design/#usage-examples-with-dynamic-ids","title":"Usage Examples with Dynamic IDs","text":"<pre><code># Let the system discover court IDs automatically\npython configurable_court_extractor.py --court civil --date-period 1m\n\n# If discovery fails, fall back to custom URL\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n\n# For debugging: manually set a court ID\nsearch_builder = CourtSearchBuilder()\nsearch_builder.set_court_id_manually('civil', 'discovered-session-id-12345')\n</code></pre>"},{"location":"configurable_court_extractor_design/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"configurable_court_extractor_design/#url-encoding-strategy","title":"URL Encoding Strategy","text":"<p>The Alabama Appeals Court portal uses a complex nested URL structure: <pre><code>?criteria=~%28advanced~false~courtID~%27{court_id}~page~%28...%29~sort~%28...%29~case~%28...%29%29\n</code></pre></p> <p>My approach: 1. Build parameters as nested dictionaries 2. Convert to the portal's specific encoding format 3. Handle special characters and escaping automatically</p>"},{"location":"configurable_court_extractor_design/#error-handling","title":"Error Handling","text":"<p>Graceful degradation: - If total page count can't be determined, process incrementally - Continue processing if individual pages fail - Provide detailed error messages with stack traces</p>"},{"location":"configurable_court_extractor_design/#performance-considerations","title":"Performance Considerations","text":"<p>Rate limiting:  - Configurable delays between requests - Respectful of server resources</p> <p>Memory efficiency: - Process pages incrementally - Don't load all data into memory at once</p> <p>Progress reporting: - Real-time feedback on processing status - Clear indication of completion</p>"},{"location":"configurable_court_extractor_design/#advantages-over-current-implementation","title":"Advantages Over Current Implementation","text":""},{"location":"configurable_court_extractor_design/#1-flexibility","title":"1. Flexibility","text":"<ul> <li>Before: Fixed search parameters in hardcoded URL</li> <li>After: Configurable search criteria via parameters OR custom URLs</li> </ul>"},{"location":"configurable_court_extractor_design/#2-maintainability","title":"2. Maintainability","text":"<ul> <li>Before: URL changes require code modification</li> <li>After: URL structure centralized in builder class with dynamic discovery</li> </ul>"},{"location":"configurable_court_extractor_design/#3-usability","title":"3. Usability","text":"<ul> <li>Before: Developers need to understand complex URL structure</li> <li>After: Simple method calls and CLI arguments</li> </ul>"},{"location":"configurable_court_extractor_design/#4-reusability","title":"4. Reusability","text":"<ul> <li>Before: Single-purpose script</li> <li>After: Reusable components for different use cases</li> </ul>"},{"location":"configurable_court_extractor_design/#5-documentation","title":"5. Documentation","text":"<ul> <li>Before: Search parameters hidden in URL</li> <li>After: Clear parameter documentation and examples</li> </ul>"},{"location":"configurable_court_extractor_design/#6-resilience-to-changes","title":"6. Resilience to Changes","text":"<ul> <li>Before: Hardcoded court IDs break when website changes</li> <li>After: Automatic discovery adapts to dynamic court ID schemes</li> </ul>"},{"location":"configurable_court_extractor_design/#7-multiple-fallback-options","title":"7. Multiple Fallback Options","text":"<ul> <li>Before: Script fails completely if URL structure changes</li> <li>After: Automatic discovery \u2192 manual discovery \u2192 custom URL bypass</li> </ul>"},{"location":"configurable_court_extractor_design/#integration-with-existing-code","title":"Integration with Existing Code","text":"<p>The new extractor can coexist with the current implementation: - Uses the same <code>ParserAppealsAL</code> class - Produces the same JSON/CSV output format - Follows the same error handling patterns</p>"},{"location":"configurable_court_extractor_design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"configurable_court_extractor_design/#additional-search-parameters","title":"Additional Search Parameters","text":"<ul> <li>Court type selection</li> <li>Attorney name filters</li> <li>Case status filters</li> <li>Judge name filters</li> </ul>"},{"location":"configurable_court_extractor_design/#advanced-features","title":"Advanced Features","text":"<ul> <li>Save/load search configurations</li> <li>Scheduled extractions</li> <li>Differential updates (only new cases)</li> <li>Export to additional formats (Excel, XML)</li> </ul>"},{"location":"configurable_court_extractor_design/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Parallel page processing</li> <li>Caching of search results</li> <li>Resume interrupted extractions</li> </ul>"},{"location":"configurable_court_extractor_design/#code-structure","title":"Code Structure","text":"<pre><code>configurable_court_extractor.py\n\u251c\u2500\u2500 CourtSearchBuilder class\n\u2502   \u251c\u2500\u2500 Parameter management methods\n\u2502   \u251c\u2500\u2500 URL building methods\n\u2502   \u2514\u2500\u2500 Validation methods\n\u251c\u2500\u2500 extract_court_cases_with_params() function\n\u2502   \u251c\u2500\u2500 Search execution logic\n\u2502   \u251c\u2500\u2500 Progress reporting\n\u2502   \u2514\u2500\u2500 Output generation\n\u2514\u2500\u2500 main() function\n    \u251c\u2500\u2500 CLI argument parsing\n    \u251c\u2500\u2500 Parameter validation\n    \u2514\u2500\u2500 Function orchestration\n</code></pre>"},{"location":"configurable_court_extractor_design/#why-this-design-is-better","title":"Why This Design is Better","text":"<ol> <li>Single Responsibility: Each class/function has one clear purpose</li> <li>Open/Closed Principle: Easy to extend without modifying existing code</li> <li>DRY (Don't Repeat Yourself): URL logic is centralized</li> <li>User-Centered: Designed around user needs, not technical constraints</li> <li>Testable: Components can be unit tested independently</li> <li>Documented: Self-documenting code with clear method names</li> </ol> <p>This design transforms a rigid, single-purpose script into a flexible, user-friendly tool that can adapt to various research needs while maintaining the reliability and performance of the original implementation.</p>"},{"location":"configurable_court_extractor_design/#complete-implementation-code","title":"Complete Implementation Code","text":"<p>Below is the full implementation of the configurable court extractor:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nConfigurable Court Case Extractor for Alabama Appeals Court\nAllows users to set custom search parameters OR use pre-built URLs\n\"\"\"\nimport json\nimport argparse\nfrom datetime import datetime, timedelta\nfrom urllib.parse import quote\nfrom opal.court_case_parser import ParserAppealsAL\nfrom opal.court_url_paginator import parse_court_url\n\n\nclass CourtSearchBuilder:\n    \"\"\"Builder class for constructing Alabama Court search URLs with court-specific parameters\"\"\"\n\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n\n        # Court definitions with their specific IDs and configurations\n        # NOTE: Court IDs may be dynamically assigned by the website\n        # These IDs should be discovered through session initialization\n        self.courts = {\n            'civil': {\n                'name': 'Alabama Civil Court of Appeals',\n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'CL',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n            },\n            'criminal': {\n                'name': 'Alabama Court of Criminal Appeals', \n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'CR',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n            },\n            'supreme': {\n                'name': 'Alabama Supreme Court',\n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'SC',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n            }\n        }\n\n        # Date period mappings\n        self.date_periods = {\n            '7d': '-7d',\n            '1m': '-1m', \n            '3m': '-3m',\n            '6m': '-6m',\n            '1y': '-1y',\n            'custom': 'custom'\n        }\n\n        self.current_court = 'civil'  # Default court\n        self.session_initialized = False\n        self.reset_params()\n\n    def reset_params(self):\n        \"\"\"Reset all parameters to defaults\"\"\"\n        court_info = self.courts[self.current_court]\n        self.params = {\n            'advanced': 'false',\n            'courtID': court_info['id'],  # May be None until discovered\n            'page': {\n                'size': 500,\n                'number': 0,\n                'totalElements': 0,\n                'totalPages': 0\n            },\n            'sort': {\n                'sortBy': 'caseHeader.filedDate',\n                'sortDesc': 'true'\n            },\n            'case': {\n                'caseCategoryID': 1000000,  # All categories\n                'caseNumberQueryTypeID': 10463,  # Contains\n                'caseTitleQueryTypeID': 300054,  # Contains\n                'filedDateChoice': '1y',  # Last year\n                'filedDateStart': '',\n                'filedDateEnd': '',\n                'excludeClosed': 'false'\n            }\n        }\n\n    def discover_court_ids(self, parser_instance):\n        \"\"\"\n        Discover court IDs by navigating to the website and inspecting the court selection interface\n\n        Args:\n            parser_instance: Instance of ParserAppealsAL with active WebDriver\n        \"\"\"\n        try:\n            # Navigate to the main search page\n            search_page_url = \"https://publicportal.alappeals.gov/portal/search/case\"\n            parser_instance.driver.get(search_page_url)\n\n            # Wait for the page to load\n            from selenium.webdriver.support.ui import WebDriverWait\n            from selenium.webdriver.support import expected_conditions as EC\n            from selenium.webdriver.common.by import By\n\n            wait = WebDriverWait(parser_instance.driver, 10)\n\n            # Look for court selection dropdown or options\n            # This is a placeholder - actual implementation would need to inspect the HTML structure\n            court_selector = wait.until(EC.presence_of_element_located((By.ID, \"court-selector\")))\n\n            # Extract court options and their IDs\n            # Implementation would parse the HTML to find court names and their corresponding IDs\n            court_options = court_selector.find_elements(By.TAG_NAME, \"option\")\n\n            for option in court_options:\n                court_name = option.text.lower()\n                court_id = option.get_attribute(\"value\")\n\n                # Map court names to our court keys\n                if \"civil\" in court_name and \"appeals\" in court_name:\n                    self.courts['civil']['id'] = court_id\n                elif \"criminal\" in court_name and \"appeals\" in court_name:\n                    self.courts['criminal']['id'] = court_id\n                elif \"supreme\" in court_name:\n                    self.courts['supreme']['id'] = court_id\n\n            self.session_initialized = True\n            print(\"Successfully discovered court IDs:\")\n            for court_key, court_info in self.courts.items():\n                print(f\"  {court_info['name']}: {court_info['id']}\")\n\n        except Exception as e:\n            print(f\"Warning: Could not discover court IDs automatically: {e}\")\n            print(\"Try running your search on the website and searching by URL populated by the search\")\n\n    def set_court_id_manually(self, court_key, court_id):\n        \"\"\"\n        Set court ID manually\n\n        Args:\n            court_key: 'civil', 'criminal', or 'supreme'\n            court_id: The discovered court ID string\n        \"\"\"\n        if court_key in self.courts:\n            self.courts[court_key]['id'] = court_id\n            print(f\"Manually set {court_key} court ID to: {court_id}\")\n        else:\n            raise ValueError(f\"Invalid court key: {court_key}\")\n\n    def set_court(self, court_key):\n        \"\"\"\n        Set the court to search\n\n        Args:\n            court_key: 'civil', 'criminal', or 'supreme'\n        \"\"\"\n        if court_key not in self.courts:\n            raise ValueError(f\"Invalid court: {court_key}. Must be one of {list(self.courts.keys())}\")\n\n        self.current_court = court_key\n        self.reset_params()  # Reset params with new court ID\n\n    def get_court_info(self):\n        \"\"\"Get information about the current court\"\"\"\n        return self.courts[self.current_court]\n\n    def validate_case_category(self, category):\n        \"\"\"Validate that the category is available for the current court\"\"\"\n        court_info = self.courts[self.current_court]\n        if category not in court_info['categories']:\n            raise ValueError(f\"Category '{category}' not available for {court_info['name']}. \"\n                           f\"Available: {court_info['categories']}\")\n        return True\n\n    def format_case_number_suggestion(self, year=None):\n        \"\"\"Suggest proper case number format for current court\"\"\"\n        court_info = self.courts[self.current_court]\n        current_year = year or datetime.now().year\n        return f\"{court_info['case_prefix']}-{current_year}-####\"\n\n    def set_date_range(self, start_date=None, end_date=None, period='1y'):\n        \"\"\"\n        Set the date range for case searches\n\n        Args:\n            start_date: Start date (YYYY-MM-DD) or None\n            end_date: End date (YYYY-MM-DD) or None  \n            period: Predefined period ('7d', '1m', '3m', '6m', '1y', 'custom')\n        \"\"\"\n        if period == 'custom' and start_date and end_date:\n            # Convert dates to the format expected by the portal\n            self.params['case']['filedDateChoice'] = 'custom'\n            self.params['case']['filedDateStart'] = start_date.replace('-', '*2f')\n            self.params['case']['filedDateEnd'] = end_date.replace('-', '*2f')\n        else:\n            # Use predefined period - validate it exists\n            if period not in self.date_periods:\n                raise ValueError(f\"Invalid date period: {period}. Must be one of {list(self.date_periods.keys())}\")\n\n            self.params['case']['filedDateChoice'] = self.date_periods[period]\n\n            # Calculate dates for display purposes\n            today = datetime.now()\n            if period == '7d':\n                start = today - timedelta(days=7)\n            elif period == '1m':\n                start = today - timedelta(days=30)\n            elif period == '3m':\n                start = today - timedelta(days=90)\n            elif period == '6m':\n                start = today - timedelta(days=180)\n            elif period == '1y':\n                start = today - timedelta(days=365)\n            else:\n                start = today - timedelta(days=365)\n\n            self.params['case']['filedDateStart'] = start.strftime('%m*2f%d*2f%Y')\n            self.params['case']['filedDateEnd'] = today.strftime('%m*2f%d*2f%Y')\n\n    def set_case_category(self, category_name=None):\n        \"\"\"\n        Set case category filter\n\n        Args:\n            category_name: Category name ('Appeal', 'Certiorari', 'Original Proceeding', \n                          'Petition', 'Certified Question') or None for all\n        \"\"\"\n        if category_name is None:\n            self.params['case']['caseCategoryID'] = 1000000  # All categories\n            return\n\n        # Validate category is available for current court\n        self.validate_case_category(category_name)\n\n        # Map category names to IDs (these would need to be discovered from the portal)\n        category_map = {\n            'Appeal': 1000001,\n            'Certiorari': 1000002, \n            'Original Proceeding': 1000003,\n            'Petition': 1000004,\n            'Certified Question': 1000005  # Supreme Court only\n        }\n\n        if category_name in category_map:\n            self.params['case']['caseCategoryID'] = category_map[category_name]\n        else:\n            raise ValueError(f\"Unknown category: {category_name}\")\n\n    def set_case_number_filter(self, case_number=None, query_type=10463):\n        \"\"\"\n        Set case number filter\n\n        Args:\n            case_number: Case number to search for\n            query_type: Query type (10463=contains, check portal for others)\n        \"\"\"\n        self.params['case']['caseNumberQueryTypeID'] = query_type\n        if case_number:\n            self.params['case']['caseNumber'] = case_number\n\n    def set_case_title_filter(self, title=None, query_type=300054):\n        \"\"\"\n        Set case title filter\n\n        Args:\n            title: Title text to search for\n            query_type: Query type (300054=contains, check portal for others)\n        \"\"\"\n        self.params['case']['caseTitleQueryTypeID'] = query_type\n        if title:\n            self.params['case']['caseTitle'] = title\n\n    def set_exclude_closed(self, exclude=False):\n        \"\"\"\n        Set whether to exclude closed cases\n\n        Args:\n            exclude: True to exclude closed cases, False to include all\n        \"\"\"\n        self.params['case']['excludeClosed'] = 'true' if exclude else 'false'\n\n    def set_sort_order(self, sort_by='caseHeader.filedDate', descending=True):\n        \"\"\"\n        Set sort order for results\n\n        Args:\n            sort_by: Field to sort by\n            descending: True for descending, False for ascending\n        \"\"\"\n        self.params['sort']['sortBy'] = sort_by\n        self.params['sort']['sortDesc'] = 'true' if descending else 'false'\n\n    def set_page_info(self, page_number=0, page_size=25, total_elements=0, total_pages=0):\n        \"\"\"Set pagination information\"\"\"\n        self.params['page'].update({\n            'number': page_number,\n            'size': page_size,\n            'totalElements': total_elements,\n            'totalPages': total_pages\n        })\n\n    def build_criteria_string(self):\n        \"\"\"Build the criteria string for the URL\"\"\"\n        criteria_parts = []\n\n        # Basic parameters\n        criteria_parts.append(f\"advanced~{self.params['advanced']}\")\n        criteria_parts.append(f\"courtID~%27{self.params['courtID']}\")\n\n        # Page parameters\n        page = self.params['page']\n        page_str = f\"page~%28size~{page['size']}~number~{page['number']}~totalElements~{page['totalElements']}~totalPages~{page['totalPages']}%29\"\n        criteria_parts.append(page_str)\n\n        # Sort parameters\n        sort = self.params['sort']\n        sort_str = f\"sort~%28sortBy~%27{sort['sortBy']}~sortDesc~{sort['sortDesc']}%29\"\n        criteria_parts.append(sort_str)\n\n        # Case parameters\n        case = self.params['case']\n        case_parts = []\n        case_parts.append(f\"caseCategoryID~{case['caseCategoryID']}\")\n        case_parts.append(f\"caseNumberQueryTypeID~{case['caseNumberQueryTypeID']}\")\n        case_parts.append(f\"caseTitleQueryTypeID~{case['caseTitleQueryTypeID']}\")\n        case_parts.append(f\"filedDateChoice~%27{case['filedDateChoice']}\")\n        case_parts.append(f\"filedDateStart~%27{case['filedDateStart']}\")\n        case_parts.append(f\"filedDateEnd~%27{case['filedDateEnd']}\")\n        case_parts.append(f\"excludeClosed~{case['excludeClosed']}\")\n\n        # Add optional case filters\n        if 'caseNumber' in case:\n            case_parts.append(f\"caseNumber~{quote(case['caseNumber'])}\")\n        if 'caseTitle' in case:\n            case_parts.append(f\"caseTitle~{quote(case['caseTitle'])}\")\n\n        case_str = f\"case~%28{'~'.join(case_parts)}%29\"\n        criteria_parts.append(case_str)\n\n        return f\"~%28{'~'.join(criteria_parts)}%29\"\n\n    def build_url(self, page_number=0):\n        \"\"\"Build complete search URL\"\"\"\n        # Update page number\n        self.set_page_info(page_number=page_number, \n                          page_size=self.params['page']['size'],\n                          total_elements=self.params['page']['totalElements'],\n                          total_pages=self.params['page']['totalPages'])\n\n        criteria = self.build_criteria_string()\n        return f\"{self.base_url}?criteria={criteria}\"\n\n\ndef extract_court_cases_with_params(\n    court='civil',\n    date_period='1y',\n    start_date=None,\n    end_date=None,\n    case_number=None,\n    case_title=None,\n    case_category=None,\n    exclude_closed=False,\n    max_pages=None,\n    output_prefix=\"court_cases\",\n    custom_url=None\n):\n    \"\"\"\n    Extract court cases with configurable search parameters OR a pre-built URL\n\n    Args:\n        court: Court to search ('civil', 'criminal', 'supreme') - ignored if custom_url provided\n        date_period: Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - ignored if custom_url provided\n        start_date: Start date for custom range (YYYY-MM-DD) - ignored if custom_url provided\n        end_date: End date for custom range (YYYY-MM-DD) - ignored if custom_url provided\n        case_number: Filter by case number (partial match) - ignored if custom_url provided\n        case_title: Filter by case title (partial match) - ignored if custom_url provided\n        case_category: Filter by category ('Appeal', 'Certiorari', etc.) - ignored if custom_url provided\n        exclude_closed: Whether to exclude closed cases - ignored if custom_url provided\n        max_pages: Maximum pages to process (None for all)\n        output_prefix: Prefix for output files\n        custom_url: Pre-built search URL with embedded parameters (overrides all other search params)\n    \"\"\"\n\n    if custom_url:\n        # Use the provided URL directly\n        print(\"Using custom URL with embedded search parameters\")\n        print(\"\u26a0\ufe0f  WARNING: Custom URLs contain session-specific parameters that expire.\")\n        print(\"   This URL will only work temporarily and may become invalid after your browser session ends.\")\n        print(\"   For reliable, repeatable searches, use the CLI search parameters instead of --url option.\")\n        print()\n        base_url = custom_url\n        court_name = \"Custom Search\"  # Generic name since we don't know the court\n    else:\n        # Build search URL from parameters\n        search_builder = CourtSearchBuilder()\n\n        # Create parser instance early for court ID discovery\n        parser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\n\n        # Discover court IDs if not already done\n        if not search_builder.session_initialized:\n            print(\"Discovering court IDs from website...\")\n            search_builder.discover_court_ids(parser)\n\n        # Set court\n        search_builder.set_court(court)\n        court_info = search_builder.get_court_info()\n        court_name = court_info['name']\n\n        # Verify court ID was discovered\n        if court_info['id'] is None:\n            raise ValueError(f\"Could not discover court ID for {court_name}. \"\n                           \"Try using the --url option with a pre-built search URL instead.\")\n\n        # Set date range\n        if date_period == 'custom':\n            if not start_date or not end_date:\n                raise ValueError(\"Custom date range requires both start_date and end_date\")\n            search_builder.set_date_range(start_date, end_date, 'custom')\n        else:\n            search_builder.set_date_range(period=date_period)\n\n        # Set filters\n        if case_number:\n            search_builder.set_case_number_filter(case_number)\n        if case_title:\n            search_builder.set_case_title_filter(case_title)\n        if case_category:\n            search_builder.set_case_category(case_category)\n\n        search_builder.set_exclude_closed(exclude_closed)\n\n        # Build initial URL\n        base_url = search_builder.build_url(0)\n\n    print(\"Alabama Appeals Court - Configurable Data Extraction\")\n    print(\"=\" * 55)\n    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Date period: {date_period}\")\n    if date_period == 'custom':\n        print(f\"Date range: {start_date} to {end_date}\")\n    if case_number:\n        print(f\"Case number filter: {case_number}\")\n    if case_title:\n        print(f\"Case title filter: {case_title}\")\n    print(f\"Exclude closed: {exclude_closed}\")\n    print(f\"Max pages: {max_pages or 'All available'}\")\n    print()\n\n    # Create parser instance (may have been created earlier for court ID discovery)\n    if 'parser' not in locals():\n        parser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\n\n    try:\n        # First, get the first page to determine total pages\n        print(\"Loading first page to determine total results...\")\n        result = parser.parse_article(base_url)\n\n        if \"cases\" not in result or not result['cases']:\n            print(\"No cases found with the specified criteria.\")\n            return\n\n        # Try to get total pages from the URL after JavaScript execution\n        if hasattr(parser, 'driver') and parser.driver:\n            current_url = parser.driver.current_url\n            _, total_pages = parse_court_url(current_url)\n\n        if not total_pages:\n            # Estimate based on first page results\n            total_pages = 1\n            print(f\"Could not determine total pages, will process incrementally\")\n        else:\n            print(f\"Found {total_pages} total pages\")\n\n        # Apply max_pages limit\n        if max_pages and max_pages &lt; total_pages:\n            total_pages = max_pages\n            print(f\"Limited to {max_pages} pages\")\n\n        all_cases = []\n\n        # Process all pages\n        for page_num in range(total_pages):\n            print(f\"Processing page {page_num + 1}...\", end='', flush=True)\n\n            if page_num == 0:\n                # Use result from first page\n                page_result = result\n            else:\n                # Build URL for subsequent pages only if not using custom URL\n                if custom_url:\n                    # For custom URLs, we need to modify pagination manually\n                    # This is a simplified approach - in practice, you'd need to parse and modify the URL\n                    page_url = custom_url.replace('number~0', f'number~{page_num}')\n                else:\n                    page_url = search_builder.build_url(page_num)\n                page_result = parser.parse_article(page_url)\n\n            if \"cases\" in page_result and page_result['cases']:\n                all_cases.extend(page_result['cases'])\n                print(f\" Found {len(page_result['cases'])} cases\")\n            else:\n                print(\" No cases found\")\n                # If no cases on this page, we might have reached the end\n                break\n\n        # Create output data\n        output_data = {\n            \"status\": \"success\",\n            \"search_parameters\": {\n                \"court\": court if not custom_url else \"Custom URL\",\n                \"date_period\": date_period if not custom_url else \"Custom URL\",\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n                \"case_number_filter\": case_number,\n                \"case_title_filter\": case_title,\n                \"case_category\": case_category,\n                \"exclude_closed\": exclude_closed,\n                \"custom_url\": custom_url\n            },\n            \"total_cases\": len(all_cases),\n            \"extraction_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n            \"extraction_time\": datetime.now().strftime(\"%H:%M:%S\"),\n            \"pages_processed\": page_num + 1,\n            \"cases\": all_cases\n        }\n\n        # Save results\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        json_filename = f\"{output_prefix}_{timestamp}.json\"\n\n        with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n            json.dump(output_data, f, indent=4, ensure_ascii=False)\n\n        print(f\"\\n\u2713 Successfully extracted {len(all_cases)} court cases\")\n        print(f\"\u2713 Results saved to {json_filename}\")\n\n        # Create CSV if there are results\n        if all_cases:\n            csv_filename = f\"{output_prefix}_{timestamp}.csv\"\n            with open(csv_filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"Court,Case Number,Case Title,Classification,Filed Date,Status,Case Link\\n\")\n\n                for case in all_cases:\n                    court = case.get('court', '').replace(',', ';')\n                    case_num = case.get('case_number', {}).get('text', '').replace(',', ';')\n                    title = case.get('case_title', '').replace(',', ';').replace('\"', \"'\")\n                    classification = case.get('classification', '').replace(',', ';')\n                    filed = case.get('filed_date', '')\n                    status = case.get('status', '')\n                    link = f\"https://publicportal.alappeals.gov{case.get('case_number', {}).get('link', '')}\"\n\n                    f.write(f'\"{court}\",\"{case_num}\",\"{title}\",\"{classification}\",\"{filed}\",\"{status}\",\"{link}\"\\n')\n\n            print(f\"\u2713 CSV table saved to {csv_filename}\")\n\n        return output_data\n\n    except Exception as e:\n        print(f\"\\nError occurred: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None\n    finally:\n        parser._close_driver()\n        print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\ndef main():\n    \"\"\"Command line interface for the configurable court extractor\"\"\"\n    parser = argparse.ArgumentParser(description='Extract Alabama Court cases with configurable search parameters OR custom URL')\n\n    # URL option that overrides all search parameters\n    parser.add_argument('--url', help='Pre-built search URL with embedded parameters (overrides all search options)')\n\n    # Search parameter arguments (ignored if --url is provided)\n    parser.add_argument('--court', choices=['civil', 'criminal', 'supreme'], \n                       default='civil', help='Court to search (default: civil)')\n    parser.add_argument('--date-period', choices=['7d', '1m', '3m', '6m', '1y', 'custom'], \n                       default='1y', help='Date period for case search (default: 1y)')\n    parser.add_argument('--start-date', help='Start date for custom range (YYYY-MM-DD)')\n    parser.add_argument('--end-date', help='End date for custom range (YYYY-MM-DD)')\n    parser.add_argument('--case-number', help='Filter by case number (e.g., CL-2024-, CR-2024-, SC-2024-)')\n    parser.add_argument('--case-title', help='Filter by case title (partial match)')\n    parser.add_argument('--case-category', \n                       choices=['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question'],\n                       help='Filter by case category')\n    parser.add_argument('--exclude-closed', action='store_true', \n                       help='Exclude closed cases from results')\n\n    # Output options (always available)\n    parser.add_argument('--max-pages', type=int, \n                       help='Maximum number of pages to process (default: all)')\n    parser.add_argument('--output-prefix', default='court_cases',\n                       help='Prefix for output files (default: court_cases)')\n\n    args = parser.parse_args()\n\n    # If URL is provided, skip all parameter validation\n    if args.url:\n        print(\"Using custom URL - all search parameter options will be ignored\")\n        print(\"\u26a0\ufe0f  IMPORTANT: Custom URLs are session-based and temporary!\")\n        print(\"   Your URL may stop working when the court website session expires.\")\n        print(\"   Consider using CLI search parameters for reliable, repeatable searches.\")\n        print()\n        extract_court_cases_with_params(\n            custom_url=args.url,\n            max_pages=args.max_pages,\n            output_prefix=args.output_prefix\n        )\n        return\n\n    # Validate custom date range\n    if args.date_period == 'custom':\n        if not args.start_date or not args.end_date:\n            parser.error(\"Custom date period requires both --start-date and --end-date\")\n\n    # Validate case category for court\n    if args.case_category:\n        builder = CourtSearchBuilder()\n        builder.set_court(args.court)\n        try:\n            builder.validate_case_category(args.case_category)\n        except ValueError as e:\n            parser.error(str(e))\n\n    # Show case number format suggestion\n    if args.case_number:\n        builder = CourtSearchBuilder()\n        builder.set_court(args.court)\n        suggested_format = builder.format_case_number_suggestion()\n        print(f\"Case number format for {builder.get_court_info()['name']}: {suggested_format}\")\n\n    # Extract cases using search parameters\n    extract_court_cases_with_params(\n        court=args.court,\n        date_period=args.date_period,\n        start_date=args.start_date,\n        end_date=args.end_date,\n        case_number=args.case_number,\n        case_title=args.case_title,\n        case_category=args.case_category,\n        exclude_closed=args.exclude_closed,\n        max_pages=args.max_pages,\n        output_prefix=args.output_prefix\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"configurable_court_extractor_design/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"configurable_court_extractor_design/#courtsearchbuilder-class","title":"CourtSearchBuilder Class","text":"<p>Line 12-17: Initialize with base URL and court ID constants. The <code>reset_params()</code> method sets up the default parameter structure.</p> <p>Line 19-46: The <code>reset_params()</code> method creates a nested dictionary structure that mirrors the complex URL parameters used by the Alabama Appeals Court portal.</p> <p>Line 48-74: <code>set_date_range()</code> handles both predefined periods and custom date ranges. It converts standard YYYY-MM-DD format to the portal's <code>*2f</code> encoding format.</p> <p>Line 140-169: <code>build_criteria_string()</code> is the core URL building logic. It constructs the complex nested parameter string with proper URL encoding.</p> <p>Line 171-180: <code>build_url()</code> ties everything together, updating pagination and returning the complete URL.</p>"},{"location":"configurable_court_extractor_design/#main-extraction-function","title":"Main Extraction Function","text":"<p>Line 584-596: Function signature with comprehensive parameters including <code>custom_url</code> option for pre-built URLs.</p> <p>Line 614-647: Dual-mode logic - uses custom URL directly OR builds URL from search parameters.</p> <p>Line 649-661: Initial setup and parameter display for user feedback, with custom URL handling.</p> <p>Line 666-689: First page processing to determine total results and pages available.</p> <p>Line 694-713: Main processing loop that handles pagination dynamically for both custom URLs and built URLs.</p> <p>Line 719-738: Output data structure creation with search parameters preserved for reproducibility, including custom URL tracking.</p> <p>Line 740-760: File saving logic for both JSON and CSV formats.</p>"},{"location":"configurable_court_extractor_design/#command-line-interface_1","title":"Command Line Interface","text":"<p>Line 781-807: Argument parser setup with <code>--url</code> option and all configuration options with help text.</p> <p>Line 811-819: Custom URL handling that bypasses all parameter validation when <code>--url</code> is provided.</p> <p>Line 821-854: Parameter validation and function execution for search-parameter mode.</p>"},{"location":"configurable_court_extractor_design/#key-design-decisions-explained","title":"Key Design Decisions Explained","text":"<ol> <li>Builder Pattern: Separates URL construction complexity from business logic</li> <li>Dual-Mode Operation: Supports both parameter-based search and pre-built URL input</li> <li>Parameter Validation: Ensures required combinations are provided (custom dates)</li> <li>Progressive Enhancement: Starts with defaults, allows selective customization</li> <li>Error Recovery: Graceful handling when page counts can't be determined</li> <li>Output Consistency: Maintains same format as original extractor</li> <li>User Feedback: Real-time progress and parameter confirmation</li> <li>URL Flexibility: Custom URLs override all search parameters for maximum flexibility</li> </ol>"},{"location":"court_scraper_analysis/","title":"Court Scraper Analysis","text":"<p>Based on your answers, I notice several important details:</p>"},{"location":"court_scraper_analysis/#1-dynamic-javascript-rendered-content","title":"1. Dynamic JavaScript-Rendered Content","text":"<ul> <li>The site loads content dynamically, which means BeautifulSoup alone won't work</li> <li>You'll need Selenium or Playwright to render JavaScript before parsing</li> </ul>"},{"location":"court_scraper_analysis/#2-complex-url-based-pagination","title":"2. Complex URL-Based Pagination","text":"<ul> <li>Pagination uses URL parameters (<code>page~(number~0)</code> becomes <code>page~(number~1)</code>)</li> <li>Total pages/elements are embedded in the URL (<code>totalElements~317~totalPages~13</code>)</li> <li>This is very different from the simple <code>/page/2</code> pattern in your news scrapers</li> </ul>"},{"location":"court_scraper_analysis/#3-structured-table-data","title":"3. Structured Table Data","text":"<ul> <li>You need 6 specific columns: Court, Case Number, Case Title, Classification, Filed Date, Open/Closed</li> <li>Case Numbers contain links you want to preserve (both text and href)</li> <li>You want all paginated results combined into one table</li> </ul>"},{"location":"court_scraper_analysis/#4-custom-url-encoding","title":"4. Custom URL Encoding","text":"<ul> <li>The URLs use a unique encoding scheme with <code>~</code> and <code>%2a2f</code> (for slashes)</li> <li>Search parameters are complex with date ranges and multiple filters</li> </ul>"},{"location":"court_scraper_analysis/#key-challenges","title":"Key Challenges:","text":"<ul> <li>Need to handle JavaScript rendering</li> <li>Must parse and manipulate the encoded URL format for pagination</li> <li>Need to extract both text and href attributes from case number links</li> <li>Different data structure (tabular vs. article paragraphs)</li> </ul>"},{"location":"court_scraper_analysis/#proposed-solution","title":"Proposed Solution:","text":"<p>Would you like me to create a new parser that handles these requirements? I'll need to: 1. Add Selenium/Playwright to requirements 2. Create a <code>CourtCaseParser</code> with JavaScript rendering 3. Build a custom pagination handler for this URL format 4. Output structured table data instead of article format</p>"},{"location":"court_scraper_requirements/","title":"Court Case Scraper Extension Requirements","text":"<p>To help you build this court case scraper extension while maintaining OPAL's modularity, I'll need the following information:</p>"},{"location":"court_scraper_requirements/#1-website-details","title":"1. Website Details","text":"<ul> <li>The URL of the court case website Example URL: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Example URLs of pages containing the tables you want to scrape Example URL after pagination of next batch of table elements: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~1~totalElements~317~totalPages~13%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Screenshots or HTML snippets of the table structure</li> </ul>"},{"location":"court_scraper_requirements/#2-data-requirements","title":"2. Data Requirements","text":"<ul> <li>What specific data fields do you need from the tables? (case number, parties, dates, status, etc.) I will need to access the following html fields for data</li> </ul> <p>Column 1 Title: Court Court</p> <p>Column 1 Content Follows this pattern: Alabama Supreme Court</p> <p>Column 2 Title: Case Number Case Number</p> <p>Column 2 Content Follows this pattern:  SC-2025-0424 </p> <p>Column 3 Title: Case Title Case Title</p> <p>Column 3 Content Follows this pattern: Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.</p> <p>Column 4 Title: Classification Classification</p> <p>Column 4 Content Follows this pattern: Appeal - Civil - Injunction Other</p> <p>Column 5 Title: Filed Date Filed Date</p> <p>Column 5 Content Follows this pattern:  06/10/2025 </p> <p>Column 6 Title: Open / Closed Open / Closed</p> <p>Column 6 Content Follows this pattern:  Open </p> <ul> <li>Do you need data from multiple tables per page or one main table?</li> </ul> <p>I only want one table with all of the results of all of the pages at that url. Even if pagination is used to reduce the number of table elements that appear at a time, I want all the results in a single table.</p> <ul> <li>Any specific formatting requirements for the extracted data?</li> </ul>"},{"location":"court_scraper_requirements/#3-navigation-pattern","title":"3. Navigation Pattern","text":"<ul> <li> <p>Does the site use pagination like the news sites? The pagination is not the same. The content is grouped into small chunks, but accessible at the same base url.</p> </li> <li> <p>Are there search/filter parameters in the URL? There are multiple search parameters in the URL. For example, here are the search terms for this url [case~%28caseCategoryID~1000000, caseNumberQueryTypeID~10463, aseTitleQueryTypeID~300054, iledDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025, excludeClosed~false%29%29]</p> </li> <li>Do you need to follow links within tables to get additional details?</li> </ul> <p>I do not want to follow the links within the table, but I do want to store the text and the reference embedded in the link.</p>"},{"location":"court_scraper_requirements/#4-technical-considerations","title":"4. Technical Considerations","text":"<ul> <li>Does the site require authentication? No</li> <li>Is the content loaded dynamically (JavaScript) or static HTML? Dynamically</li> <li>Any rate limiting concerns we should be aware of? Please keep the rate limits low</li> </ul>"},{"location":"court_scraper_requirements/#proposed-extension-architecture","title":"Proposed Extension Architecture","text":"<p>Based on OPAL's current architecture, here's how we'd extend it:</p> <ol> <li>Create a new parser class (e.g., <code>CourtCaseParser</code>) extending <code>NewsParser</code> in <code>parser_module.py</code></li> <li>Adapt or create a new URL discovery function if the pagination pattern differs from the news sites</li> <li>Modify the CLI in <code>main.py</code> to add the court parser option</li> <li>Ensure the output format makes sense for tabular data (might need to adjust from the line-by-line article format)</li> </ol>"},{"location":"court_scraper_requirements/#next-steps","title":"Next Steps","text":"<p>Please provide: 1. The court website URL 2. Description of the table structure you need to parse 3. Any specific requirements or constraints</p> <p>This will help me design the extension to fit seamlessly with your existing OPAL architecture.</p>"},{"location":"developer/BaseParser_web_scraping_guide/","title":"BaseParser: Web Scraping Fundamentals Guide","text":""},{"location":"developer/BaseParser_web_scraping_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Core Concepts</li> <li>HTTP Requests</li> <li>HTML Parsing</li> <li>Beautiful Soup</li> <li>BaseParser Architecture</li> <li>Implementation Examples</li> <li>Best Practices</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#introduction","title":"Introduction","text":"<p>The <code>BaseParser</code> class is the foundation of OPAL's web scraping system. It provides a standardized interface for extracting structured data from websites, whether they're news articles or court records. This guide explains the core concepts behind web scraping and how BaseParser implements them.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#core-concepts","title":"Core Concepts","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-is-web-scraping","title":"What is Web Scraping?","text":"<p>Web scraping is the process of extracting data from websites programmatically. It involves:</p> <ol> <li>Fetching - Downloading the HTML content from a web server</li> <li>Parsing - Analyzing the HTML structure to find specific data</li> <li>Extracting - Pulling out the desired information</li> <li>Structuring - Organizing the data into a useful format (like JSON)</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#the-web-scraping-pipeline","title":"The Web Scraping Pipeline","text":"<pre><code>URL \u2192 HTTP Request \u2192 HTML Response \u2192 Parse HTML \u2192 Extract Data \u2192 Structure Output\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#http-requests","title":"HTTP Requests","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-are-http-requests","title":"What are HTTP Requests?","text":"<p>HTTP (HyperText Transfer Protocol) requests are how programs communicate with web servers. When you visit a website, your browser sends an HTTP request to the server, which responds with the HTML content.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#key-components-of-http-requests","title":"Key Components of HTTP Requests","text":"<ol> <li>Method: Usually GET for retrieving data</li> <li>URL: The address of the resource</li> <li>Headers: Metadata about the request (User-Agent, Accept types, etc.)</li> <li>Response: The server's reply containing status code and content</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#example-from-baseparser","title":"Example from BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers\"\"\"\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, timeout=5)  # HTTP GET request\n            response.raise_for_status()  # Check for HTTP errors\n            responses.append(response.text)  # Store HTML content\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#common-http-status-codes","title":"Common HTTP Status Codes","text":"<ul> <li>200: Success - the page loaded correctly</li> <li>404: Not Found - the page doesn't exist</li> <li>403: Forbidden - access denied</li> <li>500: Server Error - problem on the website's end</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#html-parsing","title":"HTML Parsing","text":""},{"location":"developer/BaseParser_web_scraping_guide/#understanding-html-structure","title":"Understanding HTML Structure","text":"<p>HTML (HyperText Markup Language) is the standard markup language for web pages. It uses a tree-like structure of nested elements:</p> <pre><code>&lt;html&gt;\n  &lt;body&gt;\n    &lt;div class=\"article\"&gt;\n      &lt;h1&gt;Article Title&lt;/h1&gt;\n      &lt;p class=\"author\"&gt;By John Doe&lt;/p&gt;\n      &lt;div class=\"content\"&gt;\n        &lt;p&gt;First paragraph...&lt;/p&gt;\n        &lt;p&gt;Second paragraph...&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#the-document-object-model-dom","title":"The Document Object Model (DOM)","text":"<p>The DOM represents HTML as a tree structure where: - Each HTML tag is a node - Nodes can have attributes (class, id, href) - Nodes can contain text or other nodes - Nodes have parent-child relationships</p>"},{"location":"developer/BaseParser_web_scraping_guide/#parsing-strategy","title":"Parsing Strategy","text":"<ol> <li>Identify patterns: Find consistent HTML structures</li> <li>Use selectors: Target specific elements by tag, class, or ID</li> <li>Navigate relationships: Move between parent/child/sibling elements</li> <li>Extract data: Get text content or attribute values</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#beautiful-soup","title":"Beautiful Soup","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-is-beautiful-soup","title":"What is Beautiful Soup?","text":"<p>Beautiful Soup is a Python library designed for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data in a more Pythonic way.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#key-features","title":"Key Features","text":"<ol> <li>Automatic encoding detection: Handles different character encodings</li> <li>Lenient parsing: Works with poorly formatted HTML</li> <li>Powerful searching: Find elements using various methods</li> <li>Tree navigation: Move through the document structure easily</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#beautiful-soup-methods","title":"Beautiful Soup Methods","text":""},{"location":"developer/BaseParser_web_scraping_guide/#finding-elements","title":"Finding Elements","text":"<pre><code># Find single elements\nsoup.find('tag')                    # First occurrence of tag\nsoup.find('tag', class_='classname') # First tag with specific class\nsoup.find('tag', {'attribute': 'value'}) # First tag with attribute\n\n# Find multiple elements\nsoup.find_all('tag')                # All occurrences of tag\nsoup.find_all(['tag1', 'tag2'])    # All occurrences of multiple tags\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#extracting-data","title":"Extracting Data","text":"<pre><code># Get text content\nelement.text          # All text including nested elements\nelement.get_text()    # Same as .text but with options\nelement.string        # Direct text only (no nested elements)\n\n# Get attributes\nelement.get('href')   # Get specific attribute\nelement['class']      # Get class attribute (returns list)\nelement.attrs         # Get all attributes as dictionary\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#real-examples-from-opal-parsers","title":"Real Examples from OPAL Parsers","text":""},{"location":"developer/BaseParser_web_scraping_guide/#parser1819-news-article-extraction","title":"Parser1819 - News Article Extraction","text":"<pre><code>def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Extract title from &lt;title&gt; tag\n    title_tag = soup.title\n    if title_tag:\n        article['title'] = title_tag.string.strip()\n\n    # Extract author from specific div structure\n    author_date_div = soup.find('div', class_='author-date')\n    if author_date_div:\n        author_link = author_date_div.find('a')\n        if author_link:\n            article['author'] = author_link.text.strip()\n\n    # Extract all paragraphs\n    paragraphs = soup.find_all(['p'])\n    for p in paragraphs:\n        text = p.get_text().strip()\n        # Process paragraph text...\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#parserappealsal-table-data-extraction","title":"ParserAppealsAL - Table Data Extraction","text":"<pre><code>def parse_table_row(self, row) -&gt; Optional[Dict]:\n    cells = row.find_all('td')  # Find all table cells\n    if len(cells) &lt; 6:\n        return None\n\n    # Extract text from specific cells\n    court = cells[0].get_text(strip=True)\n\n    # Extract both text and link from anchor tag\n    case_number_elem = cells[1].find('a')\n    if case_number_elem:\n        case_number = {\n            \"text\": case_number_elem.get_text(strip=True),\n            \"link\": case_number_elem.get('href', '')\n        }\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#baseparser-architecture","title":"BaseParser Architecture","text":""},{"location":"developer/BaseParser_web_scraping_guide/#abstract-base-class-design","title":"Abstract Base Class Design","text":"<p>BaseParser uses Python's ABC (Abstract Base Class) to define a contract that all parsers must follow:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseParser(ABC):\n    \"\"\"Base class defining the interface for all parsers\"\"\"\n\n    @abstractmethod\n    def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n        \"\"\"Each parser must implement this method\"\"\"\n        pass\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#why-use-abstract-base-classes","title":"Why Use Abstract Base Classes?","text":"<ol> <li>Consistency: Ensures all parsers have required methods</li> <li>Polymorphism: Different parsers can be used interchangeably</li> <li>Documentation: Clear contract for what parsers must implement</li> <li>Error Prevention: Catches missing implementations at instantiation</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#baseparser-methods","title":"BaseParser Methods","text":""},{"location":"developer/BaseParser_web_scraping_guide/#make_request","title":"make_request()","text":"<ul> <li>Purpose: Fetch HTML content from URLs</li> <li>Input: List of URLs</li> <li>Output: HTML content and successful URLs</li> <li>Error Handling: Continues on failure, reports errors</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#parse_article-abstract","title":"parse_article() (abstract)","text":"<ul> <li>Purpose: Extract data from single HTML page</li> <li>Input: HTML content and URL</li> <li>Output: Dictionary of extracted data</li> <li>Implementation: Must be defined by each parser subclass</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#parse_articles","title":"parse_articles()","text":"<ul> <li>Purpose: Coordinate parsing of multiple URLs</li> <li>Input: List of URLs</li> <li>Output: JSON string of all parsed data</li> <li>Process: Fetches HTML, calls parse_article(), combines results</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#implementation-examples","title":"Implementation Examples","text":""},{"location":"developer/BaseParser_web_scraping_guide/#creating-a-new-parser","title":"Creating a New Parser","text":"<pre><code>from opal.parser_module import BaseParser\nfrom bs4 import BeautifulSoup\n\nclass MyNewsParser(BaseParser):\n    \"\"\"Custom parser for a specific news site\"\"\"\n\n    def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Initialize data structure\n        article = {\n            'url': url,\n            'title': '',\n            'author': '',\n            'date': '',\n            'content': []\n        }\n\n        # Extract title\n        title_element = soup.find('h1', class_='article-title') #These classes are website specific\n        if title_element:\n            article['title'] = title_elem.get_text(strip=True)\n\n        # Extract author\n        author_element = soup.find('span', class_='byline')\n        if author_element:\n            article['author'] = author_elem.get_text(strip=True)\n\n        # Extract content paragraphs\n        content_div = soup.find('div', class_='article-body')\n        if content_div:\n            paragraphs = content_div.find_all('p')\n            article['content'] = [p.get_text(strip=True) for p in paragraphs]\n\n        return article\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#handling-complex-html-structures","title":"Handling Complex HTML Structures","text":"<p>Sometimes data is nested or spread across multiple elements:</p> <pre><code># Handle nested structures\narticle_div = soup.find('div', class_='article')\nif article_div:\n    # Navigate to nested elements\n    header = article_div.find('header')\n    if header:\n        title = header.find('h1')\n        meta = header.find('div', class_='meta')\n\n    # Find siblings\n    content = article_div.find_next_sibling('div', class_='content')\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<pre><code>def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n    try:\n        soup = BeautifulSoup(html, 'html.parser')\n        article = {'url': url}\n\n        # Always check if elements exist\n        title_elem = soup.find('h1')\n        if title_elem:\n            article['title'] = title_elem.get_text(strip=True)\n        else:\n            article['title'] = 'No title found'\n\n        # Handle missing attributes safely\n        link_elem = soup.find('a', class_='author-link')\n        if link_elem:\n            article['author_url'] = link_elem.get('href', '')\n\n        return article\n\n    except Exception as e:\n        # Return partial data rather than failing completely\n        return {\n            'url': url,\n            'error': str(e),\n            'partial_data': True\n        }\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#best-practices","title":"Best Practices","text":""},{"location":"developer/BaseParser_web_scraping_guide/#1-respect-website-policies","title":"1. Respect Website Policies","text":"<ul> <li>Check robots.txt (example: https://1819news.com/robots.txt)</li> <li>Add delays between requests</li> <li>Use appropriate User-Agent headers</li> <li>Don't overwhelm servers</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<ul> <li>Expect missing elements</li> <li>Provide default values</li> <li>Log errors for debugging</li> <li>Continue processing other data</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#3-write-maintainable-code","title":"3. Write Maintainable Code","text":"<ul> <li>Use descriptive variable names</li> <li>Comment complex selections</li> <li>Create reusable helper functions</li> <li>Test with various page structures</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#4-optimize-performance","title":"4. Optimize Performance","text":"<ul> <li>Reuse parser instances</li> <li>Batch process URLs</li> <li>Cache results when appropriate</li> <li>Close resources properly</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#5-structure-data-consistently","title":"5. Structure Data Consistently","text":"<ul> <li>Use consistent field names</li> <li>Provide empty defaults</li> <li>Validate data types</li> <li>Document output format</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#common-challenges-and-solutions","title":"Common Challenges and Solutions","text":""},{"location":"developer/BaseParser_web_scraping_guide/#dynamic-content","title":"Dynamic Content","text":"<p>Problem: Content loaded by JavaScript isn't in initial HTML Solution: Use Selenium (like ParserAppealsAL) for JavaScript rendering</p>"},{"location":"developer/BaseParser_web_scraping_guide/#changing-html-structure","title":"Changing HTML Structure","text":"<p>Problem: Website updates break selectors Solution: Use multiple fallback selectors, test regularly</p>"},{"location":"developer/BaseParser_web_scraping_guide/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: Too many requests trigger blocking Solution: Add delays, rotate User-Agents, respect rate limits</p>"},{"location":"developer/BaseParser_web_scraping_guide/#encoding-issues","title":"Encoding Issues","text":"<p>Problem: Special characters appear corrupted Solution: Beautiful Soup handles most encoding automatically</p>"},{"location":"developer/BaseParser_web_scraping_guide/#conclusion","title":"Conclusion","text":"<p>The BaseParser provides a robust foundation for web scraping by: - Standardizing the parsing interface - Handling HTTP requests with error recovery - Leveraging Beautiful Soup for HTML parsing - Supporting both simple and complex extraction needs</p> <p>Whether scraping news articles or court records, understanding these fundamentals enables you to create effective parsers that extract structured data from any website.</p>"},{"location":"developer/ParserAppealsAL_documentation/","title":"ParserAppealsAL Documentation","text":""},{"location":"developer/ParserAppealsAL_documentation/#overview","title":"Overview","text":"<p><code>ParserAppealsAL</code> is a specialized parser designed to extract court case data from the Alabama Appeals Court Public Portal. Unlike traditional web scrapers that use simple HTTP requests, this parser employs Selenium WebDriver to handle JavaScript-rendered content, making it capable of extracting data from dynamic web applications.</p>"},{"location":"developer/ParserAppealsAL_documentation/#key-features","title":"Key Features","text":"<ul> <li>JavaScript Support: Uses Selenium WebDriver to render JavaScript-heavy pages</li> <li>Automatic Browser Management: Handles Chrome driver setup and teardown</li> <li>Rate Limiting: Built-in configurable delays between requests to avoid overwhelming the server</li> <li>Table Parsing: Specialized logic for extracting structured data from HTML tables</li> <li>Error Handling: Robust error handling with graceful fallbacks</li> <li>Headless Operation: Can run with or without a visible browser window</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#architecture","title":"Architecture","text":""},{"location":"developer/ParserAppealsAL_documentation/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser (Abstract Base Class)\n    \u2514\u2500\u2500 ParserAppealsAL\n</code></pre> <p>ParserAppealsAL inherits from the <code>BaseParser</code> base class, which defines the common interface for all parsers in the OPAL system. It overrides key methods to provide court-specific functionality.</p>"},{"location":"developer/ParserAppealsAL_documentation/#dependencies","title":"Dependencies","text":"<pre><code># Core Dependencies\nselenium &gt;= 4.0.0          # Browser automation\nwebdriver-manager &gt;= 4.0.0 # Automatic ChromeDriver management\nbeautifulsoup4            # HTML parsing\nrequests                  # HTTP requests (inherited from base)\n\n# Standard Library\njson                      # JSON data handling\ntime                      # Rate limiting\ndatetime                  # Timestamp generation\ntyping                    # Type hints\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#implementation-guide","title":"Implementation Guide","text":""},{"location":"developer/ParserAppealsAL_documentation/#1-basic-structure","title":"1. Basic Structure","text":"<p>To implement your own court parser based on ParserAppealsAL, start with this structure:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nfrom your_project.parser_module import BaseParser\n\nclass YourCourtParser(BaseParser):\n    def __init__(self, headless=True, rate_limit_seconds=3):\n        super().__init__()\n        self.headless = headless\n        self.rate_limit_seconds = rate_limit_seconds\n        self.driver = None\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#2-core-methods","title":"2. Core Methods","text":""},{"location":"developer/ParserAppealsAL_documentation/#__init__self-headless-bool-true-rate_limit_seconds-int-3","title":"<code>__init__(self, headless: bool = True, rate_limit_seconds: int = 3)</code>","text":"<p>Initializes the parser with configuration options.</p> <p>Parameters: - <code>headless</code>: Run Chrome in headless mode (no visible window) - <code>rate_limit_seconds</code>: Delay between requests to avoid rate limiting</p>"},{"location":"developer/ParserAppealsAL_documentation/#_setup_driverself","title":"<code>_setup_driver(self)</code>","text":"<p>Sets up the Chrome WebDriver with appropriate options:</p> <pre><code>def _setup_driver(self):\n    chrome_options = Options()\n    if self.headless:\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920,1080\")\n\n    service = Service(ChromeDriverManager().install())\n    self.driver = webdriver.Chrome(service=service, options=chrome_options)\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#make_requestself-url-str-timeout-int-30-optionalstr","title":"<code>make_request(self, url: str, timeout: int = 30) -&gt; Optional[str]</code>","text":"<p>Overrides the base class method to use Selenium instead of requests library.</p> <p>Key Features: - Lazy driver initialization - Waits for specific elements to load - Implements rate limiting - Returns page source HTML</p> <pre><code>def make_request(self, url, timeout=30):\n    if not self.driver:\n        self._setup_driver()\n\n    self.driver.get(url)\n\n    # Wait for your specific element\n    WebDriverWait(self.driver, timeout).until(\n        EC.presence_of_element_located((By.CSS_SELECTOR, \"table\"))\n    )\n\n    time.sleep(self.rate_limit_seconds)\n    return self.driver.page_source\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#parse_table_rowself-row-optionaldict","title":"<code>parse_table_row(self, row) -&gt; Optional[Dict]</code>","text":"<p>Extracts data from a single table row. This method is specific to the table structure of your court portal.</p> <p>Expected Table Structure: 1. Court Name 2. Case Number (with optional link) 3. Case Title 4. Classification 5. Filed Date 6. Status</p> <p>Returns: <pre><code>{\n    \"court\": \"Court of Civil Appeals\",\n    \"case_number\": {\n        \"text\": \"2230123\",\n        \"link\": \"/case/details/...\"\n    },\n    \"case_title\": \"Smith v. Jones\",\n    \"classification\": \"Civil\",\n    \"filed_date\": \"06/11/2024\",\n    \"status\": \"Active\"\n}\n</code></pre></p>"},{"location":"developer/ParserAppealsAL_documentation/#parse_articleself-url-str-dict","title":"<code>parse_article(self, url: str) -&gt; Dict</code>","text":"<p>Main parsing method that processes a single page of court results.</p> <p>Process: 1. Loads the page using <code>make_request</code> 2. Parses HTML with BeautifulSoup 3. Finds the main data table 4. Extracts data from each row 5. Returns structured results</p>"},{"location":"developer/ParserAppealsAL_documentation/#parse_all_casesself-base_url-str-page_urls-liststr-dict","title":"<code>parse_all_cases(self, base_url: str, page_urls: List[str]) -&gt; Dict</code>","text":"<p>Processes multiple pages of results and combines them.</p> <p>Returns: <pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        # List of case dictionaries\n    ]\n}\n</code></pre></p>"},{"location":"developer/ParserAppealsAL_documentation/#3-integration-with-opal-system","title":"3. Integration with OPAL System","text":"<p>The parser integrates with OPAL through the <code>IntegratedParser</code> class:</p> <pre><code>from opal.integrated_parser import IntegratedParser\nfrom your_parser import YourCourtParser\n\n# Create parser instance\nparser = IntegratedParser(YourCourtParser)\n\n# Process court data\nresult = parser.process_site(\n    base_url=\"https://your-court-portal.gov/search\",\n    suffix=\"\",  # Not used for court parsers\n    max_pages=None  # Will process all available pages\n)\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#4-url-pagination","title":"4. URL Pagination","text":"<p>Court portals often use complex URL parameters for pagination. The system includes helper functions in <code>court_url_paginator.py</code>:</p> <ul> <li><code>parse_court_url()</code>: Extracts page number and total pages from URL</li> <li><code>build_court_url()</code>: Constructs URLs for specific pages</li> <li><code>paginate_court_urls()</code>: Generates list of all page URLs</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#5-best-practices","title":"5. Best Practices","text":"<ol> <li>Error Handling: Always wrap operations in try-except blocks</li> <li>Resource Management: Ensure driver is closed in finally blocks</li> <li>Rate Limiting: Respect server limits to avoid IP bans</li> <li>Dynamic Waits: Use WebDriverWait instead of fixed sleep times when possible</li> <li>Memory Management: Close driver after processing to free resources</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#6-testing","title":"6. Testing","text":"<p>Create test scripts to validate your parser:</p> <pre><code>from your_parser import YourCourtParser\n\ndef test_single_page():\n    parser = YourCourtParser(headless=True)\n    result = parser.parse_article(\"https://court-url.gov/page1\")\n\n    assert result[\"cases\"]\n    assert len(result[\"cases\"]) &gt; 0\n\n    # Validate case structure\n    case = result[\"cases\"][0]\n    assert \"court\" in case\n    assert \"case_number\" in case\n    assert \"case_title\" in case\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#customization-guide","title":"Customization Guide","text":""},{"location":"developer/ParserAppealsAL_documentation/#adapting-for-different-court-systems","title":"Adapting for Different Court Systems","text":"<ol> <li>Table Structure: Modify <code>parse_table_row()</code> to match your court's table columns</li> <li>Wait Conditions: Update the element selector in <code>make_request()</code> </li> <li>URL Patterns: Adjust pagination logic in helper functions</li> <li>Data Fields: Add or remove fields based on available data</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#common-modifications","title":"Common Modifications","text":"<ol> <li> <p>Different Table Selectors: <pre><code># Instead of generic \"table\"\nWebDriverWait(self.driver, timeout).until(\n    EC.presence_of_element_located((By.ID, \"case-results-table\"))\n)\n</code></pre></p> </li> <li> <p>Additional Data Extraction: <pre><code># Add judge information if available\njudge = cells[6].get_text(strip=True) if len(cells) &gt; 6 else \"\"\n</code></pre></p> </li> <li> <p>Custom Headers: <pre><code># Some courts require authentication headers\nself.driver.add_cookie({\"name\": \"session\", \"value\": \"your-session-id\"})\n</code></pre></p> </li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/ParserAppealsAL_documentation/#common-issues","title":"Common Issues","text":"<ol> <li>ChromeDriver Not Found: </li> <li>Solution: webdriver-manager should handle this automatically</li> <li> <p>Manual fix: Download ChromeDriver matching your Chrome version</p> </li> <li> <p>Elements Not Loading:</p> </li> <li>Increase timeout in WebDriverWait</li> <li>Check if element selectors have changed</li> <li> <p>Verify JavaScript is executing properly</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Increase <code>rate_limit_seconds</code></li> <li>Implement exponential backoff</li> <li> <p>Consider using proxy rotation</p> </li> <li> <p>Memory Leaks:</p> </li> <li>Ensure driver is closed after use</li> <li>Implement periodic driver restarts for long runs</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Headless Mode: Significantly faster than visible browser</li> <li>Parallel Processing: Not recommended due to rate limits</li> <li>Caching: Consider caching parsed results to avoid re-parsing</li> <li>Resource Usage: Each driver instance uses ~100-200MB RAM</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#example-output","title":"Example Output","text":"<pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        {\n            \"court\": \"Court of Civil Appeals\",\n            \"case_number\": {\n                \"text\": \"CL-2024-000123\",\n                \"link\": \"/portal/case/details/123\"\n            },\n            \"case_title\": \"Smith v. Jones Corporation\",\n            \"classification\": \"Civil Appeal\",\n            \"filed_date\": \"01/10/2025\",\n            \"status\": \"Pending\"\n        },\n        {\n            \"court\": \"Court of Criminal Appeals\",\n            \"case_number\": {\n                \"text\": \"CR-2024-000456\",\n                \"link\": \"/portal/case/details/456\"\n            },\n            \"case_title\": \"State of Alabama v. Doe\",\n            \"classification\": \"Criminal Appeal\",\n            \"filed_date\": \"01/09/2025\",\n            \"status\": \"Active\"\n        }\n    ]\n}\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#security-considerations","title":"Security Considerations","text":"<ol> <li>Input Validation: Always validate URLs before processing</li> <li>Sandbox Mode: Chrome runs with --no-sandbox for compatibility</li> <li>Credential Storage: Never hardcode credentials in parser</li> <li>SSL Verification: Selenium handles SSL by default</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#future-enhancements","title":"Future Enhancements","text":"<p>Consider these improvements for production use:</p> <ol> <li>Retry Logic: Implement automatic retries for failed requests</li> <li>Progress Tracking: Add callbacks for progress updates</li> <li>Data Validation: Implement schema validation for parsed data</li> <li>Export Formats: Support multiple output formats (CSV, Excel)</li> <li>Incremental Updates: Track previously parsed cases to avoid duplicates</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/","title":"Alabama Appeals Court Public Portal Scraper - Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#overview","title":"Overview","text":"<p>Create a court case scraper extension for OPAL that extracts tabular data from the Alabama Appeals Court Public Portal. The scraper must handle JavaScript-rendered content, complex URL-based pagination, and preserve both text and link references.</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-by-step-implementation-instructions","title":"Step-by-Step Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-1-update-dependencies","title":"Step 1: Update Dependencies","text":"<p>Add the following to <code>requirements.txt</code> and <code>pyproject.toml</code>: - <code>selenium&gt;=4.0.0</code> or <code>playwright&gt;=1.40.0</code> (for JavaScript rendering) - <code>webdriver-manager&gt;=4.0.0</code> (if using Selenium for automatic driver management)</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-2-create-court-case-parser-module","title":"Step 2: Create Court Case Parser Module","text":"<p>Create a new file <code>opal/court_case_parser.py</code> with the following specifications:</p> <ol> <li>Import necessary libraries:</li> <li>Selenium/Playwright for JavaScript rendering</li> <li>BeautifulSoup for HTML parsing</li> <li> <p>Standard libraries for URL manipulation and JSON output</p> </li> <li> <p>Create <code>CourtCaseParser</code> class that extends <code>BaseParser</code>:</p> </li> <li>Override <code>make_request()</code> to use Selenium/Playwright instead of requests</li> <li>Implement JavaScript rendering with appropriate wait conditions</li> <li> <p>Add rate limiting (minimum 2-3 seconds between requests)</p> </li> <li> <p>Implement <code>parse_table_row()</code> method to extract:</p> </li> <li>Court name from <code>&lt;td class=\"text-start\"&gt;</code> (column 1)</li> <li>Case number text and href from <code>&lt;a href=\"/portal/court/...\"&gt;</code> (column 2)</li> <li>Case title from <code>&lt;td class=\"text-start\"&gt;</code> (column 3)</li> <li>Classification from <code>&lt;td class=\"text-start\"&gt;</code> (column 4)</li> <li>Filed date from <code>&lt;td class=\"text-start\"&gt;</code> (column 5)</li> <li>Open/Closed status from <code>&lt;td class=\"text-start\"&gt;</code> (column 6)</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-3-create-custom-url-pagination-handler","title":"Step 3: Create Custom URL Pagination Handler","text":"<p>Create <code>opal/court_url_paginator.py</code> with:</p> <ol> <li>URL parser function to:</li> <li>Extract and decode the complex URL parameters</li> <li>Identify current page number from <code>page~(number~X)</code></li> <li> <p>Extract total pages from <code>totalPages~X</code></p> </li> <li> <p>URL builder function to:</p> </li> <li>Take base URL and page number</li> <li>Update <code>page~(number~X)</code> parameter</li> <li>Maintain all other search parameters</li> <li> <p>Handle special encoding (<code>~</code>, <code>%2a2f</code>, etc.)</p> </li> <li> <p>Pagination iterator that:</p> </li> <li>Starts at page 0</li> <li>Continues until reaching <code>totalPages</code></li> <li>Yields properly formatted URLs for each page</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-4-implement-data-extraction-logic","title":"Step 4: Implement Data Extraction Logic","text":"<p>In <code>CourtCaseParser</code>, create <code>parse_all_cases()</code> method that:</p> <ol> <li>Initialize browser driver (Selenium/Playwright)</li> <li>Load first page and extract total pages from URL</li> <li>For each page:</li> <li>Navigate to page URL</li> <li>Wait for table to load (use explicit waits)</li> <li>Extract all table rows</li> <li>Parse each row using <code>parse_table_row()</code></li> <li>Store results with preserved link references</li> <li>Close browser driver when complete</li> <li>Return combined results from all pages as single dataset</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-5-define-output-format","title":"Step 5: Define Output Format","text":"<p>Structure the output JSON as: <pre><code>{\n  \"status\": \"success\",\n  \"total_cases\": 317,\n  \"extraction_date\": \"2025-06-11\",\n  \"cases\": [\n    {\n      \"court\": \"Alabama Supreme Court\",\n      \"case_number\": {\n        \"text\": \"SC-2025-0424\",\n        \"link\": \"/portal/court/68f021c4-6a44-4735-9a76-5360b2e8af13/case/d024d958-58a1-41c9-9fae-39c645c7977e\"\n      },\n      \"case_title\": \"Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.\",\n      \"classification\": \"Appeal - Civil - Injunction Other\",\n      \"filed_date\": \"06/10/2025\",\n      \"status\": \"Open\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-6-integrate-with-opal-cli","title":"Step 6: Integrate with OPAL CLI","text":"<p>Modify existing OPAL files:</p> <ol> <li>Update <code>opal/__init__.py</code>:</li> <li>Add <code>from .court_case_parser import CourtCaseParser</code></li> <li> <p>Add <code>from .court_url_paginator import paginate_court_urls</code></p> </li> <li> <p>Update <code>opal/integrated_parser.py</code>:</p> </li> <li>Add conditional logic to handle court case URLs differently</li> <li> <p>Use <code>paginate_court_urls</code> instead of <code>get_all_news_urls</code> for court sites</p> </li> <li> <p>Update <code>opal/main.py</code>:</p> </li> <li>Add <code>--parser court</code> option to argparse choices</li> <li>Add court parser to the parser selection logic</li> <li>Adjust output filename format for court data</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-7-handle-technical-requirements","title":"Step 7: Handle Technical Requirements","text":"<p>Implement the following in <code>CourtCaseParser</code>:</p> <ol> <li>JavaScript rendering:</li> <li>Wait for table element to be present</li> <li>Wait for data rows to load</li> <li> <p>Handle any loading spinners or dynamic content</p> </li> <li> <p>Error handling:</p> </li> <li>Timeout exceptions for slow page loads</li> <li>Missing table elements</li> <li>Network errors</li> <li> <p>Browser crashes</p> </li> <li> <p>Rate limiting:</p> </li> <li>Add configurable delay between page requests (default 3 seconds)</li> <li>Respect server response times</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-8-testing-urls","title":"Step 8: Testing URLs","text":"<p>Use these URLs for testing: - First page: <code>https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</code> - Second page: Same URL but with <code>page~(number~1)</code> and updated <code>totalElements~317~totalPages~13</code></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-9-final-integration","title":"Step 9: Final Integration","text":"<ol> <li>Test the complete flow with: <code>python -m opal --url [court_url] --parser court</code></li> <li>Ensure output file is created with court case data in tabular format</li> <li>Verify all pages are scraped and combined into single result set</li> <li>Confirm case number links are preserved in the output</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#expected-deliverables","title":"Expected Deliverables","text":"<ol> <li><code>opal/court_case_parser.py</code> - Main parser for court data</li> <li><code>opal/court_url_paginator.py</code> - URL pagination handler</li> <li>Updated <code>opal/__init__.py</code>, <code>opal/integrated_parser.py</code>, and <code>opal/main.py</code></li> <li>Updated <code>requirements.txt</code> and <code>pyproject.toml</code> with new dependencies</li> <li>JSON output file with all court cases in structured format</li> </ol>"},{"location":"getting-started/installation.md/installation/","title":"Installation","text":""},{"location":"getting-started/installation.md/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.6 or higher</li> <li>Chrome browser (for court parser)</li> </ul>"},{"location":"getting-started/installation.md/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/alabama-forward/opal_beautifulsoup\ncd opal\n\n#Install dependencies\npip install -r requirements.txt\n\n#Install OPAL\npip install -e .\n</code></pre>"},{"location":"getting-started/installation.md/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>#Check OPAL is installed\npython -m opal --help\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/","title":"Setting Up MkDocs for OPAL","text":"<p>This guide will help you set up MkDocs to create a beautiful documentation website for your OPAL project.</p>"},{"location":"guides/mkdocs_setup_guide/#1-install-mkdocs-and-theme","title":"1. Install MkDocs and Theme","text":"<pre><code># Install MkDocs and the popular Material theme\npip install mkdocs mkdocs-material mkdocs-material-extensions\n\n# Or add to your requirements.txt:\nmkdocs&gt;=1.5.0\nmkdocs-material&gt;=9.0.0\nmkdocs-material-extensions&gt;=1.3.0\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/#2-create-mkdocs-configuration","title":"2. Create MkDocs Configuration","text":"<p>Create a <code>mkdocs.yml</code> file in your project root:</p> <pre><code>site_name: OPAL - Oppositional Positions in Alabama\nsite_description: Web scraper for Alabama news sites and court records\nsite_author: Gabriel Cab\u00e1n Cubero\nsite_url: https://yourusername.github.io/opal\n\nrepo_name: opal\nrepo_url: https://github.com/yourusername/opal\nedit_uri: edit/main/docs/\n\ntheme:\n  name: material\n  palette:\n    # Light mode\n    - scheme: default\n      primary: blue\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    # Dark mode\n    - scheme: slate\n      primary: blue\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.top\n    - search.suggest\n    - search.highlight\n    - content.code.copy\n\nplugins:\n  - search\n  - autorefs\n\nmarkdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - admonition\n  - pymdownx.details\n  - pymdownx.tabbed:\n      alternate_style: true\n  - toc:\n      permalink: true\n\nnav:\n  - Home: index.md\n  - Getting Started:\n      - Installation: getting-started/installation.md\n      - Quick Start: getting-started/quickstart.md\n      - Configuration: getting-started/configuration.md\n  - User Guide:\n      - CLI Usage: user-guide/cli-usage.md\n      - Available Parsers: user-guide/parsers.md\n      - Output Formats: user-guide/output-formats.md\n  - Developer Guide:\n      - Architecture: developer/architecture.md\n      - BaseParser Guide: developer/BaseParser_web_scraping_guide.md\n      - Creating New Parsers: developer/creating-parsers.md\n      - ParserAppealsAL Documentation: developer/ParserAppealsAL_documentation.md\n  - API Reference:\n      - BaseParser: api/base-parser.md\n      - Parser1819: api/parser-1819.md\n      - ParserDailyNews: api/parser-daily-news.md\n      - ParserAppealsAL: api/parser-appeals-al.md\n  - About:\n      - Contributing: about/contributing.md\n      - License: about/license.md\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/#3-create-documentation-structure","title":"3. Create Documentation Structure","text":"<pre><code># Create the docs directory structure\nmkdir -p docs/getting-started\nmkdir -p docs/user-guide\nmkdir -p docs/developer\nmkdir -p docs/api\nmkdir -p docs/about\n\n# Move existing documentation (don't delete originals yet)\ncp docs/BaseParser_web_scraping_guide.md docs/developer/\ncp docs/ParserAppealsAL_documentation.md docs/developer/\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/#4-create-the-homepage","title":"4. Create the Homepage","text":"<p>Create <code>docs/index.md</code>:</p> <pre><code># OPAL - Oppositional Positions in Alabama\n\nWelcome to OPAL's documentation! OPAL is a web scraping tool that extracts content from Alabama news sites and court records.\n\n## Features\n\n- \ud83d\udcf0 **Multiple News Sources** - Parse articles from 1819news.com and Alabama Daily News\n- \u2696\ufe0f **Court Records** - Extract data from Alabama Appeals Court Public Portal\n- \ud83d\udd27 **Extensible Architecture** - Easy to add new parsers\n- \ud83d\udcca **Structured Output** - Clean JSON format for analysis\n- \ud83d\ude80 **CLI Tool** - Simple command-line interface\n\n## Quick Start\n\n```bash\n# Install OPAL\npip install -e .\n\n# Scrape news articles\npython -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5\n\n# Scrape court cases\npython -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/#documentation-overview","title":"Documentation Overview","text":"<ul> <li>Getting Started - Installation and setup</li> <li>User Guide - How to use OPAL</li> <li>Developer Guide - Extend OPAL with new parsers</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"guides/mkdocs_setup_guide/#built-by-alabama-forward","title":"Built by Alabama Forward","text":"<p>This project was created by Gabriel Cab\u00e1n Cubero, Data Director at Alabama Forward. <pre><code>## 5. Create Basic Documentation Files\n\nCreate `docs/getting-started/installation.md`:\n\n```markdown\n# Installation\n\n## Requirements\n\n- Python 3.6 or higher\n- Chrome browser (for court parser)\n\n## Install from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/opal.git\ncd opal\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install OPAL\npip install -e .\n</code></pre></p>"},{"location":"guides/mkdocs_setup_guide/#verify-installation","title":"Verify Installation","text":"<p><pre><code># Check OPAL is installed\npython -m opal --help\n</code></pre> <pre><code>## 6. Build and Serve Documentation\n\n```bash\n# Serve locally for development (hot reload)\nmkdocs serve\n\n# Build static site\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre></p>"},{"location":"guides/mkdocs_setup_guide/#7-github-pages-setup","title":"7. GitHub Pages Setup","text":"<ol> <li>Go to your repository settings on GitHub</li> <li>Navigate to \"Pages\"</li> <li>Set source to \"Deploy from a branch\"</li> <li>Select <code>gh-pages</code> branch and <code>/ (root)</code> folder</li> <li>Your site will be available at <code>https://yourusername.github.io/opal</code></li> </ol>"},{"location":"guides/mkdocs_setup_guide/#8-add-to-gitignore","title":"8. Add to .gitignore","text":"<pre><code>echo \"site/\" &gt;&gt; .gitignore  # MkDocs build output\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/#9-useful-commands","title":"9. Useful Commands","text":"<pre><code># Local development server (http://127.0.0.1:8000)\nmkdocs serve\n\n# Build documentation\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n\n# Get help\nmkdocs --help\n</code></pre>"},{"location":"guides/mkdocs_setup_guide/#10-advanced-features","title":"10. Advanced Features","text":"<p>You can add: - Code syntax highlighting with language-specific formatting - Mermaid diagrams for architecture visualization - API documentation auto-generated from docstrings - Search functionality (included by default) - PDF export with additional plugins - Multiple languages support</p>"},{"location":"guides/mkdocs_setup_guide/#next-steps","title":"Next Steps","text":"<ol> <li>Install MkDocs: Run <code>pip install mkdocs mkdocs-material</code></li> <li>Create mkdocs.yml: Copy the configuration from step 2</li> <li>Organize docs: Move your existing markdown files to the appropriate folders</li> <li>Run locally: Use <code>mkdocs serve</code> to preview your documentation</li> <li>Deploy: Use <code>mkdocs gh-deploy</code> to publish to GitHub Pages</li> </ol>"},{"location":"guides/mkdocs_setup_guide/#tips","title":"Tips","text":"<ul> <li>Keep documentation close to code - update docs when you change features</li> <li>Use meaningful file names that match your navigation structure</li> <li>Include code examples and real-world usage scenarios</li> <li>Add screenshots for complex features</li> <li>Keep a changelog to track documentation updates</li> </ul>"},{"location":"guides/mkdocs_setup_guide/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>MkDocs Plugins</li> </ul>"},{"location":"guides/user_agent_headers_guide/","title":"User-Agent Headers Guide","text":""},{"location":"guides/user_agent_headers_guide/#what-is-a-user-agent","title":"What is a User-Agent?","text":"<p>User-Agent headers are strings that identify the client (browser, bot, or application) making an HTTP request to a web server.</p> <p>It's an HTTP header that tells the server: - What software is making the request - What version it is - What operating system it's running on</p>"},{"location":"guides/user_agent_headers_guide/#examples-of-user-agent-strings","title":"Examples of User-Agent Strings","text":""},{"location":"guides/user_agent_headers_guide/#chrome-browser","title":"Chrome Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#python-requests-default","title":"Python Requests (default)","text":"<pre><code>python-requests/2.28.0\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#googlebot","title":"Googlebot","text":"<pre><code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#firefox-browser","title":"Firefox Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#safari-browser","title":"Safari Browser","text":"<pre><code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#why-user-agents-matter","title":"Why User-Agents Matter","text":"<ol> <li>Server Behavior: Websites may serve different content based on User-Agent</li> <li>Access Control: Some sites block requests with suspicious or missing User-Agents</li> <li>Analytics: Helps websites understand their traffic</li> <li>Bot Detection: Sites use it to identify and potentially block scrapers</li> <li>Content Optimization: Sites may serve mobile vs desktop versions</li> </ol>"},{"location":"guides/user_agent_headers_guide/#setting-user-agent-in-python","title":"Setting User-Agent in Python","text":""},{"location":"guides/user_agent_headers_guide/#basic-example","title":"Basic Example","text":"<pre><code>import requests\n\n# Without User-Agent (might be blocked)\nresponse = requests.get('https://example.com')\n\n# With User-Agent (appears as a browser)\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#advanced-example-with-multiple-headers","title":"Advanced Example with Multiple Headers","text":"<pre><code>headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Accept-Encoding': 'gzip, deflate',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n}\n\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#common-user-agent-patterns","title":"Common User-Agent Patterns","text":""},{"location":"guides/user_agent_headers_guide/#desktop-browsers","title":"Desktop Browsers","text":"<pre><code># Windows Chrome\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n\n# macOS Safari\n'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'\n\n# Linux Firefox\n'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#mobile-browsers","title":"Mobile Browsers","text":"<pre><code># iPhone Safari\n'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n\n# Android Chrome\n'Mozilla/5.0 (Linux; Android 11; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36'\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#custom-bot-honest-approach","title":"Custom Bot (Honest Approach)","text":"<pre><code>'OPAL-Bot/1.0 (+https://github.com/yourusername/opal)'\n'MyCompany-Scraper/2.1 (contact@mycompany.com)'\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#implementing-user-agents-in-opal","title":"Implementing User-Agents in OPAL","text":""},{"location":"guides/user_agent_headers_guide/#enhanced-baseparser","title":"Enhanced BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers with proper headers\"\"\"\n\n    # Realistic browser headers\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.9',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'DNT': '1',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n    }\n\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, headers=headers, timeout=5)\n            response.raise_for_status()\n            responses.append(response.text)\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n\n    return responses, successful_urls\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#user-agent-rotation","title":"User-Agent Rotation","text":"<pre><code>import random\n\nclass RotatingUserAgentParser(BaseParser):\n    def __init__(self):\n        super().__init__()\n        self.user_agents = [\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0',\n        ]\n\n    def get_random_user_agent(self):\n        return random.choice(self.user_agents)\n\n    def make_request(self, urls):\n        # Use different User-Agent for each request\n        headers = {'User-Agent': self.get_random_user_agent()}\n        # ... rest of request logic\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#best-practices","title":"Best Practices","text":""},{"location":"guides/user_agent_headers_guide/#1-be-strategic","title":"1. Be Strategic","text":"<ul> <li>Use real User-Agents: Copy from actual browsers</li> <li>Stay current: Browser versions change frequently</li> <li>Match behavior: If you claim to be Chrome, act like Chrome</li> </ul>"},{"location":"guides/user_agent_headers_guide/#2-be-respectful","title":"2. Be Respectful","text":"<ul> <li>Respect robots.txt: Even with a browser User-Agent</li> <li>Rate limit: Don't overwhelm servers</li> <li>Be honest when possible: Some sites appreciate transparent bots</li> </ul>"},{"location":"guides/user_agent_headers_guide/#3-be-consistent","title":"3. Be Consistent","text":"<ul> <li>Use complete headers: Include Accept, Accept-Language, etc.</li> <li>Maintain session: Use the same User-Agent throughout a session</li> <li>Handle responses: Check if the site is behaving differently</li> </ul>"},{"location":"guides/user_agent_headers_guide/#4-be-prepared","title":"4. Be Prepared","text":"<ul> <li>Rotate User-Agents: Avoid detection patterns</li> <li>Handle blocks: Have fallback strategies</li> <li>Monitor changes: Sites may update their detection methods</li> </ul>"},{"location":"guides/user_agent_headers_guide/#user-agent-detection-techniques","title":"User-Agent Detection Techniques","text":"<p>Websites can detect fake User-Agents by:</p> <ol> <li>Header Analysis: Checking if browser behavior matches the User-Agent</li> <li>Missing Headers: Looking for headers real browsers always send</li> <li>JavaScript Testing: Testing browser capabilities that match the claimed version</li> <li>Request Patterns: Analyzing timing and request sequences</li> <li>Feature Detection: Checking for browser-specific features</li> </ol>"},{"location":"guides/user_agent_headers_guide/#common-mistakes","title":"Common Mistakes","text":""},{"location":"guides/user_agent_headers_guide/#1-outdated-user-agents","title":"1. Outdated User-Agents","text":"<pre><code># Bad - very old browser version\n'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 Chrome/45.0.2454.85'\n\n# Good - recent browser version\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/91.0.4472.124'\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#2-inconsistent-headers","title":"2. Inconsistent Headers","text":"<pre><code># Bad - claims to be Chrome but uses Firefox Accept header\nheaders = {\n    'User-Agent': 'Chrome/91.0.4472.124',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'  # Firefox style\n}\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#3-missing-common-headers","title":"3. Missing Common Headers","text":"<pre><code># Bad - only User-Agent\nheaders = {'User-Agent': 'Mozilla/5.0...'}\n\n# Good - realistic browser headers\nheaders = {\n    'User-Agent': 'Mozilla/5.0...',\n    'Accept': 'text/html,application/xhtml+xml...',\n    'Accept-Language': 'en-US,en;q=0.9',\n    'Accept-Encoding': 'gzip, deflate, br',\n}\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#testing-user-agents","title":"Testing User-Agents","text":""},{"location":"guides/user_agent_headers_guide/#check-what-youre-sending","title":"Check What You're Sending","text":"<pre><code>import requests\n\n# Test your headers\nresponse = requests.get('https://httpbin.org/headers', headers=your_headers)\nprint(response.json())\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#verify-server-response","title":"Verify Server Response","text":"<pre><code># Check if the site is treating you differently\nresponse_bot = requests.get(url)  # Default requests User-Agent\nresponse_browser = requests.get(url, headers=browser_headers)\n\nif response_bot.content != response_browser.content:\n    print(\"Site serves different content based on User-Agent\")\n</code></pre>"},{"location":"guides/user_agent_headers_guide/#tools-and-resources","title":"Tools and Resources","text":"<ul> <li>Browser DevTools: Copy real User-Agent strings from Network tab</li> <li>User-Agent Databases: Sites like whatismybrowser.com</li> <li>Header Checkers: Use httpbin.org to test your headers</li> <li>Browser Testing: Use Selenium to see what real browsers send</li> </ul>"},{"location":"guides/user_agent_headers_guide/#conclusion","title":"Conclusion","text":"<p>User-Agent headers are a crucial part of web scraping that can mean the difference between successful data extraction and being blocked. Use them thoughtfully and responsibly to build robust scrapers that respect both the technical and ethical aspects of web crawling.</p>"}]}