{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OPAL - Oppositional Positions in ALabama","text":"<p>Welcome to OPAL's documentation! OPAL is a web scraping tool that extracts content from websites like Alabama news sites and court records.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udcf0 Multiple News Sources - Parse articles from 1819news.com and Alabama Daily News</li> <li>\u2696\ufe0f Court Records - Extract data from Alabama Appeals Court Public Portal</li> <li>\ud83d\udd27 Extensible Architecture - Easy to add new parsers</li> <li>\ud83d\udcca Structured Output - Clean JSON format for analysis</li> <li>\ud83d\ude80 CLI Tool - Simple command-line interface</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>```bash</p>"},{"location":"#install-opal","title":"Install OPAL","text":"<p>pip install -e .</p>"},{"location":"#scrape-news-articles","title":"Scrape news articles","text":"<p>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5</p>"},{"location":"#scrape-court-cases","title":"Scrape court cases","text":"<p>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court</p>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li>Getting Started - Installation and setup</li> <li>User Guide - How to use OPAL</li> <li>Developer Guide - Extend Opal with new parsers</li> </ul>"},{"location":"#built-by-alabama-forward","title":"Built by Alabama Forward","text":"<p>This project was created by Gabriel Cab\u00e1n Cubero, Data Director at Alabama Forward.</p>"},{"location":"about/contributing/","title":"Contributing to OPAL","text":"<p>We welcome contributions to OPAL! This guide will help you get started.</p>"},{"location":"about/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/yourusername/opal.git\ncd opal\n</code></pre></li> <li>Create a new branch for your feature:    <pre><code>git checkout -b feature-name\n</code></pre></li> </ol>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\npip install -e .\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install pytest black flake8\n</code></pre></p> </li> </ol>"},{"location":"about/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"about/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add docstrings to all functions and classes</li> <li>Keep lines under 88 characters (Black's default)</li> </ul>"},{"location":"about/contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new features</li> <li>Ensure all tests pass before submitting</li> <li>Run tests with: <code>pytest tests/</code></li> </ul>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for new features</li> <li>Include docstrings in your code</li> <li>Update README if needed</li> </ul>"},{"location":"about/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"about/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code follows the style guidelines</li> <li>Update documentation as needed</li> <li>Add tests for new functionality</li> <li>Ensure all tests pass</li> <li>Update the CHANGELOG.md with your changes</li> </ol>"},{"location":"about/contributing/#adding-new-parsers","title":"Adding New Parsers","text":"<p>When contributing a new parser:</p> <ol> <li>Follow the BaseParser structure</li> <li>Include comprehensive error handling</li> <li>Add documentation to the parser class</li> <li>Create an example in the documentation</li> <li>Test with various edge cases</li> </ol>"},{"location":"about/contributing/#reporting-issues","title":"Reporting Issues","text":"<ul> <li>Use GitHub Issues to report bugs</li> <li>Include detailed reproduction steps</li> <li>Provide error messages and logs</li> <li>Specify your Python version and OS</li> </ul>"},{"location":"about/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers and help them get started</li> <li>Focus on constructive criticism</li> <li>Respect differing viewpoints</li> </ul>"},{"location":"about/contributing/#questions","title":"Questions?","text":"<p>Feel free to open an issue for any questions about contributing!</p>"},{"location":"about/license/","title":"License","text":"<p>OPAL is released under the MIT License.</p>"},{"location":"about/license/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2024 Gabriel Cab\u00e1n Cubero / Alabama Forward</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/license/#what-this-means","title":"What this means","text":"<ul> <li>You can use OPAL for commercial and non-commercial purposes</li> <li>You can modify the code to suit your needs</li> <li>You can distribute the software</li> <li>You must include the copyright notice in copies</li> <li>The software is provided \"as is\" without warranty</li> </ul>"},{"location":"about/license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>OPAL uses the following open-source libraries:</p> <ul> <li>BeautifulSoup4 - MIT License</li> <li>Requests - Apache License 2.0</li> <li>Selenium - Apache License 2.0</li> <li>MkDocs - BSD License</li> <li>Material for MkDocs - MIT License</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to OPAL, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"api/base-parser/","title":"BaseParser API Reference","text":"<p>The <code>BaseParser</code> class provides the foundation for all OPAL parsers.</p>"},{"location":"api/base-parser/#class-definition","title":"Class Definition","text":"<pre><code>class BaseParser:\n    def __init__(self, url, suffix=\"\", max_pages=5)\n</code></pre>"},{"location":"api/base-parser/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str required Base URL to scrape <code>suffix</code> str <code>\"\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"api/base-parser/#attributes","title":"Attributes","text":""},{"location":"api/base-parser/#headers","title":"headers","text":"<p><pre><code>self.headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\n</code></pre> HTTP headers for requests</p>"},{"location":"api/base-parser/#logger","title":"logger","text":"<p><pre><code>self.logger: logging.Logger\n</code></pre> Logger instance for debugging</p>"},{"location":"api/base-parser/#methods","title":"Methods","text":""},{"location":"api/base-parser/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Main method to extract articles. Must be implemented by subclasses.</p> <p>Returns: List of dictionaries containing article data</p>"},{"location":"api/base-parser/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extract article URLs from a page. Must be implemented by subclasses.</p> <p>Parameters: - <code>page_url</code>: URL of the page to extract links from</p> <p>Returns: List of article URLs</p>"},{"location":"api/base-parser/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parse individual article data. Must be implemented by subclasses.</p> <p>Parameters: - <code>article_url</code>: URL of the article to parse</p> <p>Returns: Dictionary with article data</p>"},{"location":"api/base-parser/#save_to_json","title":"save_to_json()","text":"<p><pre><code>def save_to_json(self, data: List[Dict], filename: str = \"opal_output.json\")\n</code></pre> Save extracted data to JSON file.</p> <p>Parameters: - <code>data</code>: List of article dictionaries - <code>filename</code>: Output filename</p>"},{"location":"api/base-parser/#usage-example","title":"Usage Example","text":"<pre><code>from opal.BaseParser import BaseParser\n\nclass MyParser(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"MyParser\"\n\n    def extract_article_data(self):\n        # Implementation\n        pass\n\n    def get_article_links(self, page_url):\n        # Implementation\n        pass\n\n    def parse_article(self, article_url):\n        # Implementation\n        pass\n\n# Use the parser\nparser = MyParser(\"https://example.com\", suffix=\"/articles\", max_pages=10)\ndata = parser.extract_article_data()\nparser.save_to_json(data)\n</code></pre>"},{"location":"api/parser-1819/","title":"Parser1819 API Reference","text":"<p>Parser for 1819 News website.</p>"},{"location":"api/parser-1819/#class-definition","title":"Class Definition","text":"<pre><code>class Parser1819(BaseParser):\n    def __init__(self, url=\"https://1819news.com/\", suffix=\"/news/item\", max_pages=5)\n</code></pre>"},{"location":"api/parser-1819/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str <code>\"https://1819news.com/\"</code> Base URL for 1819 News <code>suffix</code> str <code>\"/news/item\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"api/parser-1819/#methods","title":"Methods","text":""},{"location":"api/parser-1819/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Extracts articles from multiple pages of 1819 News.</p> <p>Returns: List of article dictionaries with keys: - <code>title</code>: Article headline - <code>content</code>: Full article text - <code>date</code>: Publication date - <code>author</code>: Article author - <code>url</code>: Article URL - <code>tags</code>: List of article tags</p>"},{"location":"api/parser-1819/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extracts article URLs from a 1819 News page.</p> <p>Parameters: - <code>page_url</code>: URL of the page to scrape</p> <p>Returns: List of article URLs matching the suffix pattern</p>"},{"location":"api/parser-1819/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parses individual 1819 News article.</p> <p>Parameters: - <code>article_url</code>: URL of the article</p> <p>Returns: Dictionary with article data</p>"},{"location":"api/parser-1819/#usage-example","title":"Usage Example","text":"<pre><code>from opal.Parser1819 import Parser1819\n\n# Create parser instance\nparser = Parser1819(\n    url=\"https://1819news.com/\",\n    suffix=\"/news/item\",\n    max_pages=10\n)\n\n# Extract articles\narticles = parser.extract_article_data()\n\n# Save to JSON\nparser.save_to_json(articles, \"1819_news_articles.json\")\n\n# Access article data\nfor article in articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"Date: {article['date']}\")\n    print(f\"Author: {article['author']}\")\n</code></pre>"},{"location":"api/parser-1819/#output-format","title":"Output Format","text":"<pre><code>{\n  \"title\": \"Alabama Legislature Passes New Education Bill\",\n  \"content\": \"Full article text...\",\n  \"date\": \"2024-01-15\",\n  \"author\": \"John Smith\",\n  \"url\": \"https://1819news.com/news/item/education-bill-2024\",\n  \"tags\": [\"education\", \"legislature\", \"alabama\"]\n}\n</code></pre>"},{"location":"api/parser-1819/#notes","title":"Notes","text":"<ul> <li>Uses BeautifulSoup for HTML parsing</li> <li>Handles pagination automatically</li> <li>Filters links by suffix to ensure only articles are scraped</li> <li>Includes error handling for missing elements</li> </ul>"},{"location":"api/parser-appeals-al/","title":"ParserAppealsAL API Reference","text":"<p>Parser for Alabama Appeals Court Public Portal.</p>"},{"location":"api/parser-appeals-al/#class-definition","title":"Class Definition","text":"<pre><code>class ParserAppealsAL(BaseParser):\n    def __init__(self, url=\"https://publicportal.alappeals.gov/portal/search/case/results\")\n</code></pre>"},{"location":"api/parser-appeals-al/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str Portal URL Base URL for court portal"},{"location":"api/parser-appeals-al/#methods","title":"Methods","text":""},{"location":"api/parser-appeals-al/#setup_driver","title":"setup_driver()","text":"<p><pre><code>def setup_driver(self) -&gt; webdriver.Chrome\n</code></pre> Sets up Chrome WebDriver with appropriate options.</p> <p>Returns: Configured Chrome WebDriver instance</p>"},{"location":"api/parser-appeals-al/#extract_court_data","title":"extract_court_data()","text":"<p><pre><code>def extract_court_data(self) -&gt; List[Dict]\n</code></pre> Main method to extract court case data.</p> <p>Returns: List of court case dictionaries</p>"},{"location":"api/parser-appeals-al/#parse_case_row","title":"parse_case_row()","text":"<p><pre><code>def parse_case_row(self, row: WebElement) -&gt; Dict\n</code></pre> Parses individual case row from search results.</p> <p>Parameters: - <code>row</code>: Selenium WebElement representing a case row</p> <p>Returns: Dictionary with case information</p>"},{"location":"api/parser-appeals-al/#get_case_details","title":"get_case_details()","text":"<p><pre><code>def get_case_details(self, case_url: str) -&gt; Dict\n</code></pre> Fetches detailed information for a specific case.</p> <p>Parameters: - <code>case_url</code>: URL to the case details page</p> <p>Returns: Dictionary with detailed case information</p>"},{"location":"api/parser-appeals-al/#usage-example","title":"Usage Example","text":"<pre><code>from opal.ParserAppealsAL import ParserAppealsAL\n\n# Create parser instance\nparser = ParserAppealsAL()\n\n# Extract court cases\ncases = parser.extract_court_data()\n\n# Save to JSON\nparser.save_to_json(cases, \"court_cases.json\")\n\n# Process cases\nfor case in cases:\n    print(f\"Case: {case['case_number']}\")\n    print(f\"Title: {case['case_title']}\")\n    print(f\"Status: {case['status']}\")\n</code></pre>"},{"location":"api/parser-appeals-al/#output-format","title":"Output Format","text":"<pre><code>{\n  \"case_number\": \"2024-CV-001234\",\n  \"case_title\": \"State of Alabama v. John Doe\",\n  \"court\": \"Alabama Court of Civil Appeals\",\n  \"date_filed\": \"2024-01-10\",\n  \"status\": \"Active\",\n  \"judge\": \"Hon. Jane Smith\",\n  \"parties\": {\n    \"appellant\": \"John Doe\",\n    \"appellee\": \"State of Alabama\"\n  },\n  \"attorneys\": [\n    {\n      \"name\": \"James Johnson\",\n      \"role\": \"Attorney for Appellant\"\n    }\n  ],\n  \"docket_entries\": [\n    {\n      \"date\": \"2024-01-10\",\n      \"description\": \"Notice of Appeal Filed\",\n      \"document_url\": \"https://publicportal.alappeals.gov/document/12345\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/parser-appeals-al/#special-features","title":"Special Features","text":""},{"location":"api/parser-appeals-al/#selenium-webdriver","title":"Selenium WebDriver","text":"<ul> <li>Automatically installs ChromeDriver</li> <li>Handles JavaScript-rendered content</li> <li>Supports dynamic page interactions</li> </ul>"},{"location":"api/parser-appeals-al/#error-handling","title":"Error Handling","text":"<ul> <li>Retries failed page loads</li> <li>Handles stale element exceptions</li> <li>Logs detailed error information</li> </ul>"},{"location":"api/parser-appeals-al/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Includes delays between requests</li> <li>Respects server load</li> </ul>"},{"location":"api/parser-appeals-al/#notes","title":"Notes","text":"<ul> <li>Requires Chrome browser installed</li> <li>Uses Selenium for JavaScript support</li> <li>Handles pagination automatically</li> <li>Extracts both case list and detailed case information</li> <li>Includes comprehensive error handling for web automation</li> </ul>"},{"location":"api/parser-daily-news/","title":"ParserDailyNews API Reference","text":"<p>Parser for Alabama Daily News website.</p>"},{"location":"api/parser-daily-news/#class-definition","title":"Class Definition","text":"<pre><code>class ParserDailyNews(BaseParser):\n    def __init__(self, url=\"https://www.aldailynews.com/\", suffix=\"/news/item\", max_pages=5)\n</code></pre>"},{"location":"api/parser-daily-news/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str <code>\"https://www.aldailynews.com/\"</code> Base URL for Alabama Daily News <code>suffix</code> str <code>\"/news/item\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"api/parser-daily-news/#methods","title":"Methods","text":""},{"location":"api/parser-daily-news/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Extracts articles from Alabama Daily News.</p> <p>Returns: List of article dictionaries with keys: - <code>title</code>: Article headline - <code>content</code>: Full article text - <code>date</code>: Publication date - <code>author</code>: Article author - <code>url</code>: Article URL - <code>category</code>: Article category</p>"},{"location":"api/parser-daily-news/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extracts article URLs from an Alabama Daily News page.</p> <p>Parameters: - <code>page_url</code>: URL of the page to scrape</p> <p>Returns: List of article URLs</p>"},{"location":"api/parser-daily-news/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parses individual Alabama Daily News article.</p> <p>Parameters: - <code>article_url</code>: URL of the article</p> <p>Returns: Dictionary with article data</p>"},{"location":"api/parser-daily-news/#usage-example","title":"Usage Example","text":"<pre><code>from opal.ParserDailyNews import ParserDailyNews\n\n# Create parser instance\nparser = ParserDailyNews(\n    url=\"https://www.aldailynews.com/\",\n    suffix=\"/news/item\",\n    max_pages=5\n)\n\n# Extract articles\narticles = parser.extract_article_data()\n\n# Save to JSON\nparser.save_to_json(articles, \"adn_articles.json\")\n\n# Process articles\nfor article in articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"Category: {article['category']}\")\n    print(f\"Date: {article['date']}\")\n</code></pre>"},{"location":"api/parser-daily-news/#output-format","title":"Output Format","text":"<pre><code>{\n  \"title\": \"Governor Announces Infrastructure Plan\",\n  \"content\": \"Full article text...\",\n  \"date\": \"2024-01-15\",\n  \"author\": \"Jane Doe\",\n  \"url\": \"https://www.aldailynews.com/news/item/infrastructure-plan\",\n  \"category\": \"Politics\"\n}\n</code></pre>"},{"location":"api/parser-daily-news/#notes","title":"Notes","text":"<ul> <li>Handles Alabama Daily News specific HTML structure</li> <li>Extracts category information when available</li> <li>Supports pagination through page parameters</li> <li>Includes robust error handling for missing elements</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/","title":"BaseParser: Web Scraping Fundamentals Guide","text":""},{"location":"developer/BaseParser_web_scraping_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Core Concepts</li> <li>HTTP Requests</li> <li>HTML Parsing</li> <li>Beautiful Soup</li> <li>BaseParser Architecture</li> <li>Implementation Examples</li> <li>Best Practices</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#introduction","title":"Introduction","text":"<p>The <code>BaseParser</code> class is the foundation of OPAL's web scraping system. It provides a standardized interface for extracting structured data from websites, whether they're news articles or court records. This guide explains the core concepts behind web scraping and how BaseParser implements them.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#core-concepts","title":"Core Concepts","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-is-web-scraping","title":"What is Web Scraping?","text":"<p>Web scraping is the process of extracting data from websites programmatically. It involves:</p> <ol> <li>Fetching - Downloading the HTML content from a web server</li> <li>Parsing - Analyzing the HTML structure to find specific data</li> <li>Extracting - Pulling out the desired information</li> <li>Structuring - Organizing the data into a useful format (like JSON)</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#the-web-scraping-pipeline","title":"The Web Scraping Pipeline","text":"<pre><code>URL \u2192 HTTP Request \u2192 HTML Response \u2192 Parse HTML \u2192 Extract Data \u2192 Structure Output\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#http-requests","title":"HTTP Requests","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-are-http-requests","title":"What are HTTP Requests?","text":"<p>HTTP (HyperText Transfer Protocol) requests are how programs communicate with web servers. When you visit a website, your browser sends an HTTP request to the server, which responds with the HTML content.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#key-components-of-http-requests","title":"Key Components of HTTP Requests","text":"<ol> <li>Method: Usually GET for retrieving data</li> <li>URL: The address of the resource</li> <li>Headers: Metadata about the request (User-Agent, Accept types, etc.)</li> <li>Response: The server's reply containing status code and content</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#example-from-baseparser","title":"Example from BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers\"\"\"\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, timeout=5)  # HTTP GET request\n            response.raise_for_status()  # Check for HTTP errors\n            responses.append(response.text)  # Store HTML content\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#common-http-status-codes","title":"Common HTTP Status Codes","text":"<ul> <li>200: Success - the page loaded correctly</li> <li>404: Not Found - the page doesn't exist</li> <li>403: Forbidden - access denied</li> <li>500: Server Error - problem on the website's end</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#html-parsing","title":"HTML Parsing","text":""},{"location":"developer/BaseParser_web_scraping_guide/#understanding-html-structure","title":"Understanding HTML Structure","text":"<p>HTML (HyperText Markup Language) is the standard markup language for web pages. It uses a tree-like structure of nested elements:</p> <pre><code>&lt;html&gt;\n  &lt;body&gt;\n    &lt;div class=\"article\"&gt;\n      &lt;h1&gt;Article Title&lt;/h1&gt;\n      &lt;p class=\"author\"&gt;By John Doe&lt;/p&gt;\n      &lt;div class=\"content\"&gt;\n        &lt;p&gt;First paragraph...&lt;/p&gt;\n        &lt;p&gt;Second paragraph...&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#the-document-object-model-dom","title":"The Document Object Model (DOM)","text":"<p>The DOM represents HTML as a tree structure where: - Each HTML tag is a node - Nodes can have attributes (class, id, href) - Nodes can contain text or other nodes - Nodes have parent-child relationships</p>"},{"location":"developer/BaseParser_web_scraping_guide/#parsing-strategy","title":"Parsing Strategy","text":"<ol> <li>Identify patterns: Find consistent HTML structures</li> <li>Use selectors: Target specific elements by tag, class, or ID</li> <li>Navigate relationships: Move between parent/child/sibling elements</li> <li>Extract data: Get text content or attribute values</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#beautiful-soup","title":"Beautiful Soup","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-is-beautiful-soup","title":"What is Beautiful Soup?","text":"<p>Beautiful Soup is a Python library designed for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data in a more Pythonic way.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#key-features","title":"Key Features","text":"<ol> <li>Automatic encoding detection: Handles different character encodings</li> <li>Lenient parsing: Works with poorly formatted HTML</li> <li>Powerful searching: Find elements using various methods</li> <li>Tree navigation: Move through the document structure easily</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#beautiful-soup-methods","title":"Beautiful Soup Methods","text":""},{"location":"developer/BaseParser_web_scraping_guide/#finding-elements","title":"Finding Elements","text":"<pre><code># Find single elements\nsoup.find('tag')                    # First occurrence of tag\nsoup.find('tag', class_='classname') # First tag with specific class\nsoup.find('tag', {'attribute': 'value'}) # First tag with attribute\n\n# Find multiple elements\nsoup.find_all('tag')                # All occurrences of tag\nsoup.find_all(['tag1', 'tag2'])    # All occurrences of multiple tags\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#extracting-data","title":"Extracting Data","text":"<pre><code># Get text content\nelement.text          # All text including nested elements\nelement.get_text()    # Same as .text but with options\nelement.string        # Direct text only (no nested elements)\n\n# Get attributes\nelement.get('href')   # Get specific attribute\nelement['class']      # Get class attribute (returns list)\nelement.attrs         # Get all attributes as dictionary\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#real-examples-from-opal-parsers","title":"Real Examples from OPAL Parsers","text":""},{"location":"developer/BaseParser_web_scraping_guide/#parser1819-news-article-extraction","title":"Parser1819 - News Article Extraction","text":"<pre><code>def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Extract title from &lt;title&gt; tag\n    title_tag = soup.title\n    if title_tag:\n        article['title'] = title_tag.string.strip()\n\n    # Extract author from specific div structure\n    author_date_div = soup.find('div', class_='author-date')\n    if author_date_div:\n        author_link = author_date_div.find('a')\n        if author_link:\n            article['author'] = author_link.text.strip()\n\n    # Extract all paragraphs\n    paragraphs = soup.find_all(['p'])\n    for p in paragraphs:\n        text = p.get_text().strip()\n        # Process paragraph text...\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#parserappealsal-table-data-extraction","title":"ParserAppealsAL - Table Data Extraction","text":"<pre><code>def parse_table_row(self, row) -&gt; Optional[Dict]:\n    cells = row.find_all('td')  # Find all table cells\n    if len(cells) &lt; 6:\n        return None\n\n    # Extract text from specific cells\n    court = cells[0].get_text(strip=True)\n\n    # Extract both text and link from anchor tag\n    case_number_elem = cells[1].find('a')\n    if case_number_elem:\n        case_number = {\n            \"text\": case_number_elem.get_text(strip=True),\n            \"link\": case_number_elem.get('href', '')\n        }\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#baseparser-architecture","title":"BaseParser Architecture","text":""},{"location":"developer/BaseParser_web_scraping_guide/#abstract-base-class-design","title":"Abstract Base Class Design","text":"<p>BaseParser uses Python's ABC (Abstract Base Class) to define a contract that all parsers must follow:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseParser(ABC):\n    \"\"\"Base class defining the interface for all parsers\"\"\"\n\n    @abstractmethod\n    def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n        \"\"\"Each parser must implement this method\"\"\"\n        pass\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#why-use-abstract-base-classes","title":"Why Use Abstract Base Classes?","text":"<ol> <li>Consistency: Ensures all parsers have required methods</li> <li>Polymorphism: Different parsers can be used interchangeably</li> <li>Documentation: Clear contract for what parsers must implement</li> <li>Error Prevention: Catches missing implementations at instantiation</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#baseparser-methods","title":"BaseParser Methods","text":""},{"location":"developer/BaseParser_web_scraping_guide/#make_request","title":"make_request()","text":"<ul> <li>Purpose: Fetch HTML content from URLs</li> <li>Input: List of URLs</li> <li>Output: HTML content and successful URLs</li> <li>Error Handling: Continues on failure, reports errors</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#parse_article-abstract","title":"parse_article() (abstract)","text":"<ul> <li>Purpose: Extract data from single HTML page</li> <li>Input: HTML content and URL</li> <li>Output: Dictionary of extracted data</li> <li>Implementation: Must be defined by each parser subclass</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#parse_articles","title":"parse_articles()","text":"<ul> <li>Purpose: Coordinate parsing of multiple URLs</li> <li>Input: List of URLs</li> <li>Output: JSON string of all parsed data</li> <li>Process: Fetches HTML, calls parse_article(), combines results</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#implementation-examples","title":"Implementation Examples","text":""},{"location":"developer/BaseParser_web_scraping_guide/#creating-a-new-parser","title":"Creating a New Parser","text":"<pre><code>from opal.parser_module import BaseParser\nfrom bs4 import BeautifulSoup\n\nclass MyNewsParser(BaseParser):\n    \"\"\"Custom parser for a specific news site\"\"\"\n\n    def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Initialize data structure\n        article = {\n            'url': url,\n            'title': '',\n            'author': '',\n            'date': '',\n            'content': []\n        }\n\n        # Extract title\n        title_element = soup.find('h1', class_='article-title') #These classes are website specific\n        if title_element:\n            article['title'] = title_elem.get_text(strip=True)\n\n        # Extract author\n        author_element = soup.find('span', class_='byline')\n        if author_element:\n            article['author'] = author_elem.get_text(strip=True)\n\n        # Extract content paragraphs\n        content_div = soup.find('div', class_='article-body')\n        if content_div:\n            paragraphs = content_div.find_all('p')\n            article['content'] = [p.get_text(strip=True) for p in paragraphs]\n\n        return article\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#handling-complex-html-structures","title":"Handling Complex HTML Structures","text":"<p>Sometimes data is nested or spread across multiple elements:</p> <pre><code># Handle nested structures\narticle_div = soup.find('div', class_='article')\nif article_div:\n    # Navigate to nested elements\n    header = article_div.find('header')\n    if header:\n        title = header.find('h1')\n        meta = header.find('div', class_='meta')\n\n    # Find siblings\n    content = article_div.find_next_sibling('div', class_='content')\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<pre><code>def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n    try:\n        soup = BeautifulSoup(html, 'html.parser')\n        article = {'url': url}\n\n        # Always check if elements exist\n        title_elem = soup.find('h1')\n        if title_elem:\n            article['title'] = title_elem.get_text(strip=True)\n        else:\n            article['title'] = 'No title found'\n\n        # Handle missing attributes safely\n        link_elem = soup.find('a', class_='author-link')\n        if link_elem:\n            article['author_url'] = link_elem.get('href', '')\n\n        return article\n\n    except Exception as e:\n        # Return partial data rather than failing completely\n        return {\n            'url': url,\n            'error': str(e),\n            'partial_data': True\n        }\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#best-practices","title":"Best Practices","text":""},{"location":"developer/BaseParser_web_scraping_guide/#1-respect-website-policies","title":"1. Respect Website Policies","text":"<ul> <li>Check robots.txt (example: https://1819news.com/robots.txt)</li> <li>Add delays between requests</li> <li>Use appropriate User-Agent headers</li> <li>Don't overwhelm servers</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<ul> <li>Expect missing elements</li> <li>Provide default values</li> <li>Log errors for debugging</li> <li>Continue processing other data</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#3-write-maintainable-code","title":"3. Write Maintainable Code","text":"<ul> <li>Use descriptive variable names</li> <li>Comment complex selections</li> <li>Create reusable helper functions</li> <li>Test with various page structures</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#4-optimize-performance","title":"4. Optimize Performance","text":"<ul> <li>Reuse parser instances</li> <li>Batch process URLs</li> <li>Cache results when appropriate</li> <li>Close resources properly</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#5-structure-data-consistently","title":"5. Structure Data Consistently","text":"<ul> <li>Use consistent field names</li> <li>Provide empty defaults</li> <li>Validate data types</li> <li>Document output format</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#common-challenges-and-solutions","title":"Common Challenges and Solutions","text":""},{"location":"developer/BaseParser_web_scraping_guide/#dynamic-content","title":"Dynamic Content","text":"<p>Problem: Content loaded by JavaScript isn't in initial HTML Solution: Use Selenium (like ParserAppealsAL) for JavaScript rendering</p>"},{"location":"developer/BaseParser_web_scraping_guide/#changing-html-structure","title":"Changing HTML Structure","text":"<p>Problem: Website updates break selectors Solution: Use multiple fallback selectors, test regularly</p>"},{"location":"developer/BaseParser_web_scraping_guide/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: Too many requests trigger blocking Solution: Add delays, rotate User-Agents, respect rate limits</p>"},{"location":"developer/BaseParser_web_scraping_guide/#encoding-issues","title":"Encoding Issues","text":"<p>Problem: Special characters appear corrupted Solution: Beautiful Soup handles most encoding automatically</p>"},{"location":"developer/BaseParser_web_scraping_guide/#conclusion","title":"Conclusion","text":"<p>The BaseParser provides a robust foundation for web scraping by: - Standardizing the parsing interface - Handling HTTP requests with error recovery - Leveraging Beautiful Soup for HTML parsing - Supporting both simple and complex extraction needs</p> <p>Whether scraping news articles or court records, understanding these fundamentals enables you to create effective parsers that extract structured data from any website.</p>"},{"location":"developer/ParserAppealsAL_documentation/","title":"ParserAppealsAL Documentation","text":""},{"location":"developer/ParserAppealsAL_documentation/#overview","title":"Overview","text":"<p><code>ParserAppealsAL</code> is a specialized parser designed to extract court case data from the Alabama Appeals Court Public Portal. Unlike traditional web scrapers that use simple HTTP requests, this parser employs Selenium WebDriver to handle JavaScript-rendered content, making it capable of extracting data from dynamic web applications.</p>"},{"location":"developer/ParserAppealsAL_documentation/#key-features","title":"Key Features","text":"<ul> <li>JavaScript Support: Uses Selenium WebDriver to render JavaScript-heavy pages</li> <li>Automatic Browser Management: Handles Chrome driver setup and teardown</li> <li>Rate Limiting: Built-in configurable delays between requests to avoid overwhelming the server</li> <li>Table Parsing: Specialized logic for extracting structured data from HTML tables</li> <li>Error Handling: Robust error handling with graceful fallbacks</li> <li>Headless Operation: Can run with or without a visible browser window</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#architecture","title":"Architecture","text":""},{"location":"developer/ParserAppealsAL_documentation/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser (Abstract Base Class)\n    \u2514\u2500\u2500 ParserAppealsAL\n</code></pre> <p>ParserAppealsAL inherits from the <code>BaseParser</code> base class, which defines the common interface for all parsers in the OPAL system. It overrides key methods to provide court-specific functionality.</p>"},{"location":"developer/ParserAppealsAL_documentation/#dependencies","title":"Dependencies","text":"<pre><code># Core Dependencies\nselenium &gt;= 4.0.0          # Browser automation\nwebdriver-manager &gt;= 4.0.0 # Automatic ChromeDriver management\nbeautifulsoup4            # HTML parsing\nrequests                  # HTTP requests (inherited from base)\n\n# Standard Library\njson                      # JSON data handling\ntime                      # Rate limiting\ndatetime                  # Timestamp generation\ntyping                    # Type hints\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#implementation-guide","title":"Implementation Guide","text":""},{"location":"developer/ParserAppealsAL_documentation/#1-basic-structure","title":"1. Basic Structure","text":"<p>To implement your own court parser based on ParserAppealsAL, start with this structure:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nfrom your_project.parser_module import BaseParser\n\nclass YourCourtParser(BaseParser):\n    def __init__(self, headless=True, rate_limit_seconds=3):\n        super().__init__()\n        self.headless = headless\n        self.rate_limit_seconds = rate_limit_seconds\n        self.driver = None\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#2-core-methods","title":"2. Core Methods","text":""},{"location":"developer/ParserAppealsAL_documentation/#__init__self-headless-bool-true-rate_limit_seconds-int-3","title":"<code>__init__(self, headless: bool = True, rate_limit_seconds: int = 3)</code>","text":"<p>Initializes the parser with configuration options.</p> <p>Parameters: - <code>headless</code>: Run Chrome in headless mode (no visible window) - <code>rate_limit_seconds</code>: Delay between requests to avoid rate limiting</p>"},{"location":"developer/ParserAppealsAL_documentation/#_setup_driverself","title":"<code>_setup_driver(self)</code>","text":"<p>Sets up the Chrome WebDriver with appropriate options:</p> <pre><code>def _setup_driver(self):\n    chrome_options = Options()\n    if self.headless:\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920,1080\")\n\n    service = Service(ChromeDriverManager().install())\n    self.driver = webdriver.Chrome(service=service, options=chrome_options)\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#make_requestself-url-str-timeout-int-30-optionalstr","title":"<code>make_request(self, url: str, timeout: int = 30) -&gt; Optional[str]</code>","text":"<p>Overrides the base class method to use Selenium instead of requests library.</p> <p>Key Features: - Lazy driver initialization - Waits for specific elements to load - Implements rate limiting - Returns page source HTML</p> <pre><code>def make_request(self, url, timeout=30):\n    if not self.driver:\n        self._setup_driver()\n\n    self.driver.get(url)\n\n    # Wait for your specific element\n    WebDriverWait(self.driver, timeout).until(\n        EC.presence_of_element_located((By.CSS_SELECTOR, \"table\"))\n    )\n\n    time.sleep(self.rate_limit_seconds)\n    return self.driver.page_source\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#parse_table_rowself-row-optionaldict","title":"<code>parse_table_row(self, row) -&gt; Optional[Dict]</code>","text":"<p>Extracts data from a single table row. This method is specific to the table structure of your court portal.</p> <p>Expected Table Structure: 1. Court Name 2. Case Number (with optional link) 3. Case Title 4. Classification 5. Filed Date 6. Status</p> <p>Returns: <pre><code>{\n    \"court\": \"Court of Civil Appeals\",\n    \"case_number\": {\n        \"text\": \"2230123\",\n        \"link\": \"/case/details/...\"\n    },\n    \"case_title\": \"Smith v. Jones\",\n    \"classification\": \"Civil\",\n    \"filed_date\": \"06/11/2024\",\n    \"status\": \"Active\"\n}\n</code></pre></p>"},{"location":"developer/ParserAppealsAL_documentation/#parse_articleself-url-str-dict","title":"<code>parse_article(self, url: str) -&gt; Dict</code>","text":"<p>Main parsing method that processes a single page of court results.</p> <p>Process: 1. Loads the page using <code>make_request</code> 2. Parses HTML with BeautifulSoup 3. Finds the main data table 4. Extracts data from each row 5. Returns structured results</p>"},{"location":"developer/ParserAppealsAL_documentation/#parse_all_casesself-base_url-str-page_urls-liststr-dict","title":"<code>parse_all_cases(self, base_url: str, page_urls: List[str]) -&gt; Dict</code>","text":"<p>Processes multiple pages of results and combines them.</p> <p>Returns: <pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        # List of case dictionaries\n    ]\n}\n</code></pre></p>"},{"location":"developer/ParserAppealsAL_documentation/#3-integration-with-opal-system","title":"3. Integration with OPAL System","text":"<p>The parser integrates with OPAL through the <code>IntegratedParser</code> class:</p> <pre><code>from opal.integrated_parser import IntegratedParser\nfrom your_parser import YourCourtParser\n\n# Create parser instance\nparser = IntegratedParser(YourCourtParser)\n\n# Process court data\nresult = parser.process_site(\n    base_url=\"https://your-court-portal.gov/search\",\n    suffix=\"\",  # Not used for court parsers\n    max_pages=None  # Will process all available pages\n)\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#4-url-pagination","title":"4. URL Pagination","text":"<p>Court portals often use complex URL parameters for pagination. The system includes helper functions in <code>court_url_paginator.py</code>:</p> <ul> <li><code>parse_court_url()</code>: Extracts page number and total pages from URL</li> <li><code>build_court_url()</code>: Constructs URLs for specific pages</li> <li><code>paginate_court_urls()</code>: Generates list of all page URLs</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#5-best-practices","title":"5. Best Practices","text":"<ol> <li>Error Handling: Always wrap operations in try-except blocks</li> <li>Resource Management: Ensure driver is closed in finally blocks</li> <li>Rate Limiting: Respect server limits to avoid IP bans</li> <li>Dynamic Waits: Use WebDriverWait instead of fixed sleep times when possible</li> <li>Memory Management: Close driver after processing to free resources</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#6-testing","title":"6. Testing","text":"<p>Create test scripts to validate your parser:</p> <pre><code>from your_parser import YourCourtParser\n\ndef test_single_page():\n    parser = YourCourtParser(headless=True)\n    result = parser.parse_article(\"https://court-url.gov/page1\")\n\n    assert result[\"cases\"]\n    assert len(result[\"cases\"]) &gt; 0\n\n    # Validate case structure\n    case = result[\"cases\"][0]\n    assert \"court\" in case\n    assert \"case_number\" in case\n    assert \"case_title\" in case\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#customization-guide","title":"Customization Guide","text":""},{"location":"developer/ParserAppealsAL_documentation/#adapting-for-different-court-systems","title":"Adapting for Different Court Systems","text":"<ol> <li>Table Structure: Modify <code>parse_table_row()</code> to match your court's table columns</li> <li>Wait Conditions: Update the element selector in <code>make_request()</code> </li> <li>URL Patterns: Adjust pagination logic in helper functions</li> <li>Data Fields: Add or remove fields based on available data</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#common-modifications","title":"Common Modifications","text":"<ol> <li> <p>Different Table Selectors: <pre><code># Instead of generic \"table\"\nWebDriverWait(self.driver, timeout).until(\n    EC.presence_of_element_located((By.ID, \"case-results-table\"))\n)\n</code></pre></p> </li> <li> <p>Additional Data Extraction: <pre><code># Add judge information if available\njudge = cells[6].get_text(strip=True) if len(cells) &gt; 6 else \"\"\n</code></pre></p> </li> <li> <p>Custom Headers: <pre><code># Some courts require authentication headers\nself.driver.add_cookie({\"name\": \"session\", \"value\": \"your-session-id\"})\n</code></pre></p> </li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/ParserAppealsAL_documentation/#common-issues","title":"Common Issues","text":"<ol> <li>ChromeDriver Not Found: </li> <li>Solution: webdriver-manager should handle this automatically</li> <li> <p>Manual fix: Download ChromeDriver matching your Chrome version</p> </li> <li> <p>Elements Not Loading:</p> </li> <li>Increase timeout in WebDriverWait</li> <li>Check if element selectors have changed</li> <li> <p>Verify JavaScript is executing properly</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Increase <code>rate_limit_seconds</code></li> <li>Implement exponential backoff</li> <li> <p>Consider using proxy rotation</p> </li> <li> <p>Memory Leaks:</p> </li> <li>Ensure driver is closed after use</li> <li>Implement periodic driver restarts for long runs</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Headless Mode: Significantly faster than visible browser</li> <li>Parallel Processing: Not recommended due to rate limits</li> <li>Caching: Consider caching parsed results to avoid re-parsing</li> <li>Resource Usage: Each driver instance uses ~100-200MB RAM</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#example-output","title":"Example Output","text":"<pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        {\n            \"court\": \"Court of Civil Appeals\",\n            \"case_number\": {\n                \"text\": \"CL-2024-000123\",\n                \"link\": \"/portal/case/details/123\"\n            },\n            \"case_title\": \"Smith v. Jones Corporation\",\n            \"classification\": \"Civil Appeal\",\n            \"filed_date\": \"01/10/2025\",\n            \"status\": \"Pending\"\n        },\n        {\n            \"court\": \"Court of Criminal Appeals\",\n            \"case_number\": {\n                \"text\": \"CR-2024-000456\",\n                \"link\": \"/portal/case/details/456\"\n            },\n            \"case_title\": \"State of Alabama v. Doe\",\n            \"classification\": \"Criminal Appeal\",\n            \"filed_date\": \"01/09/2025\",\n            \"status\": \"Active\"\n        }\n    ]\n}\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#security-considerations","title":"Security Considerations","text":"<ol> <li>Input Validation: Always validate URLs before processing</li> <li>Sandbox Mode: Chrome runs with --no-sandbox for compatibility</li> <li>Credential Storage: Never hardcode credentials in parser</li> <li>SSL Verification: Selenium handles SSL by default</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#future-enhancements","title":"Future Enhancements","text":"<p>Consider these improvements for production use:</p> <ol> <li>Retry Logic: Implement automatic retries for failed requests</li> <li>Progress Tracking: Add callbacks for progress updates</li> <li>Data Validation: Implement schema validation for parsed data</li> <li>Export Formats: Support multiple output formats (CSV, Excel)</li> <li>Incremental Updates: Track previously parsed cases to avoid duplicates</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/","title":"Alabama Appeals Court Public Portal Scraper - Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#overview","title":"Overview","text":"<p>Create a court case scraper extension for OPAL that extracts tabular data from the Alabama Appeals Court Public Portal. The scraper must handle JavaScript-rendered content, complex URL-based pagination, and preserve both text and link references.</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-by-step-implementation-instructions","title":"Step-by-Step Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-1-update-dependencies","title":"Step 1: Update Dependencies","text":"<p>Add the following to <code>requirements.txt</code> and <code>pyproject.toml</code>: - <code>selenium&gt;=4.0.0</code> or <code>playwright&gt;=1.40.0</code> (for JavaScript rendering) - <code>webdriver-manager&gt;=4.0.0</code> (if using Selenium for automatic driver management)</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-2-create-court-case-parser-module","title":"Step 2: Create Court Case Parser Module","text":"<p>Create a new file <code>opal/court_case_parser.py</code> with the following specifications:</p> <ol> <li>Import necessary libraries:</li> <li>Selenium/Playwright for JavaScript rendering</li> <li>BeautifulSoup for HTML parsing</li> <li> <p>Standard libraries for URL manipulation and JSON output</p> </li> <li> <p>Create <code>CourtCaseParser</code> class that extends <code>BaseParser</code>:</p> </li> <li>Override <code>make_request()</code> to use Selenium/Playwright instead of requests</li> <li>Implement JavaScript rendering with appropriate wait conditions</li> <li> <p>Add rate limiting (minimum 2-3 seconds between requests)</p> </li> <li> <p>Implement <code>parse_table_row()</code> method to extract:</p> </li> <li>Court name from <code>&lt;td class=\"text-start\"&gt;</code> (column 1)</li> <li>Case number text and href from <code>&lt;a href=\"/portal/court/...\"&gt;</code> (column 2)</li> <li>Case title from <code>&lt;td class=\"text-start\"&gt;</code> (column 3)</li> <li>Classification from <code>&lt;td class=\"text-start\"&gt;</code> (column 4)</li> <li>Filed date from <code>&lt;td class=\"text-start\"&gt;</code> (column 5)</li> <li>Open/Closed status from <code>&lt;td class=\"text-start\"&gt;</code> (column 6)</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-3-create-custom-url-pagination-handler","title":"Step 3: Create Custom URL Pagination Handler","text":"<p>Create <code>opal/court_url_paginator.py</code> with:</p> <ol> <li>URL parser function to:</li> <li>Extract and decode the complex URL parameters</li> <li>Identify current page number from <code>page~(number~X)</code></li> <li> <p>Extract total pages from <code>totalPages~X</code></p> </li> <li> <p>URL builder function to:</p> </li> <li>Take base URL and page number</li> <li>Update <code>page~(number~X)</code> parameter</li> <li>Maintain all other search parameters</li> <li> <p>Handle special encoding (<code>~</code>, <code>%2a2f</code>, etc.)</p> </li> <li> <p>Pagination iterator that:</p> </li> <li>Starts at page 0</li> <li>Continues until reaching <code>totalPages</code></li> <li>Yields properly formatted URLs for each page</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-4-implement-data-extraction-logic","title":"Step 4: Implement Data Extraction Logic","text":"<p>In <code>CourtCaseParser</code>, create <code>parse_all_cases()</code> method that:</p> <ol> <li>Initialize browser driver (Selenium/Playwright)</li> <li>Load first page and extract total pages from URL</li> <li>For each page:</li> <li>Navigate to page URL</li> <li>Wait for table to load (use explicit waits)</li> <li>Extract all table rows</li> <li>Parse each row using <code>parse_table_row()</code></li> <li>Store results with preserved link references</li> <li>Close browser driver when complete</li> <li>Return combined results from all pages as single dataset</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-5-define-output-format","title":"Step 5: Define Output Format","text":"<p>Structure the output JSON as: <pre><code>{\n  \"status\": \"success\",\n  \"total_cases\": 317,\n  \"extraction_date\": \"2025-06-11\",\n  \"cases\": [\n    {\n      \"court\": \"Alabama Supreme Court\",\n      \"case_number\": {\n        \"text\": \"SC-2025-0424\",\n        \"link\": \"/portal/court/68f021c4-6a44-4735-9a76-5360b2e8af13/case/d024d958-58a1-41c9-9fae-39c645c7977e\"\n      },\n      \"case_title\": \"Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.\",\n      \"classification\": \"Appeal - Civil - Injunction Other\",\n      \"filed_date\": \"06/10/2025\",\n      \"status\": \"Open\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-6-integrate-with-opal-cli","title":"Step 6: Integrate with OPAL CLI","text":"<p>Modify existing OPAL files:</p> <ol> <li>Update <code>opal/__init__.py</code>:</li> <li>Add <code>from .court_case_parser import CourtCaseParser</code></li> <li> <p>Add <code>from .court_url_paginator import paginate_court_urls</code></p> </li> <li> <p>Update <code>opal/integrated_parser.py</code>:</p> </li> <li>Add conditional logic to handle court case URLs differently</li> <li> <p>Use <code>paginate_court_urls</code> instead of <code>get_all_news_urls</code> for court sites</p> </li> <li> <p>Update <code>opal/main.py</code>:</p> </li> <li>Add <code>--parser court</code> option to argparse choices</li> <li>Add court parser to the parser selection logic</li> <li>Adjust output filename format for court data</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-7-handle-technical-requirements","title":"Step 7: Handle Technical Requirements","text":"<p>Implement the following in <code>CourtCaseParser</code>:</p> <ol> <li>JavaScript rendering:</li> <li>Wait for table element to be present</li> <li>Wait for data rows to load</li> <li> <p>Handle any loading spinners or dynamic content</p> </li> <li> <p>Error handling:</p> </li> <li>Timeout exceptions for slow page loads</li> <li>Missing table elements</li> <li>Network errors</li> <li> <p>Browser crashes</p> </li> <li> <p>Rate limiting:</p> </li> <li>Add configurable delay between page requests (default 3 seconds)</li> <li>Respect server response times</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-8-testing-urls","title":"Step 8: Testing URLs","text":"<p>Use these URLs for testing: - First page: <code>https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</code> - Second page: Same URL but with <code>page~(number~1)</code> and updated <code>totalElements~317~totalPages~13</code></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-9-final-integration","title":"Step 9: Final Integration","text":"<ol> <li>Test the complete flow with: <code>python -m opal --url [court_url] --parser court</code></li> <li>Ensure output file is created with court case data in tabular format</li> <li>Verify all pages are scraped and combined into single result set</li> <li>Confirm case number links are preserved in the output</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#expected-deliverables","title":"Expected Deliverables","text":"<ol> <li><code>opal/court_case_parser.py</code> - Main parser for court data</li> <li><code>opal/court_url_paginator.py</code> - URL pagination handler</li> <li>Updated <code>opal/__init__.py</code>, <code>opal/integrated_parser.py</code>, and <code>opal/main.py</code></li> <li>Updated <code>requirements.txt</code> and <code>pyproject.toml</code> with new dependencies</li> <li>JSON output file with all court cases in structured format</li> </ol>"},{"location":"developer/architecture/","title":"Architecture","text":"<p>OPAL follows a modular architecture that makes it easy to add new parsers and extend functionality.</p>"},{"location":"developer/architecture/#core-components","title":"Core Components","text":""},{"location":"developer/architecture/#baseparser","title":"BaseParser","text":"<p>The foundation of all parsers, providing: - Common web scraping functionality - Error handling and retry logic - Logging infrastructure - Output formatting</p>"},{"location":"developer/architecture/#parser-classes","title":"Parser Classes","text":"<p>Each website has its own parser class that inherits from BaseParser: - <code>Parser1819</code>: For 1819 News - <code>ParserDailyNews</code>: For Alabama Daily News - <code>ParserAppealsAL</code>: For Alabama Appeals Court</p>"},{"location":"developer/architecture/#main-module","title":"Main Module","text":"<p>The <code>__main__.py</code> module handles: - Command-line argument parsing - Parser instantiation - Execution flow - Output management</p>"},{"location":"developer/architecture/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser\n\u251c\u2500\u2500 Parser1819\n\u251c\u2500\u2500 ParserDailyNews\n\u2514\u2500\u2500 ParserAppealsAL\n</code></pre>"},{"location":"developer/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Input: User provides URL and parser type via CLI</li> <li>Initialization: Main module creates parser instance</li> <li>Scraping: Parser fetches and processes web pages</li> <li>Extraction: Parser extracts structured data</li> <li>Output: Data saved to JSON file</li> </ol>"},{"location":"developer/architecture/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"developer/architecture/#template-method-pattern","title":"Template Method Pattern","text":"<p>BaseParser defines the scraping workflow: <pre><code>class BaseParser:\n    def scrape(self):\n        self.setup()\n        data = self.extract_data()\n        self.save_output(data)\n</code></pre></p>"},{"location":"developer/architecture/#factory-pattern","title":"Factory Pattern","text":"<p>Parser selection based on command-line argument: <pre><code>parsers = {\n    'Parser1819': Parser1819,\n    'ParserDailyNews': ParserDailyNews,\n    'court': ParserAppealsAL\n}\nparser_class = parsers[args.parser]\n</code></pre></p>"},{"location":"developer/architecture/#extension-points","title":"Extension Points","text":""},{"location":"developer/architecture/#adding-new-parsers","title":"Adding New Parsers","text":"<ol> <li>Create new class inheriting from BaseParser</li> <li>Implement required methods:</li> <li><code>extract_article_data()</code></li> <li><code>get_article_links()</code></li> <li><code>parse_article()</code></li> <li>Register in main module</li> </ol>"},{"location":"developer/architecture/#customizing-output","title":"Customizing Output","text":"<p>Override <code>format_output()</code> method to customize data structure.</p>"},{"location":"developer/architecture/#adding-features","title":"Adding Features","text":"<ul> <li>Authentication: Add login methods</li> <li>Caching: Implement request caching</li> <li>Rate limiting: Add delay mechanisms</li> </ul>"},{"location":"developer/configurable_court_extractor_design/","title":"Configurable Court Extractor Design","text":""},{"location":"developer/configurable_court_extractor_design/#problem-statement","title":"Problem Statement","text":"<p>The current <code>extract_all_court_cases.py</code> has hardcoded search parameters in the URL, making it inflexible for different search criteria. Users cannot easily change: - Date ranges - Case number filters - Case title filters - Whether to include/exclude closed cases</p>"},{"location":"developer/configurable_court_extractor_design/#solution-overview","title":"Solution Overview","text":"<p>I designed a configurable court extractor that separates URL construction from data extraction, allowing users to specify search parameters via command line arguments or function parameters.</p>"},{"location":"developer/configurable_court_extractor_design/#architecture","title":"Architecture","text":""},{"location":"developer/configurable_court_extractor_design/#1-courtsearchbuilder-class","title":"1. CourtSearchBuilder Class","text":"<p>Purpose: Encapsulates the complex URL building logic for Alabama Appeals Court searches.</p> <p>Why I designed it this way: - Separation of Concerns: URL building is separate from data extraction - Maintainability: Changes to URL structure only affect one class - Reusability: Can be used by different scripts or tools - Readability: Clear methods for each search parameter</p> <pre><code>class CourtSearchBuilder:\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n        self.court_id = \"68f021c4-6a44-4735-9a76-5360b2e8af13\"\n        self.reset_params()\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#2-key-methods-explained","title":"2. Key Methods Explained","text":""},{"location":"developer/configurable_court_extractor_design/#set_date_range","title":"<code>set_date_range()</code>","text":"<p>Purpose: Handle different date range options Design rationale: - Supports both predefined periods (<code>-1y</code>, <code>-6m</code>) and custom date ranges - Automatically converts dates to the portal's expected format (<code>*2f</code> encoding) - Provides sensible defaults</p>"},{"location":"developer/configurable_court_extractor_design/#build_criteria_string","title":"<code>build_criteria_string()</code>","text":"<p>Purpose: Construct the complex URL-encoded criteria parameter Design rationale: - Handles the intricate URL encoding required by the portal - Builds the nested parameter structure programmatically - Reduces human error in URL construction</p>"},{"location":"developer/configurable_court_extractor_design/#build_url","title":"<code>build_url()</code>","text":"<p>Purpose: Create complete search URLs with pagination Design rationale: - Updates page numbers dynamically - Maintains other search parameters across pages - Returns ready-to-use URLs</p>"},{"location":"developer/configurable_court_extractor_design/#configuration-options","title":"Configuration Options","text":""},{"location":"developer/configurable_court_extractor_design/#court-selection","title":"Court Selection","text":"<pre><code># Available courts\ncourts = {\n    'civil': 'Alabama Civil Court of Appeals',\n    'criminal': 'Alabama Court of Criminal Appeals', \n    'supreme': 'Alabama Supreme Court'\n}\n\n# Select court\nsearch_builder.set_court('civil')  # or 'criminal', 'supreme'\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-number-formats","title":"Case Number Formats","text":"<pre><code># Open-ended search\nsearch_builder.set_case_number_filter('2024-001')\n\n# Court-specific formats\nsearch_builder.set_case_number_filter('CL-2024-0001')  # Civil Appeals\nsearch_builder.set_case_number_filter('CR-2024-0001')  # Criminal Appeals  \nsearch_builder.set_case_number_filter('SC-2024-0001')  # Supreme Court\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-categories","title":"Case Categories","text":"<pre><code># For Civil Appeals and Criminal Appeals\ncategories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n\n# For Supreme Court (includes additional option)\nsupreme_categories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n\n# Set category\nsearch_builder.set_case_category('Appeal')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#date-filters","title":"Date Filters","text":"<pre><code># Predefined periods (matching portal options)\nsearch_builder.set_date_range(period='7d')   # Last 7 days\nsearch_builder.set_date_range(period='1m')   # Last month\nsearch_builder.set_date_range(period='3m')   # Last 3 months\nsearch_builder.set_date_range(period='6m')   # Last 6 months\nsearch_builder.set_date_range(period='1y')   # Last year\n\n# Custom date range\nsearch_builder.set_date_range('2024-01-01', '2024-12-31', 'custom')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-title-and-status-filters","title":"Case Title and Status Filters","text":"<pre><code># Filter by case title (partial match)\nsearch_builder.set_case_title_filter('Smith v Jones')\n\n# Exclude closed cases\nsearch_builder.set_exclude_closed(True)\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#command-line-interface","title":"Command Line Interface","text":"<p>Why I included CLI arguments: - User-friendly: No need to modify code for different searches - Scriptable: Can be integrated into automated workflows - Documented: Built-in help shows all options</p>"},{"location":"developer/configurable_court_extractor_design/#usage-examples","title":"Usage Examples","text":""},{"location":"developer/configurable_court_extractor_design/#option-1-use-built-in-search-parameters-recommended","title":"Option 1: Use Built-in Search Parameters (Recommended)","text":"<pre><code># Extract all cases from last year (default from all courts)\npython configurable_court_extractor.py\n\n# Extract cases from Alabama Supreme Court only\npython configurable_court_extractor.py --court supreme\n\n# Extract cases from last 7 days from Criminal Appeals\npython configurable_court_extractor.py --court criminal --date-period 7d\n\n# Extract Appeal cases from Civil Court\npython configurable_court_extractor.py --court civil --case-category Appeal\n\n# Extract cases with custom date range from Supreme Court\npython configurable_court_extractor.py --court supreme --date-period custom --start-date 2024-01-01 --end-date 2024-06-30\n\n# Filter by specific case number format\npython configurable_court_extractor.py --court civil --case-number \"CL-2024-\"\n\n# Filter by case title in Criminal Appeals\npython configurable_court_extractor.py --court criminal --case-title \"State v\"\n\n# Exclude closed cases from Supreme Court\npython configurable_court_extractor.py --court supreme --exclude-closed\n\n# Extract Certified Questions from Supreme Court (unique to Supreme Court)\npython configurable_court_extractor.py --court supreme --case-category \"Certified Question\"\n\n# Comprehensive search with multiple filters\npython configurable_court_extractor.py --court civil --case-category Appeal --date-period 3m --exclude-closed --output-prefix \"civil_appeals_q1\"\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#option-2-use-pre-built-url-with-embedded-search-terms","title":"Option 2: Use Pre-built URL with Embedded Search Terms","text":"<p>\u26a0\ufe0f  WARNING: Custom URLs are temporary and session-based. They may stop working when the website session expires.</p> <pre><code># Use your existing URL with search terms already embedded\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29\"\n\n# Use custom URL with limited pages and custom output prefix\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\" --max-pages 5 --output-prefix \"my_custom_search\"\n\n# Any URL from the portal search interface works\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=YOUR_CUSTOM_SEARCH_CRITERIA\"\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#hybrid-approach","title":"Hybrid Approach","text":"<pre><code># You can also programmatically call the function with a custom URL\nfrom configurable_court_extractor import extract_court_cases_with_params\n\n# Use your existing URL\nyour_url = \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\nresult = extract_court_cases_with_params(custom_url=your_url, max_pages=10)\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#dynamic-court-id-discovery","title":"Dynamic Court ID Discovery","text":""},{"location":"developer/configurable_court_extractor_design/#the-problem-with-dynamic-ids","title":"The Problem with Dynamic IDs","text":"<p>Modern web applications often generate session-specific or dynamic identifiers that change between visits. The Alabama Appeals Court portal appears to use dynamic court IDs that are assigned during the user's session rather than being static, predictable values.</p>"},{"location":"developer/configurable_court_extractor_design/#solution-approach","title":"Solution Approach","text":"<p>Option 1: Automatic Discovery (Recommended) The <code>discover_court_ids()</code> method navigates to the court's search interface and programmatically extracts the current court IDs by:</p> <ol> <li>Loading the search page - Navigates to the main case search interface</li> <li>Inspecting form elements - Locates the court selection dropdown or form elements</li> <li>Extracting ID mappings - Parses the HTML to find court names and their corresponding dynamic IDs</li> <li>Caching for session - Stores the discovered IDs for the duration of the session</li> </ol> <p>Option 2: Manual Discovery If automatic discovery fails, users can:</p> <ol> <li>Inspect browser network traffic - Use browser developer tools to monitor the search requests</li> <li>Extract court ID from URL - Copy a working search URL and extract the court ID parameter</li> <li>Set manually - Use <code>set_court_id_manually()</code> to override the discovered ID</li> </ol> <p>Option 3: URL Bypass (Fallback) When court ID discovery completely fails, users can:</p> <ol> <li>Use browser to build URL - Manually configure search on the website</li> <li>Copy complete URL - Get the full URL with embedded parameters</li> <li>Use --url option - Pass the pre-built URL directly, bypassing all parameter building</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#implementation-benefits","title":"Implementation Benefits","text":"<ol> <li>Resilient to changes - Automatically adapts to new court ID schemes</li> <li>Fallback options - Multiple strategies when automatic discovery fails</li> <li>User-friendly - Handles complexity behind the scenes</li> <li>Transparent - Shows discovered IDs to user for verification</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#usage-examples-with-dynamic-ids","title":"Usage Examples with Dynamic IDs","text":"<pre><code># Let the system discover court IDs automatically\npython configurable_court_extractor.py --court civil --date-period 1m\n\n# If discovery fails, fall back to custom URL\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n\n# For debugging: manually set a court ID\nsearch_builder = CourtSearchBuilder()\nsearch_builder.set_court_id_manually('civil', 'discovered-session-id-12345')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"developer/configurable_court_extractor_design/#url-encoding-strategy","title":"URL Encoding Strategy","text":"<p>The Alabama Appeals Court portal uses a complex nested URL structure: <pre><code>?criteria=~%28advanced~false~courtID~%27{court_id}~page~%28...%29~sort~%28...%29~case~%28...%29%29\n</code></pre></p> <p>My approach: 1. Build parameters as nested dictionaries 2. Convert to the portal's specific encoding format 3. Handle special characters and escaping automatically</p>"},{"location":"developer/configurable_court_extractor_design/#error-handling","title":"Error Handling","text":"<p>Graceful degradation: - If total page count can't be determined, process incrementally - Continue processing if individual pages fail - Provide detailed error messages with stack traces</p>"},{"location":"developer/configurable_court_extractor_design/#performance-considerations","title":"Performance Considerations","text":"<p>Rate limiting:  - Configurable delays between requests - Respectful of server resources</p> <p>Memory efficiency: - Process pages incrementally - Don't load all data into memory at once</p> <p>Progress reporting: - Real-time feedback on processing status - Clear indication of completion</p>"},{"location":"developer/configurable_court_extractor_design/#advantages-over-current-implementation","title":"Advantages Over Current Implementation","text":""},{"location":"developer/configurable_court_extractor_design/#1-flexibility","title":"1. Flexibility","text":"<ul> <li>Before: Fixed search parameters in hardcoded URL</li> <li>After: Configurable search criteria via parameters OR custom URLs</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#2-maintainability","title":"2. Maintainability","text":"<ul> <li>Before: URL changes require code modification</li> <li>After: URL structure centralized in builder class with dynamic discovery</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#3-usability","title":"3. Usability","text":"<ul> <li>Before: Developers need to understand complex URL structure</li> <li>After: Simple method calls and CLI arguments</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#4-reusability","title":"4. Reusability","text":"<ul> <li>Before: Single-purpose script</li> <li>After: Reusable components for different use cases</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#5-documentation","title":"5. Documentation","text":"<ul> <li>Before: Search parameters hidden in URL</li> <li>After: Clear parameter documentation and examples</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#6-resilience-to-changes","title":"6. Resilience to Changes","text":"<ul> <li>Before: Hardcoded court IDs break when website changes</li> <li>After: Automatic discovery adapts to dynamic court ID schemes</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#7-multiple-fallback-options","title":"7. Multiple Fallback Options","text":"<ul> <li>Before: Script fails completely if URL structure changes</li> <li>After: Automatic discovery \u2192 manual discovery \u2192 custom URL bypass</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#integration-with-existing-code","title":"Integration with Existing Code","text":"<p>The new extractor can coexist with the current implementation: - Uses the same <code>ParserAppealsAL</code> class - Produces the same JSON/CSV output format - Follows the same error handling patterns</p>"},{"location":"developer/configurable_court_extractor_design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"developer/configurable_court_extractor_design/#additional-search-parameters","title":"Additional Search Parameters","text":"<ul> <li>Court type selection</li> <li>Attorney name filters</li> <li>Case status filters</li> <li>Judge name filters</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#advanced-features","title":"Advanced Features","text":"<ul> <li>Save/load search configurations</li> <li>Scheduled extractions</li> <li>Differential updates (only new cases)</li> <li>Export to additional formats (Excel, XML)</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Parallel page processing</li> <li>Caching of search results</li> <li>Resume interrupted extractions</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#code-structure","title":"Code Structure","text":"<pre><code>configurable_court_extractor.py\n\u251c\u2500\u2500 CourtSearchBuilder class\n\u2502   \u251c\u2500\u2500 Parameter management methods\n\u2502   \u251c\u2500\u2500 URL building methods\n\u2502   \u2514\u2500\u2500 Validation methods\n\u251c\u2500\u2500 extract_court_cases_with_params() function\n\u2502   \u251c\u2500\u2500 Search execution logic\n\u2502   \u251c\u2500\u2500 Progress reporting\n\u2502   \u2514\u2500\u2500 Output generation\n\u2514\u2500\u2500 main() function\n    \u251c\u2500\u2500 CLI argument parsing\n    \u251c\u2500\u2500 Parameter validation\n    \u2514\u2500\u2500 Function orchestration\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#why-this-design-is-better","title":"Why This Design is Better","text":"<ol> <li>Single Responsibility: Each class/function has one clear purpose</li> <li>Open/Closed Principle: Easy to extend without modifying existing code</li> <li>DRY (Don't Repeat Yourself): URL logic is centralized</li> <li>User-Centered: Designed around user needs, not technical constraints</li> <li>Testable: Components can be unit tested independently</li> <li>Documented: Self-documenting code with clear method names</li> </ol> <p>This design transforms a rigid, single-purpose script into a flexible, user-friendly tool that can adapt to various research needs while maintaining the reliability and performance of the original implementation.</p>"},{"location":"developer/configurable_court_extractor_design/#complete-implementation-code","title":"Complete Implementation Code","text":"<p>Below is the full implementation of the configurable court extractor:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nConfigurable Court Case Extractor for Alabama Appeals Court\nAllows users to set custom search parameters OR use pre-built URLs\n\"\"\"\nimport json\nimport argparse\nfrom datetime import datetime, timedelta\nfrom urllib.parse import quote\nfrom opal.court_case_parser import ParserAppealsAL\nfrom opal.court_url_paginator import parse_court_url\n\n\nclass CourtSearchBuilder:\n    \"\"\"Builder class for constructing Alabama Court search URLs with court-specific parameters\"\"\"\n\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n\n        # Court definitions with their specific IDs and configurations\n        # NOTE: Court IDs may be dynamically assigned by the website\n        # These IDs should be discovered through session initialization\n        self.courts = {\n            'civil': {\n                'name': 'Alabama Civil Court of Appeals',\n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'CL',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n            },\n            'criminal': {\n                'name': 'Alabama Court of Criminal Appeals', \n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'CR',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n            },\n            'supreme': {\n                'name': 'Alabama Supreme Court',\n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'SC',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n            }\n        }\n\n        # Date period mappings\n        self.date_periods = {\n            '7d': '-7d',\n            '1m': '-1m', \n            '3m': '-3m',\n            '6m': '-6m',\n            '1y': '-1y',\n            'custom': 'custom'\n        }\n\n        self.current_court = 'civil'  # Default court\n        self.session_initialized = False\n        self.reset_params()\n\n    def reset_params(self):\n        \"\"\"Reset all parameters to defaults\"\"\"\n        court_info = self.courts[self.current_court]\n        self.params = {\n            'advanced': 'false',\n            'courtID': court_info['id'],  # May be None until discovered\n            'page': {\n                'size': 500,\n                'number': 0,\n                'totalElements': 0,\n                'totalPages': 0\n            },\n            'sort': {\n                'sortBy': 'caseHeader.filedDate',\n                'sortDesc': 'true'\n            },\n            'case': {\n                'caseCategoryID': 1000000,  # All categories\n                'caseNumberQueryTypeID': 10463,  # Contains\n                'caseTitleQueryTypeID': 300054,  # Contains\n                'filedDateChoice': '1y',  # Last year\n                'filedDateStart': '',\n                'filedDateEnd': '',\n                'excludeClosed': 'false'\n            }\n        }\n\n    def discover_court_ids(self, parser_instance):\n        \"\"\"\n        Discover court IDs by navigating to the website and inspecting the court selection interface\n\n        Args:\n            parser_instance: Instance of ParserAppealsAL with active WebDriver\n        \"\"\"\n        try:\n            # Navigate to the main search page\n            search_page_url = \"https://publicportal.alappeals.gov/portal/search/case\"\n            parser_instance.driver.get(search_page_url)\n\n            # Wait for the page to load\n            from selenium.webdriver.support.ui import WebDriverWait\n            from selenium.webdriver.support import expected_conditions as EC\n            from selenium.webdriver.common.by import By\n\n            wait = WebDriverWait(parser_instance.driver, 10)\n\n            # Look for court selection dropdown or options\n            # This is a placeholder - actual implementation would need to inspect the HTML structure\n            court_selector = wait.until(EC.presence_of_element_located((By.ID, \"court-selector\")))\n\n            # Extract court options and their IDs\n            # Implementation would parse the HTML to find court names and their corresponding IDs\n            court_options = court_selector.find_elements(By.TAG_NAME, \"option\")\n\n            for option in court_options:\n                court_name = option.text.lower()\n                court_id = option.get_attribute(\"value\")\n\n                # Map court names to our court keys\n                if \"civil\" in court_name and \"appeals\" in court_name:\n                    self.courts['civil']['id'] = court_id\n                elif \"criminal\" in court_name and \"appeals\" in court_name:\n                    self.courts['criminal']['id'] = court_id\n                elif \"supreme\" in court_name:\n                    self.courts['supreme']['id'] = court_id\n\n            self.session_initialized = True\n            print(\"Successfully discovered court IDs:\")\n            for court_key, court_info in self.courts.items():\n                print(f\"  {court_info['name']}: {court_info['id']}\")\n\n        except Exception as e:\n            print(f\"Warning: Could not discover court IDs automatically: {e}\")\n            print(\"Try running your search on the website and searching by URL populated by the search\")\n\n    def set_court_id_manually(self, court_key, court_id):\n        \"\"\"\n        Set court ID manually\n\n        Args:\n            court_key: 'civil', 'criminal', or 'supreme'\n            court_id: The discovered court ID string\n        \"\"\"\n        if court_key in self.courts:\n            self.courts[court_key]['id'] = court_id\n            print(f\"Manually set {court_key} court ID to: {court_id}\")\n        else:\n            raise ValueError(f\"Invalid court key: {court_key}\")\n\n    def set_court(self, court_key):\n        \"\"\"\n        Set the court to search\n\n        Args:\n            court_key: 'civil', 'criminal', or 'supreme'\n        \"\"\"\n        if court_key not in self.courts:\n            raise ValueError(f\"Invalid court: {court_key}. Must be one of {list(self.courts.keys())}\")\n\n        self.current_court = court_key\n        self.reset_params()  # Reset params with new court ID\n\n    def get_court_info(self):\n        \"\"\"Get information about the current court\"\"\"\n        return self.courts[self.current_court]\n\n    def validate_case_category(self, category):\n        \"\"\"Validate that the category is available for the current court\"\"\"\n        court_info = self.courts[self.current_court]\n        if category not in court_info['categories']:\n            raise ValueError(f\"Category '{category}' not available for {court_info['name']}. \"\n                           f\"Available: {court_info['categories']}\")\n        return True\n\n    def format_case_number_suggestion(self, year=None):\n        \"\"\"Suggest proper case number format for current court\"\"\"\n        court_info = self.courts[self.current_court]\n        current_year = year or datetime.now().year\n        return f\"{court_info['case_prefix']}-{current_year}-####\"\n\n    def set_date_range(self, start_date=None, end_date=None, period='1y'):\n        \"\"\"\n        Set the date range for case searches\n\n        Args:\n            start_date: Start date (YYYY-MM-DD) or None\n            end_date: End date (YYYY-MM-DD) or None  \n            period: Predefined period ('7d', '1m', '3m', '6m', '1y', 'custom')\n        \"\"\"\n        if period == 'custom' and start_date and end_date:\n            # Convert dates to the format expected by the portal\n            self.params['case']['filedDateChoice'] = 'custom'\n            self.params['case']['filedDateStart'] = start_date.replace('-', '*2f')\n            self.params['case']['filedDateEnd'] = end_date.replace('-', '*2f')\n        else:\n            # Use predefined period - validate it exists\n            if period not in self.date_periods:\n                raise ValueError(f\"Invalid date period: {period}. Must be one of {list(self.date_periods.keys())}\")\n\n            self.params['case']['filedDateChoice'] = self.date_periods[period]\n\n            # Calculate dates for display purposes\n            today = datetime.now()\n            if period == '7d':\n                start = today - timedelta(days=7)\n            elif period == '1m':\n                start = today - timedelta(days=30)\n            elif period == '3m':\n                start = today - timedelta(days=90)\n            elif period == '6m':\n                start = today - timedelta(days=180)\n            elif period == '1y':\n                start = today - timedelta(days=365)\n            else:\n                start = today - timedelta(days=365)\n\n            self.params['case']['filedDateStart'] = start.strftime('%m*2f%d*2f%Y')\n            self.params['case']['filedDateEnd'] = today.strftime('%m*2f%d*2f%Y')\n\n    def set_case_category(self, category_name=None):\n        \"\"\"\n        Set case category filter\n\n        Args:\n            category_name: Category name ('Appeal', 'Certiorari', 'Original Proceeding', \n                          'Petition', 'Certified Question') or None for all\n        \"\"\"\n        if category_name is None:\n            self.params['case']['caseCategoryID'] = 1000000  # All categories\n            return\n\n        # Validate category is available for current court\n        self.validate_case_category(category_name)\n\n        # Map category names to IDs (these would need to be discovered from the portal)\n        category_map = {\n            'Appeal': 1000001,\n            'Certiorari': 1000002, \n            'Original Proceeding': 1000003,\n            'Petition': 1000004,\n            'Certified Question': 1000005  # Supreme Court only\n        }\n\n        if category_name in category_map:\n            self.params['case']['caseCategoryID'] = category_map[category_name]\n        else:\n            raise ValueError(f\"Unknown category: {category_name}\")\n\n    def set_case_number_filter(self, case_number=None, query_type=10463):\n        \"\"\"\n        Set case number filter\n\n        Args:\n            case_number: Case number to search for\n            query_type: Query type (10463=contains, check portal for others)\n        \"\"\"\n        self.params['case']['caseNumberQueryTypeID'] = query_type\n        if case_number:\n            self.params['case']['caseNumber'] = case_number\n\n    def set_case_title_filter(self, title=None, query_type=300054):\n        \"\"\"\n        Set case title filter\n\n        Args:\n            title: Title text to search for\n            query_type: Query type (300054=contains, check portal for others)\n        \"\"\"\n        self.params['case']['caseTitleQueryTypeID'] = query_type\n        if title:\n            self.params['case']['caseTitle'] = title\n\n    def set_exclude_closed(self, exclude=False):\n        \"\"\"\n        Set whether to exclude closed cases\n\n        Args:\n            exclude: True to exclude closed cases, False to include all\n        \"\"\"\n        self.params['case']['excludeClosed'] = 'true' if exclude else 'false'\n\n    def set_sort_order(self, sort_by='caseHeader.filedDate', descending=True):\n        \"\"\"\n        Set sort order for results\n\n        Args:\n            sort_by: Field to sort by\n            descending: True for descending, False for ascending\n        \"\"\"\n        self.params['sort']['sortBy'] = sort_by\n        self.params['sort']['sortDesc'] = 'true' if descending else 'false'\n\n    def set_page_info(self, page_number=0, page_size=25, total_elements=0, total_pages=0):\n        \"\"\"Set pagination information\"\"\"\n        self.params['page'].update({\n            'number': page_number,\n            'size': page_size,\n            'totalElements': total_elements,\n            'totalPages': total_pages\n        })\n\n    def build_criteria_string(self):\n        \"\"\"Build the criteria string for the URL\"\"\"\n        criteria_parts = []\n\n        # Basic parameters\n        criteria_parts.append(f\"advanced~{self.params['advanced']}\")\n        criteria_parts.append(f\"courtID~%27{self.params['courtID']}\")\n\n        # Page parameters\n        page = self.params['page']\n        page_str = f\"page~%28size~{page['size']}~number~{page['number']}~totalElements~{page['totalElements']}~totalPages~{page['totalPages']}%29\"\n        criteria_parts.append(page_str)\n\n        # Sort parameters\n        sort = self.params['sort']\n        sort_str = f\"sort~%28sortBy~%27{sort['sortBy']}~sortDesc~{sort['sortDesc']}%29\"\n        criteria_parts.append(sort_str)\n\n        # Case parameters\n        case = self.params['case']\n        case_parts = []\n        case_parts.append(f\"caseCategoryID~{case['caseCategoryID']}\")\n        case_parts.append(f\"caseNumberQueryTypeID~{case['caseNumberQueryTypeID']}\")\n        case_parts.append(f\"caseTitleQueryTypeID~{case['caseTitleQueryTypeID']}\")\n        case_parts.append(f\"filedDateChoice~%27{case['filedDateChoice']}\")\n        case_parts.append(f\"filedDateStart~%27{case['filedDateStart']}\")\n        case_parts.append(f\"filedDateEnd~%27{case['filedDateEnd']}\")\n        case_parts.append(f\"excludeClosed~{case['excludeClosed']}\")\n\n        # Add optional case filters\n        if 'caseNumber' in case:\n            case_parts.append(f\"caseNumber~{quote(case['caseNumber'])}\")\n        if 'caseTitle' in case:\n            case_parts.append(f\"caseTitle~{quote(case['caseTitle'])}\")\n\n        case_str = f\"case~%28{'~'.join(case_parts)}%29\"\n        criteria_parts.append(case_str)\n\n        return f\"~%28{'~'.join(criteria_parts)}%29\"\n\n    def build_url(self, page_number=0):\n        \"\"\"Build complete search URL\"\"\"\n        # Update page number\n        self.set_page_info(page_number=page_number, \n                          page_size=self.params['page']['size'],\n                          total_elements=self.params['page']['totalElements'],\n                          total_pages=self.params['page']['totalPages'])\n\n        criteria = self.build_criteria_string()\n        return f\"{self.base_url}?criteria={criteria}\"\n\n\ndef extract_court_cases_with_params(\n    court='civil',\n    date_period='1y',\n    start_date=None,\n    end_date=None,\n    case_number=None,\n    case_title=None,\n    case_category=None,\n    exclude_closed=False,\n    max_pages=None,\n    output_prefix=\"court_cases\",\n    custom_url=None\n):\n    \"\"\"\n    Extract court cases with configurable search parameters OR a pre-built URL\n\n    Args:\n        court: Court to search ('civil', 'criminal', 'supreme') - ignored if custom_url provided\n        date_period: Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - ignored if custom_url provided\n        start_date: Start date for custom range (YYYY-MM-DD) - ignored if custom_url provided\n        end_date: End date for custom range (YYYY-MM-DD) - ignored if custom_url provided\n        case_number: Filter by case number (partial match) - ignored if custom_url provided\n        case_title: Filter by case title (partial match) - ignored if custom_url provided\n        case_category: Filter by category ('Appeal', 'Certiorari', etc.) - ignored if custom_url provided\n        exclude_closed: Whether to exclude closed cases - ignored if custom_url provided\n        max_pages: Maximum pages to process (None for all)\n        output_prefix: Prefix for output files\n        custom_url: Pre-built search URL with embedded parameters (overrides all other search params)\n    \"\"\"\n\n    if custom_url:\n        # Use the provided URL directly\n        print(\"Using custom URL with embedded search parameters\")\n        print(\"\u26a0\ufe0f  WARNING: Custom URLs contain session-specific parameters that expire.\")\n        print(\"   This URL will only work temporarily and may become invalid after your browser session ends.\")\n        print(\"   For reliable, repeatable searches, use the CLI search parameters instead of --url option.\")\n        print()\n        base_url = custom_url\n        court_name = \"Custom Search\"  # Generic name since we don't know the court\n    else:\n        # Build search URL from parameters\n        search_builder = CourtSearchBuilder()\n\n        # Create parser instance early for court ID discovery\n        parser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\n\n        # Discover court IDs if not already done\n        if not search_builder.session_initialized:\n            print(\"Discovering court IDs from website...\")\n            search_builder.discover_court_ids(parser)\n\n        # Set court\n        search_builder.set_court(court)\n        court_info = search_builder.get_court_info()\n        court_name = court_info['name']\n\n        # Verify court ID was discovered\n        if court_info['id'] is None:\n            raise ValueError(f\"Could not discover court ID for {court_name}. \"\n                           \"Try using the --url option with a pre-built search URL instead.\")\n\n        # Set date range\n        if date_period == 'custom':\n            if not start_date or not end_date:\n                raise ValueError(\"Custom date range requires both start_date and end_date\")\n            search_builder.set_date_range(start_date, end_date, 'custom')\n        else:\n            search_builder.set_date_range(period=date_period)\n\n        # Set filters\n        if case_number:\n            search_builder.set_case_number_filter(case_number)\n        if case_title:\n            search_builder.set_case_title_filter(case_title)\n        if case_category:\n            search_builder.set_case_category(case_category)\n\n        search_builder.set_exclude_closed(exclude_closed)\n\n        # Build initial URL\n        base_url = search_builder.build_url(0)\n\n    print(\"Alabama Appeals Court - Configurable Data Extraction\")\n    print(\"=\" * 55)\n    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Date period: {date_period}\")\n    if date_period == 'custom':\n        print(f\"Date range: {start_date} to {end_date}\")\n    if case_number:\n        print(f\"Case number filter: {case_number}\")\n    if case_title:\n        print(f\"Case title filter: {case_title}\")\n    print(f\"Exclude closed: {exclude_closed}\")\n    print(f\"Max pages: {max_pages or 'All available'}\")\n    print()\n\n    # Create parser instance (may have been created earlier for court ID discovery)\n    if 'parser' not in locals():\n        parser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\n\n    try:\n        # First, get the first page to determine total pages\n        print(\"Loading first page to determine total results...\")\n        result = parser.parse_article(base_url)\n\n        if \"cases\" not in result or not result['cases']:\n            print(\"No cases found with the specified criteria.\")\n            return\n\n        # Try to get total pages from the URL after JavaScript execution\n        if hasattr(parser, 'driver') and parser.driver:\n            current_url = parser.driver.current_url\n            _, total_pages = parse_court_url(current_url)\n\n        if not total_pages:\n            # Estimate based on first page results\n            total_pages = 1\n            print(f\"Could not determine total pages, will process incrementally\")\n        else:\n            print(f\"Found {total_pages} total pages\")\n\n        # Apply max_pages limit\n        if max_pages and max_pages &lt; total_pages:\n            total_pages = max_pages\n            print(f\"Limited to {max_pages} pages\")\n\n        all_cases = []\n\n        # Process all pages\n        for page_num in range(total_pages):\n            print(f\"Processing page {page_num + 1}...\", end='', flush=True)\n\n            if page_num == 0:\n                # Use result from first page\n                page_result = result\n            else:\n                # Build URL for subsequent pages only if not using custom URL\n                if custom_url:\n                    # For custom URLs, we need to modify pagination manually\n                    # This is a simplified approach - in practice, you'd need to parse and modify the URL\n                    page_url = custom_url.replace('number~0', f'number~{page_num}')\n                else:\n                    page_url = search_builder.build_url(page_num)\n                page_result = parser.parse_article(page_url)\n\n            if \"cases\" in page_result and page_result['cases']:\n                all_cases.extend(page_result['cases'])\n                print(f\" Found {len(page_result['cases'])} cases\")\n            else:\n                print(\" No cases found\")\n                # If no cases on this page, we might have reached the end\n                break\n\n        # Create output data\n        output_data = {\n            \"status\": \"success\",\n            \"search_parameters\": {\n                \"court\": court if not custom_url else \"Custom URL\",\n                \"date_period\": date_period if not custom_url else \"Custom URL\",\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n                \"case_number_filter\": case_number,\n                \"case_title_filter\": case_title,\n                \"case_category\": case_category,\n                \"exclude_closed\": exclude_closed,\n                \"custom_url\": custom_url\n            },\n            \"total_cases\": len(all_cases),\n            \"extraction_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n            \"extraction_time\": datetime.now().strftime(\"%H:%M:%S\"),\n            \"pages_processed\": page_num + 1,\n            \"cases\": all_cases\n        }\n\n        # Save results\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        json_filename = f\"{output_prefix}_{timestamp}.json\"\n\n        with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n            json.dump(output_data, f, indent=4, ensure_ascii=False)\n\n        print(f\"\\n\u2713 Successfully extracted {len(all_cases)} court cases\")\n        print(f\"\u2713 Results saved to {json_filename}\")\n\n        # Create CSV if there are results\n        if all_cases:\n            csv_filename = f\"{output_prefix}_{timestamp}.csv\"\n            with open(csv_filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"Court,Case Number,Case Title,Classification,Filed Date,Status,Case Link\\n\")\n\n                for case in all_cases:\n                    court = case.get('court', '').replace(',', ';')\n                    case_num = case.get('case_number', {}).get('text', '').replace(',', ';')\n                    title = case.get('case_title', '').replace(',', ';').replace('\"', \"'\")\n                    classification = case.get('classification', '').replace(',', ';')\n                    filed = case.get('filed_date', '')\n                    status = case.get('status', '')\n                    link = f\"https://publicportal.alappeals.gov{case.get('case_number', {}).get('link', '')}\"\n\n                    f.write(f'\"{court}\",\"{case_num}\",\"{title}\",\"{classification}\",\"{filed}\",\"{status}\",\"{link}\"\\n')\n\n            print(f\"\u2713 CSV table saved to {csv_filename}\")\n\n        return output_data\n\n    except Exception as e:\n        print(f\"\\nError occurred: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None\n    finally:\n        parser._close_driver()\n        print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\ndef main():\n    \"\"\"Command line interface for the configurable court extractor\"\"\"\n    parser = argparse.ArgumentParser(description='Extract Alabama Court cases with configurable search parameters OR custom URL')\n\n    # URL option that overrides all search parameters\n    parser.add_argument('--url', help='Pre-built search URL with embedded parameters (overrides all search options)')\n\n    # Search parameter arguments (ignored if --url is provided)\n    parser.add_argument('--court', choices=['civil', 'criminal', 'supreme'], \n                       default='civil', help='Court to search (default: civil)')\n    parser.add_argument('--date-period', choices=['7d', '1m', '3m', '6m', '1y', 'custom'], \n                       default='1y', help='Date period for case search (default: 1y)')\n    parser.add_argument('--start-date', help='Start date for custom range (YYYY-MM-DD)')\n    parser.add_argument('--end-date', help='End date for custom range (YYYY-MM-DD)')\n    parser.add_argument('--case-number', help='Filter by case number (e.g., CL-2024-, CR-2024-, SC-2024-)')\n    parser.add_argument('--case-title', help='Filter by case title (partial match)')\n    parser.add_argument('--case-category', \n                       choices=['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question'],\n                       help='Filter by case category')\n    parser.add_argument('--exclude-closed', action='store_true', \n                       help='Exclude closed cases from results')\n\n    # Output options (always available)\n    parser.add_argument('--max-pages', type=int, \n                       help='Maximum number of pages to process (default: all)')\n    parser.add_argument('--output-prefix', default='court_cases',\n                       help='Prefix for output files (default: court_cases)')\n\n    args = parser.parse_args()\n\n    # If URL is provided, skip all parameter validation\n    if args.url:\n        print(\"Using custom URL - all search parameter options will be ignored\")\n        print(\"\u26a0\ufe0f  IMPORTANT: Custom URLs are session-based and temporary!\")\n        print(\"   Your URL may stop working when the court website session expires.\")\n        print(\"   Consider using CLI search parameters for reliable, repeatable searches.\")\n        print()\n        extract_court_cases_with_params(\n            custom_url=args.url,\n            max_pages=args.max_pages,\n            output_prefix=args.output_prefix\n        )\n        return\n\n    # Validate custom date range\n    if args.date_period == 'custom':\n        if not args.start_date or not args.end_date:\n            parser.error(\"Custom date period requires both --start-date and --end-date\")\n\n    # Validate case category for court\n    if args.case_category:\n        builder = CourtSearchBuilder()\n        builder.set_court(args.court)\n        try:\n            builder.validate_case_category(args.case_category)\n        except ValueError as e:\n            parser.error(str(e))\n\n    # Show case number format suggestion\n    if args.case_number:\n        builder = CourtSearchBuilder()\n        builder.set_court(args.court)\n        suggested_format = builder.format_case_number_suggestion()\n        print(f\"Case number format for {builder.get_court_info()['name']}: {suggested_format}\")\n\n    # Extract cases using search parameters\n    extract_court_cases_with_params(\n        court=args.court,\n        date_period=args.date_period,\n        start_date=args.start_date,\n        end_date=args.end_date,\n        case_number=args.case_number,\n        case_title=args.case_title,\n        case_category=args.case_category,\n        exclude_closed=args.exclude_closed,\n        max_pages=args.max_pages,\n        output_prefix=args.output_prefix\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"developer/configurable_court_extractor_design/#courtsearchbuilder-class","title":"CourtSearchBuilder Class","text":"<p>Line 12-17: Initialize with base URL and court ID constants. The <code>reset_params()</code> method sets up the default parameter structure.</p> <p>Line 19-46: The <code>reset_params()</code> method creates a nested dictionary structure that mirrors the complex URL parameters used by the Alabama Appeals Court portal.</p> <p>Line 48-74: <code>set_date_range()</code> handles both predefined periods and custom date ranges. It converts standard YYYY-MM-DD format to the portal's <code>*2f</code> encoding format.</p> <p>Line 140-169: <code>build_criteria_string()</code> is the core URL building logic. It constructs the complex nested parameter string with proper URL encoding.</p> <p>Line 171-180: <code>build_url()</code> ties everything together, updating pagination and returning the complete URL.</p>"},{"location":"developer/configurable_court_extractor_design/#main-extraction-function","title":"Main Extraction Function","text":"<p>Line 584-596: Function signature with comprehensive parameters including <code>custom_url</code> option for pre-built URLs.</p> <p>Line 614-647: Dual-mode logic - uses custom URL directly OR builds URL from search parameters.</p> <p>Line 649-661: Initial setup and parameter display for user feedback, with custom URL handling.</p> <p>Line 666-689: First page processing to determine total results and pages available.</p> <p>Line 694-713: Main processing loop that handles pagination dynamically for both custom URLs and built URLs.</p> <p>Line 719-738: Output data structure creation with search parameters preserved for reproducibility, including custom URL tracking.</p> <p>Line 740-760: File saving logic for both JSON and CSV formats.</p>"},{"location":"developer/configurable_court_extractor_design/#command-line-interface_1","title":"Command Line Interface","text":"<p>Line 781-807: Argument parser setup with <code>--url</code> option and all configuration options with help text.</p> <p>Line 811-819: Custom URL handling that bypasses all parameter validation when <code>--url</code> is provided.</p> <p>Line 821-854: Parameter validation and function execution for search-parameter mode.</p>"},{"location":"developer/configurable_court_extractor_design/#key-design-decisions-explained","title":"Key Design Decisions Explained","text":"<ol> <li>Builder Pattern: Separates URL construction complexity from business logic</li> <li>Dual-Mode Operation: Supports both parameter-based search and pre-built URL input</li> <li>Parameter Validation: Ensures required combinations are provided (custom dates)</li> <li>Progressive Enhancement: Starts with defaults, allows selective customization</li> <li>Error Recovery: Graceful handling when page counts can't be determined</li> <li>Output Consistency: Maintains same format as original extractor</li> <li>User Feedback: Real-time progress and parameter confirmation</li> <li>URL Flexibility: Custom URLs override all search parameters for maximum flexibility</li> </ol>"},{"location":"developer/court_scraper_analysis/","title":"Court Scraper Analysis","text":"<p>Based on your answers, I notice several important details:</p>"},{"location":"developer/court_scraper_analysis/#1-dynamic-javascript-rendered-content","title":"1. Dynamic JavaScript-Rendered Content","text":"<ul> <li>The site loads content dynamically, which means BeautifulSoup alone won't work</li> <li>You'll need Selenium or Playwright to render JavaScript before parsing</li> </ul>"},{"location":"developer/court_scraper_analysis/#2-complex-url-based-pagination","title":"2. Complex URL-Based Pagination","text":"<ul> <li>Pagination uses URL parameters (<code>page~(number~0)</code> becomes <code>page~(number~1)</code>)</li> <li>Total pages/elements are embedded in the URL (<code>totalElements~317~totalPages~13</code>)</li> <li>This is very different from the simple <code>/page/2</code> pattern in your news scrapers</li> </ul>"},{"location":"developer/court_scraper_analysis/#3-structured-table-data","title":"3. Structured Table Data","text":"<ul> <li>You need 6 specific columns: Court, Case Number, Case Title, Classification, Filed Date, Open/Closed</li> <li>Case Numbers contain links you want to preserve (both text and href)</li> <li>You want all paginated results combined into one table</li> </ul>"},{"location":"developer/court_scraper_analysis/#4-custom-url-encoding","title":"4. Custom URL Encoding","text":"<ul> <li>The URLs use a unique encoding scheme with <code>~</code> and <code>%2a2f</code> (for slashes)</li> <li>Search parameters are complex with date ranges and multiple filters</li> </ul>"},{"location":"developer/court_scraper_analysis/#key-challenges","title":"Key Challenges:","text":"<ul> <li>Need to handle JavaScript rendering</li> <li>Must parse and manipulate the encoded URL format for pagination</li> <li>Need to extract both text and href attributes from case number links</li> <li>Different data structure (tabular vs. article paragraphs)</li> </ul>"},{"location":"developer/court_scraper_analysis/#proposed-solution","title":"Proposed Solution:","text":"<p>Would you like me to create a new parser that handles these requirements? I'll need to: 1. Add Selenium/Playwright to requirements 2. Create a <code>CourtCaseParser</code> with JavaScript rendering 3. Build a custom pagination handler for this URL format 4. Output structured table data instead of article format</p>"},{"location":"developer/court_scraper_requirements/","title":"Court Case Scraper Extension Requirements","text":"<p>To help you build this court case scraper extension while maintaining OPAL's modularity, I'll need the following information:</p>"},{"location":"developer/court_scraper_requirements/#1-website-details","title":"1. Website Details","text":"<ul> <li>The URL of the court case website Example URL: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Example URLs of pages containing the tables you want to scrape Example URL after pagination of next batch of table elements: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~1~totalElements~317~totalPages~13%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Screenshots or HTML snippets of the table structure</li> </ul>"},{"location":"developer/court_scraper_requirements/#2-data-requirements","title":"2. Data Requirements","text":"<ul> <li>What specific data fields do you need from the tables? (case number, parties, dates, status, etc.) I will need to access the following html fields for data</li> </ul> <p>Column 1 Title: Court Court</p> <p>Column 1 Content Follows this pattern: Alabama Supreme Court</p> <p>Column 2 Title: Case Number Case Number</p> <p>Column 2 Content Follows this pattern:  SC-2025-0424 </p> <p>Column 3 Title: Case Title Case Title</p> <p>Column 3 Content Follows this pattern: Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.</p> <p>Column 4 Title: Classification Classification</p> <p>Column 4 Content Follows this pattern: Appeal - Civil - Injunction Other</p> <p>Column 5 Title: Filed Date Filed Date</p> <p>Column 5 Content Follows this pattern:  06/10/2025 </p> <p>Column 6 Title: Open / Closed Open / Closed</p> <p>Column 6 Content Follows this pattern:  Open </p> <ul> <li>Do you need data from multiple tables per page or one main table?</li> </ul> <p>I only want one table with all of the results of all of the pages at that url. Even if pagination is used to reduce the number of table elements that appear at a time, I want all the results in a single table.</p> <ul> <li>Any specific formatting requirements for the extracted data?</li> </ul>"},{"location":"developer/court_scraper_requirements/#3-navigation-pattern","title":"3. Navigation Pattern","text":"<ul> <li> <p>Does the site use pagination like the news sites? The pagination is not the same. The content is grouped into small chunks, but accessible at the same base url.</p> </li> <li> <p>Are there search/filter parameters in the URL? There are multiple search parameters in the URL. For example, here are the search terms for this url [case~%28caseCategoryID~1000000, caseNumberQueryTypeID~10463, aseTitleQueryTypeID~300054, iledDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025, excludeClosed~false%29%29]</p> </li> <li>Do you need to follow links within tables to get additional details?</li> </ul> <p>I do not want to follow the links within the table, but I do want to store the text and the reference embedded in the link.</p>"},{"location":"developer/court_scraper_requirements/#4-technical-considerations","title":"4. Technical Considerations","text":"<ul> <li>Does the site require authentication? No</li> <li>Is the content loaded dynamically (JavaScript) or static HTML? Dynamically</li> <li>Any rate limiting concerns we should be aware of? Please keep the rate limits low</li> </ul>"},{"location":"developer/court_scraper_requirements/#proposed-extension-architecture","title":"Proposed Extension Architecture","text":"<p>Based on OPAL's current architecture, here's how we'd extend it:</p> <ol> <li>Create a new parser class (e.g., <code>CourtCaseParser</code>) extending <code>NewsParser</code> in <code>parser_module.py</code></li> <li>Adapt or create a new URL discovery function if the pagination pattern differs from the news sites</li> <li>Modify the CLI in <code>main.py</code> to add the court parser option</li> <li>Ensure the output format makes sense for tabular data (might need to adjust from the line-by-line article format)</li> </ol>"},{"location":"developer/court_scraper_requirements/#next-steps","title":"Next Steps","text":"<p>Please provide: 1. The court website URL 2. Description of the table structure you need to parse 3. Any specific requirements or constraints</p> <p>This will help me design the extension to fit seamlessly with your existing OPAL architecture.</p>"},{"location":"developer/creating-parsers/","title":"Creating New Parsers","text":"<p>This guide explains how to create a new parser for OPAL to support additional websites.</p>"},{"location":"developer/creating-parsers/#overview","title":"Overview","text":"<p>All parsers inherit from the <code>BaseParser</code> class, which provides common functionality for web scraping.</p>"},{"location":"developer/creating-parsers/#step-1-create-parser-class","title":"Step 1: Create Parser Class","text":"<p>Create a new Python file in the <code>opal</code> directory:</p> <pre><code>from opal.BaseParser import BaseParser\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass ParserExample(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"ExampleParser\"\n</code></pre>"},{"location":"developer/creating-parsers/#step-2-implement-required-methods","title":"Step 2: Implement Required Methods","text":""},{"location":"developer/creating-parsers/#get_article_links","title":"get_article_links()","text":"<p>Extract article URLs from the main page:</p> <pre><code>def get_article_links(self, page_url):\n    \"\"\"Extract article links from a page.\"\"\"\n    response = requests.get(page_url, headers=self.headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    links = []\n    for article in soup.find_all('article'):\n        link = article.find('a')\n        if link and link.get('href'):\n            full_url = self.url + link['href']\n            links.append(full_url)\n\n    return links\n</code></pre>"},{"location":"developer/creating-parsers/#parse_article","title":"parse_article()","text":"<p>Extract data from individual articles:</p> <pre><code>def parse_article(self, article_url):\n    \"\"\"Parse individual article.\"\"\"\n    response = requests.get(article_url, headers=self.headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    return {\n        'title': soup.find('h1').text.strip(),\n        'content': soup.find('div', class_='content').text.strip(),\n        'date': soup.find('time')['datetime'],\n        'author': soup.find('span', class_='author').text.strip(),\n        'url': article_url\n    }\n</code></pre>"},{"location":"developer/creating-parsers/#extract_article_data","title":"extract_article_data()","text":"<p>Main method that orchestrates the scraping:</p> <pre><code>def extract_article_data(self):\n    \"\"\"Main extraction method.\"\"\"\n    all_articles = []\n\n    for page in range(1, self.max_pages + 1):\n        page_url = f\"{self.url}/page/{page}\"\n        links = self.get_article_links(page_url)\n\n        for link in links:\n            try:\n                article_data = self.parse_article(link)\n                all_articles.append(article_data)\n            except Exception as e:\n                self.logger.error(f\"Error parsing {link}: {e}\")\n\n    return all_articles\n</code></pre>"},{"location":"developer/creating-parsers/#step-3-handle-special-cases","title":"Step 3: Handle Special Cases","text":""},{"location":"developer/creating-parsers/#javascript-rendered-content","title":"JavaScript-Rendered Content","text":"<p>Use Selenium for dynamic content:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndef setup_driver(self):\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless')\n    service = Service(ChromeDriverManager().install())\n    return webdriver.Chrome(service=service, options=options)\n</code></pre>"},{"location":"developer/creating-parsers/#pagination","title":"Pagination","text":"<p>Handle different pagination styles:</p> <pre><code>def get_next_page_url(self, current_page):\n    # URL parameter style\n    return f\"{self.url}?page={current_page}\"\n\n    # Or path style\n    return f\"{self.url}/page/{current_page}\"\n\n    # Or offset style\n    offset = (current_page - 1) * 20\n    return f\"{self.url}?offset={offset}\"\n</code></pre>"},{"location":"developer/creating-parsers/#step-4-register-parser","title":"Step 4: Register Parser","text":"<p>Add your parser to <code>__main__.py</code>:</p> <pre><code>from opal.ParserExample import ParserExample\n\n# In the parser selection logic\nif args.parser == 'example':\n    parser = ParserExample(args.url, args.suffix, args.max_pages)\n</code></pre>"},{"location":"developer/creating-parsers/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Always wrap parsing logic in try-except blocks</li> <li>Logging: Use self.logger for debugging information</li> <li>Headers: Use appropriate User-Agent headers</li> <li>Rate Limiting: Add delays between requests if needed</li> <li>Testing: Test with various edge cases (empty content, missing elements)</li> </ol>"},{"location":"developer/creating-parsers/#example-complete-parser","title":"Example: Complete Parser","text":"<pre><code>from opal.BaseParser import BaseParser\nfrom bs4 import BeautifulSoup\nimport requests\nimport time\n\nclass ParserNewsSite(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"NewsSiteParser\"\n\n    def get_article_links(self, page_url):\n        response = requests.get(page_url, headers=self.headers)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        links = []\n        for item in soup.select('.article-item'):\n            link = item.select_one('a.title-link')\n            if link:\n                full_url = self.url + link['href']\n                links.append(full_url)\n\n        return links\n\n    def parse_article(self, article_url):\n        response = requests.get(article_url, headers=self.headers)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        return {\n            'title': soup.select_one('h1.article-title').text.strip(),\n            'content': soup.select_one('.article-body').text.strip(),\n            'date': soup.select_one('time.publish-date')['datetime'],\n            'author': soup.select_one('.author-name').text.strip(),\n            'url': article_url,\n            'tags': [tag.text for tag in soup.select('.tag')]\n        }\n\n    def extract_article_data(self):\n        all_articles = []\n\n        for page in range(1, self.max_pages + 1):\n            page_url = f\"{self.url}/articles?page={page}\"\n            self.logger.info(f\"Scraping page {page}\")\n\n            links = self.get_article_links(page_url)\n\n            for link in links:\n                try:\n                    article = self.parse_article(link)\n                    all_articles.append(article)\n                    time.sleep(1)  # Be respectful\n                except Exception as e:\n                    self.logger.error(f\"Error: {e}\")\n\n        return all_articles\n</code></pre>"},{"location":"developer/creating-parsers/#testing-your-parser","title":"Testing Your Parser","text":"<pre><code># Test with a small number of pages first\npython -m opal --url https://example.com --parser example --max_pages 2\n\n# Check the output\ncat opal_output.json | python -m json.tool\n</code></pre>"},{"location":"developer/user_agent_headers_guide/","title":"User-Agent Headers Guide","text":""},{"location":"developer/user_agent_headers_guide/#what-is-a-user-agent","title":"What is a User-Agent?","text":"<p>User-Agent headers are strings that identify the client (browser, bot, or application) making an HTTP request to a web server.</p> <p>It's an HTTP header that tells the server: - What software is making the request - What version it is - What operating system it's running on</p>"},{"location":"developer/user_agent_headers_guide/#examples-of-user-agent-strings","title":"Examples of User-Agent Strings","text":""},{"location":"developer/user_agent_headers_guide/#chrome-browser","title":"Chrome Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#python-requests-default","title":"Python Requests (default)","text":"<pre><code>python-requests/2.28.0\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#googlebot","title":"Googlebot","text":"<pre><code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#firefox-browser","title":"Firefox Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#safari-browser","title":"Safari Browser","text":"<pre><code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#why-user-agents-matter","title":"Why User-Agents Matter","text":"<ol> <li>Server Behavior: Websites may serve different content based on User-Agent</li> <li>Access Control: Some sites block requests with suspicious or missing User-Agents</li> <li>Analytics: Helps websites understand their traffic</li> <li>Bot Detection: Sites use it to identify and potentially block scrapers</li> <li>Content Optimization: Sites may serve mobile vs desktop versions</li> </ol>"},{"location":"developer/user_agent_headers_guide/#setting-user-agent-in-python","title":"Setting User-Agent in Python","text":""},{"location":"developer/user_agent_headers_guide/#basic-example","title":"Basic Example","text":"<pre><code>import requests\n\n# Without User-Agent (might be blocked)\nresponse = requests.get('https://example.com')\n\n# With User-Agent (appears as a browser)\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#advanced-example-with-multiple-headers","title":"Advanced Example with Multiple Headers","text":"<pre><code>headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Accept-Encoding': 'gzip, deflate',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n}\n\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#common-user-agent-patterns","title":"Common User-Agent Patterns","text":""},{"location":"developer/user_agent_headers_guide/#desktop-browsers","title":"Desktop Browsers","text":"<pre><code># Windows Chrome\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n\n# macOS Safari\n'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'\n\n# Linux Firefox\n'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#mobile-browsers","title":"Mobile Browsers","text":"<pre><code># iPhone Safari\n'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n\n# Android Chrome\n'Mozilla/5.0 (Linux; Android 11; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#custom-bot-honest-approach","title":"Custom Bot (Honest Approach)","text":"<pre><code>'OPAL-Bot/1.0 (+https://github.com/yourusername/opal)'\n'MyCompany-Scraper/2.1 (contact@mycompany.com)'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#implementing-user-agents-in-opal","title":"Implementing User-Agents in OPAL","text":""},{"location":"developer/user_agent_headers_guide/#enhanced-baseparser","title":"Enhanced BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers with proper headers\"\"\"\n\n    # Realistic browser headers\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.9',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'DNT': '1',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n    }\n\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, headers=headers, timeout=5)\n            response.raise_for_status()\n            responses.append(response.text)\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n\n    return responses, successful_urls\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#user-agent-rotation","title":"User-Agent Rotation","text":"<pre><code>import random\n\nclass RotatingUserAgentParser(BaseParser):\n    def __init__(self):\n        super().__init__()\n        self.user_agents = [\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0',\n        ]\n\n    def get_random_user_agent(self):\n        return random.choice(self.user_agents)\n\n    def make_request(self, urls):\n        # Use different User-Agent for each request\n        headers = {'User-Agent': self.get_random_user_agent()}\n        # ... rest of request logic\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#best-practices","title":"Best Practices","text":""},{"location":"developer/user_agent_headers_guide/#1-be-strategic","title":"1. Be Strategic","text":"<ul> <li>Use real User-Agents: Copy from actual browsers</li> <li>Stay current: Browser versions change frequently</li> <li>Match behavior: If you claim to be Chrome, act like Chrome</li> </ul>"},{"location":"developer/user_agent_headers_guide/#2-be-respectful","title":"2. Be Respectful","text":"<ul> <li>Respect robots.txt: Even with a browser User-Agent</li> <li>Rate limit: Don't overwhelm servers</li> <li>Be honest when possible: Some sites appreciate transparent bots</li> </ul>"},{"location":"developer/user_agent_headers_guide/#3-be-consistent","title":"3. Be Consistent","text":"<ul> <li>Use complete headers: Include Accept, Accept-Language, etc.</li> <li>Maintain session: Use the same User-Agent throughout a session</li> <li>Handle responses: Check if the site is behaving differently</li> </ul>"},{"location":"developer/user_agent_headers_guide/#4-be-prepared","title":"4. Be Prepared","text":"<ul> <li>Rotate User-Agents: Avoid detection patterns</li> <li>Handle blocks: Have fallback strategies</li> <li>Monitor changes: Sites may update their detection methods</li> </ul>"},{"location":"developer/user_agent_headers_guide/#user-agent-detection-techniques","title":"User-Agent Detection Techniques","text":"<p>Websites can detect fake User-Agents by:</p> <ol> <li>Header Analysis: Checking if browser behavior matches the User-Agent</li> <li>Missing Headers: Looking for headers real browsers always send</li> <li>JavaScript Testing: Testing browser capabilities that match the claimed version</li> <li>Request Patterns: Analyzing timing and request sequences</li> <li>Feature Detection: Checking for browser-specific features</li> </ol>"},{"location":"developer/user_agent_headers_guide/#common-mistakes","title":"Common Mistakes","text":""},{"location":"developer/user_agent_headers_guide/#1-outdated-user-agents","title":"1. Outdated User-Agents","text":"<pre><code># Bad - very old browser version\n'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 Chrome/45.0.2454.85'\n\n# Good - recent browser version\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/91.0.4472.124'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#2-inconsistent-headers","title":"2. Inconsistent Headers","text":"<pre><code># Bad - claims to be Chrome but uses Firefox Accept header\nheaders = {\n    'User-Agent': 'Chrome/91.0.4472.124',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'  # Firefox style\n}\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#3-missing-common-headers","title":"3. Missing Common Headers","text":"<pre><code># Bad - only User-Agent\nheaders = {'User-Agent': 'Mozilla/5.0...'}\n\n# Good - realistic browser headers\nheaders = {\n    'User-Agent': 'Mozilla/5.0...',\n    'Accept': 'text/html,application/xhtml+xml...',\n    'Accept-Language': 'en-US,en;q=0.9',\n    'Accept-Encoding': 'gzip, deflate, br',\n}\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#testing-user-agents","title":"Testing User-Agents","text":""},{"location":"developer/user_agent_headers_guide/#check-what-youre-sending","title":"Check What You're Sending","text":"<pre><code>import requests\n\n# Test your headers\nresponse = requests.get('https://httpbin.org/headers', headers=your_headers)\nprint(response.json())\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#verify-server-response","title":"Verify Server Response","text":"<pre><code># Check if the site is treating you differently\nresponse_bot = requests.get(url)  # Default requests User-Agent\nresponse_browser = requests.get(url, headers=browser_headers)\n\nif response_bot.content != response_browser.content:\n    print(\"Site serves different content based on User-Agent\")\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#tools-and-resources","title":"Tools and Resources","text":"<ul> <li>Browser DevTools: Copy real User-Agent strings from Network tab</li> <li>User-Agent Databases: Sites like whatismybrowser.com</li> <li>Header Checkers: Use httpbin.org to test your headers</li> <li>Browser Testing: Use Selenium to see what real browsers send</li> </ul>"},{"location":"developer/user_agent_headers_guide/#conclusion","title":"Conclusion","text":"<p>User-Agent headers are a crucial part of web scraping that can mean the difference between successful data extraction and being blocked. Use them thoughtfully and responsibly to build robust scrapers that respect both the technical and ethical aspects of web crawling.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>OPAL can be configured through command-line arguments and environment variables.</p>"},{"location":"getting-started/configuration/#command-line-arguments","title":"Command-Line Arguments","text":""},{"location":"getting-started/configuration/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>--url</code>: The base URL to scrape</li> <li><code>--parser</code>: The parser to use (Parser1819, ParserDailyNews, court)</li> </ul>"},{"location":"getting-started/configuration/#optional-arguments","title":"Optional Arguments","text":"<ul> <li><code>--suffix</code>: URL suffix for news articles (default: '')</li> <li><code>--max_pages</code>: Maximum number of pages to scrape (default: 5)</li> <li><code>--output</code>: Output file path (default: opal_output.json)</li> <li><code>--log-level</code>: Logging level (DEBUG, INFO, WARNING, ERROR)</li> </ul>"},{"location":"getting-started/configuration/#parser-specific-configuration","title":"Parser-Specific Configuration","text":""},{"location":"getting-started/configuration/#news-parsers-parser1819-parserdailynews","title":"News Parsers (Parser1819, ParserDailyNews)","text":"<ul> <li>Require <code>--suffix</code> parameter for article URLs</li> <li>Support pagination with <code>--max_pages</code></li> </ul>"},{"location":"getting-started/configuration/#court-parser","title":"Court Parser","text":"<ul> <li>Automatically handles Chrome WebDriver setup</li> <li>Processes all available court cases</li> <li>No pagination parameters needed</li> </ul>"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"<p>By default, OPAL outputs data in JSON format to <code>opal_output.json</code>. You can specify a different output file:</p> <pre><code>python -m opal --url https://example.com --parser Parser1819 --output my_data.json\n</code></pre>"},{"location":"getting-started/configuration/#logging","title":"Logging","text":"<p>Control logging verbosity with <code>--log-level</code>:</p> <pre><code>python -m opal --url https://example.com --parser Parser1819 --log-level DEBUG\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.6 or higher</li> <li>Chrome browser (for court parser)</li> </ul>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/alabama-forward/opal_beautifulsoup\ncd opal\n\n#Install dependencies\npip install -r requirements.txt\n\n#Install OPAL\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>#Check OPAL is installed\npython -m opal --help\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with OPAL quickly.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#news-scraping","title":"News Scraping","text":"<p>To scrape news articles from 1819news.com:</p> <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5\n</code></pre>"},{"location":"getting-started/quickstart/#court-records-scraping","title":"Court Records Scraping","text":"<p>To scrape court cases from Alabama Appeals Court:</p> <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre>"},{"location":"getting-started/quickstart/#common-options","title":"Common Options","text":"<ul> <li><code>--url</code>: The base URL to scrape</li> <li><code>--parser</code>: The parser to use (Parser1819, ParserDailyNews, court)</li> <li><code>--suffix</code>: URL suffix for news articles</li> <li><code>--max_pages</code>: Maximum number of pages to scrape</li> <li><code>--output</code>: Output file path (default: opal_output.json)</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the CLI Usage Guide for detailed command options</li> <li>Check Available Parsers for all supported websites</li> <li>Learn about Output Formats for data analysis</li> </ul>"},{"location":"user-guide/cli-usage/","title":"CLI Usage","text":"<p>The OPAL command-line interface provides a simple way to scrape content from supported websites.</p>"},{"location":"user-guide/cli-usage/#basic-command-structure","title":"Basic Command Structure","text":"<pre><code>python -m opal --url &lt;URL&gt; --parser &lt;PARSER&gt; [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli-usage/#arguments","title":"Arguments","text":""},{"location":"user-guide/cli-usage/#required-arguments","title":"Required Arguments","text":"Argument Description Example <code>--url</code> The base URL to scrape <code>https://1819news.com/</code> <code>--parser</code> Parser to use <code>Parser1819</code>, <code>ParserDailyNews</code>, <code>court</code>"},{"location":"user-guide/cli-usage/#optional-arguments","title":"Optional Arguments","text":"Argument Description Default <code>--suffix</code> URL suffix for articles <code>''</code> <code>--max_pages</code> Maximum pages to scrape <code>5</code> <code>--output</code> Output file path <code>opal_output.json</code> <code>--log-level</code> Logging level <code>INFO</code>"},{"location":"user-guide/cli-usage/#examples","title":"Examples","text":""},{"location":"user-guide/cli-usage/#scraping-1819-news","title":"Scraping 1819 News","text":"<pre><code>python -m opal \\\n    --url https://1819news.com/ \\\n    --parser Parser1819 \\\n    --suffix /news/item \\\n    --max_pages 10 \\\n    --output 1819_articles.json\n</code></pre>"},{"location":"user-guide/cli-usage/#scraping-alabama-daily-news","title":"Scraping Alabama Daily News","text":"<pre><code>python -m opal \\\n    --url https://www.aldailynews.com/ \\\n    --parser ParserDailyNews \\\n    --suffix /news/item \\\n    --max_pages 5\n</code></pre>"},{"location":"user-guide/cli-usage/#scraping-court-records","title":"Scraping Court Records","text":"<pre><code>python -m opal \\\n    --url https://publicportal.alappeals.gov/portal/search/case/results \\\n    --parser court \\\n    --output court_cases.json\n</code></pre>"},{"location":"user-guide/cli-usage/#output","title":"Output","text":"<p>All scraped data is saved in JSON format with the following structure:</p> <pre><code>{\n  \"results\": [\n    {\n      \"title\": \"Article Title\",\n      \"content\": \"Article content...\",\n      \"date\": \"2024-01-01\",\n      \"url\": \"https://example.com/article\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"Parser1819\",\n    \"scraped_at\": \"2024-01-01T12:00:00\",\n    \"total_items\": 25\n  }\n}\n</code></pre>"},{"location":"user-guide/output-formats/","title":"Output Formats","text":"<p>OPAL outputs scraped data in structured JSON format for easy analysis and processing.</p>"},{"location":"user-guide/output-formats/#json-structure","title":"JSON Structure","text":""},{"location":"user-guide/output-formats/#news-articles","title":"News Articles","text":"<pre><code>{\n  \"results\": [\n    {\n      \"title\": \"Article headline\",\n      \"content\": \"Full article text...\",\n      \"date\": \"2024-01-15\",\n      \"author\": \"John Doe\",\n      \"url\": \"https://example.com/article-url\",\n      \"tags\": [\"politics\", \"alabama\"],\n      \"image_url\": \"https://example.com/image.jpg\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"Parser1819\",\n    \"base_url\": \"https://1819news.com/\",\n    \"scraped_at\": \"2024-01-15T10:30:00Z\",\n    \"total_items\": 50,\n    \"pages_scraped\": 5\n  }\n}\n</code></pre>"},{"location":"user-guide/output-formats/#court-cases","title":"Court Cases","text":"<pre><code>{\n  \"results\": [\n    {\n      \"case_number\": \"2024-CV-001234\",\n      \"case_title\": \"State v. Defendant\",\n      \"court\": \"Alabama Court of Civil Appeals\",\n      \"date_filed\": \"2024-01-10\",\n      \"status\": \"Active\",\n      \"parties\": {\n        \"plaintiff\": \"State of Alabama\",\n        \"defendant\": \"John Doe\"\n      },\n      \"docket_entries\": [\n        {\n          \"date\": \"2024-01-10\",\n          \"description\": \"Case filed\",\n          \"document_url\": \"https://example.com/doc.pdf\"\n        }\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"ParserAppealsAL\",\n    \"scraped_at\": \"2024-01-15T10:30:00Z\",\n    \"total_cases\": 25\n  }\n}\n</code></pre>"},{"location":"user-guide/output-formats/#working-with-output","title":"Working with Output","text":""},{"location":"user-guide/output-formats/#python-example","title":"Python Example","text":"<pre><code>import json\n\n# Load scraped data\nwith open('opal_output.json', 'r') as f:\n    data = json.load(f)\n\n# Access articles\nfor article in data['results']:\n    print(f\"Title: {article['title']}\")\n    print(f\"Date: {article['date']}\")\n    print(f\"URL: {article['url']}\")\n    print(\"---\")\n\n# Get metadata\nprint(f\"Total items: {data['metadata']['total_items']}\")\n</code></pre>"},{"location":"user-guide/output-formats/#data-analysis","title":"Data Analysis","text":"<p>The JSON output can be easily imported into: - Pandas DataFrames for analysis - Excel/CSV for spreadsheet work - Database systems for storage - Visualization tools for insights</p>"},{"location":"user-guide/output-formats/#error-handling","title":"Error Handling","text":"<p>Failed scrapes include error information:</p> <pre><code>{\n  \"results\": [],\n  \"metadata\": {\n    \"source\": \"Parser1819\",\n    \"error\": \"Connection timeout\",\n    \"scraped_at\": \"2024-01-15T10:30:00Z\"\n  }\n}\n</code></pre>"},{"location":"user-guide/parsers/","title":"Available Parsers","text":"<p>OPAL includes several parsers for different Alabama news and government websites.</p>"},{"location":"user-guide/parsers/#news-parsers","title":"News Parsers","text":""},{"location":"user-guide/parsers/#parser1819","title":"Parser1819","text":"<ul> <li>Website: 1819 News</li> <li>Content: Conservative news outlet covering Alabama politics and culture</li> <li>Usage:    <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#parserdailynews","title":"ParserDailyNews","text":"<ul> <li>Website: Alabama Daily News</li> <li>Content: Daily news covering Alabama politics, business, and current events</li> <li>Usage:   <pre><code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /news/item\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#government-parsers","title":"Government Parsers","text":""},{"location":"user-guide/parsers/#court-parser-parserappealsal","title":"Court Parser (ParserAppealsAL)","text":"<ul> <li>Website: Alabama Appeals Court Public Portal</li> <li>Content: Court cases, opinions, and legal documents</li> <li>Features:</li> <li>Automated Chrome WebDriver handling</li> <li>JavaScript-rendered content support</li> <li>Case details extraction</li> <li>Usage:   <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#parser-capabilities","title":"Parser Capabilities","text":"Parser Pagination JavaScript Support Authentication Parser1819 \u2713 \u2717 \u2717 ParserDailyNews \u2713 \u2717 \u2717 ParserAppealsAL \u2713 \u2713 \u2717"},{"location":"user-guide/parsers/#adding-new-parsers","title":"Adding New Parsers","text":"<p>To add support for a new website, see the Creating New Parsers guide.</p>"}]}