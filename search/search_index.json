{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OPAL - Oppositional Positions in ALabama","text":"<p>Welcome to OPAL's documentation! OPAL is a web scraping tool that extracts content from websites like Alabama news sites and court records.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udcf0 Multiple News Sources - Parse articles from 1819news.com and Alabama Daily News</li> <li>\u2696\ufe0f Court Records - Extract data from Alabama Appeals Court Public Portal</li> <li>\ud83d\udd27 Extensible Architecture - Easy to add new parsers</li> <li>\ud83d\udcca Structured Output - Clean JSON format for analysis</li> <li>\ud83d\ude80 CLI Tool - Simple command-line interface</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#install-opal","title":"Install OPAL","text":"<p>pip install -e .</p>"},{"location":"#scrape-news-articles","title":"Scrape news articles","text":"<p>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5</p>"},{"location":"#scrape-court-cases","title":"Scrape court cases","text":"<p>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court</p>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li>Getting Started - Installation and setup</li> <li>User Guide - How to use OPAL</li> <li>Developer Guide - Extend Opal with new parsers</li> </ul>"},{"location":"#built-by-alabama-forward","title":"Built by Alabama Forward","text":"<p>This project was created by Gabriel Cab\u00e1n Cubero, Data Director at Alabama Forward.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>Complete reference for all OPAL components and their integration points.</p>"},{"location":"api_reference/#core-components","title":"Core Components","text":""},{"location":"api_reference/#parserappealsal","title":"ParserAppealsAL","text":"<p>The main court parser for Alabama Appeals Court system.</p> <pre><code>from opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL(\n    headless=True,\n    rate_limit_seconds=3\n)\n\n# Parse a specific URL\nresult = parser.parse_article(url)\n\n# Process all court cases from a search\ncases = parser.parse_all_cases(base_url, page_urls)\n</code></pre> <p>Constructor Parameters: - <code>headless</code> (bool): Run browser in headless mode (default: True) - <code>rate_limit_seconds</code> (int): Seconds between requests (default: 3)</p> <p>Key Methods: - <code>parse_article(url)</code> - Parse single page/URL - <code>parse_all_cases(base_url, page_urls)</code> - Parse multiple pages - <code>_setup_driver()</code> - Initialize WebDriver - <code>_close_driver()</code> - Clean up WebDriver</p>"},{"location":"api_reference/#courtsearchbuilder","title":"CourtSearchBuilder","text":"<p>Builder class for constructing Alabama Court search URLs with court-specific parameters.</p> <pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\n\n# Create builder\nbuilder = CourtSearchBuilder()\n\n# Configure search\nbuilder.set_court(\"civil\")\nbuilder.set_date_range(period=\"1m\")\nbuilder.set_case_category(\"Appeal\")\nbuilder.set_exclude_closed(True)\n\n# Build URL\nsearch_url = builder.build_url(page_number=0)\n</code></pre> <p>Available Methods:</p> <p>Court Configuration: - <code>set_court(court_key)</code> - Set court ('civil', 'criminal', 'supreme') - <code>get_court_info()</code> - Get current court information - <code>discover_court_ids(parser_instance)</code> - Auto-discover court IDs - <code>set_court_id_manually(court_key, court_id)</code> - Manual court ID override</p> <p>Search Parameters: - <code>set_date_range(start_date=None, end_date=None, period='1y')</code> - Date filtering - <code>set_case_category(category_name=None)</code> - Case type filtering - <code>set_case_number_filter(case_number=None)</code> - Case number filtering - <code>set_case_title_filter(title=None)</code> - Case title filtering - <code>set_exclude_closed(exclude=False)</code> - Exclude closed cases</p> <p>URL Building: - <code>build_url(page_number=0)</code> - Build complete search URL - <code>build_criteria_string()</code> - Build URL criteria parameters</p>"},{"location":"api_reference/#extract_court_cases_with_params","title":"extract_court_cases_with_params()","text":"<p>Main extraction function supporting both parameter-based and URL-based searches.</p> <pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\n# Parameter-based search\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='1m',\n    case_category='Appeal',\n    exclude_closed=True,\n    max_pages=5,\n    output_prefix=\"civil_appeals\"\n)\n\n# Custom URL search\nresults = extract_court_cases_with_params(\n    custom_url=\"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\",\n    max_pages=5\n)\n</code></pre> <p>Parameters: - <code>court</code> (str): Court type ('civil', 'criminal', 'supreme') - <code>date_period</code> (str): Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - <code>start_date</code>, <code>end_date</code> (str): Custom date range (YYYY-MM-DD) - <code>case_number</code>, <code>case_title</code>, <code>case_category</code> (str): Filtering options - <code>exclude_closed</code> (bool): Exclude closed cases - <code>max_pages</code> (int): Maximum pages to process - <code>output_prefix</code> (str): Output file prefix - <code>custom_url</code> (str): Pre-built search URL (overrides search params)</p>"},{"location":"api_reference/#courturlpaginator-functions","title":"CourtURLPaginator Functions","text":"<p>Utilities for handling court system pagination.</p> <pre><code>from opal.court_url_paginator import (\n    parse_court_url,\n    build_court_url,\n    paginate_court_urls,\n    extract_total_pages_from_first_load,\n    is_court_url\n)\n\n# Parse pagination info\ncurrent_page, total_pages = parse_court_url(court_url)\n\n# Build page-specific URL\npage_3_url = build_court_url(base_url, 3)\n\n# Generate all page URLs\nall_urls = paginate_court_urls(first_page_url, parser)\n</code></pre> <p>Key Functions: - <code>parse_court_url(url)</code> - Extract page info from URL \u2192 (current_page, total_pages) - <code>build_court_url(base_url, page_number)</code> - Build URL for specific page - <code>paginate_court_urls(base_url, parser=None)</code> - Generate all page URLs - <code>extract_total_pages_from_first_load(url, parser)</code> - Get total pages dynamically - <code>is_court_url(url)</code> - Validate Appeals Court URL</p>"},{"location":"api_reference/#integratedparser","title":"IntegratedParser","text":"<p>Unified interface for court and news parsing with automatic detection.</p> <pre><code>from opal.integrated_parser import IntegratedParser\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Create integrated parser with specific parser class\nparser = IntegratedParser(ParserAppealsAL)\n\n# Process URL (auto-detects court vs news)\nresult = parser.process_site(url)\n</code></pre> <p>Constructor: - <code>parser_class</code> (Type[BaseParser]): Parser class to use</p> <p>Key Methods: - <code>process_site(base_url, suffix=\"\", max_pages=None)</code> - Process entire site</p>"},{"location":"api_reference/#extract_all_court_cases","title":"extract_all_court_cases()","text":"<p>Standalone script function for extracting all available court cases.</p> <pre><code>from opal.extract_all_court_cases import extract_all_court_cases\n\n# Extract all cases (hardcoded pagination)\nresults = extract_all_court_cases()\n</code></pre>"},{"location":"api_reference/#integration-patterns","title":"Integration Patterns","text":""},{"location":"api_reference/#1-basic-court-extraction","title":"1. Basic Court Extraction","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\n# Simple extraction\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='7d',\n    exclude_closed=True\n)\n\nif results and results['status'] == 'success':\n    cases = results['cases']\n    print(f\"Found {len(cases)} cases\")\n</code></pre>"},{"location":"api_reference/#2-advanced-search-building","title":"2. Advanced Search Building","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Create components\nbuilder = CourtSearchBuilder()\nparser = ParserAppealsAL(headless=True)\n\n# Auto-discover court IDs\nbuilder.discover_court_ids(parser)\n\n# Build complex search\nbuilder.set_court('supreme')\nbuilder.set_date_range(start_date='2024-01-01', end_date='2024-12-31', period='custom')\nbuilder.set_case_category('Certiorari')\n\n# Get URL and extract\nsearch_url = builder.build_url()\nresults = extract_court_cases_with_params(custom_url=search_url)\n</code></pre>"},{"location":"api_reference/#3-manual-pagination-processing","title":"3. Manual Pagination Processing","text":"<pre><code>from opal.court_url_paginator import paginate_court_urls, parse_court_url\nfrom opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL()\n\n# Get all page URLs\npage_urls = paginate_court_urls(search_url, parser)\n\n# Process each page manually\nall_cases = []\nfor i, url in enumerate(page_urls):\n    print(f\"Processing page {i+1}/{len(page_urls)}\")\n    result = parser.parse_article(url)\n    if 'cases' in result:\n        all_cases.extend(result['cases'])\n\nparser._close_driver()\n</code></pre>"},{"location":"api_reference/#4-integration-with-error-handling","title":"4. Integration with Error Handling","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef safe_extract(court_type, **kwargs):\n    \"\"\"Extract with comprehensive error handling\"\"\"\n    try:\n        results = extract_court_cases_with_params(\n            court=court_type,\n            **kwargs\n        )\n\n        if results and results['status'] == 'success':\n            return results['cases']\n        else:\n            logging.error(\"Extraction failed\")\n            return []\n\n    except Exception as e:\n        logging.error(f\"Extraction error: {e}\")\n        return []\n\n# Usage\ncases = safe_extract('civil', date_period='1m', max_pages=5)\n</code></pre>"},{"location":"api_reference/#5-batch-processing-multiple-courts","title":"5. Batch Processing Multiple Courts","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nimport json\nfrom datetime import datetime\n\ndef batch_extract_courts(courts, date_period=\"1m\", max_pages=5):\n    \"\"\"Extract data for multiple courts\"\"\"\n    results = {}\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n    for court in courts:\n        print(f\"Processing {court} court...\")\n\n        try:\n            court_results = extract_court_cases_with_params(\n                court=court,\n                date_period=date_period,\n                max_pages=max_pages,\n                output_prefix=f\"{court}_{timestamp}\"\n            )\n\n            if court_results:\n                results[court] = {\n                    'status': 'success',\n                    'total_cases': court_results['total_cases'],\n                    'pages_processed': court_results['pages_processed']\n                }\n            else:\n                results[court] = {'status': 'failed'}\n\n        except Exception as e:\n            results[court] = {'status': 'error', 'error': str(e)}\n\n    # Save summary\n    with open(f\"batch_summary_{timestamp}.json\", 'w') as f:\n        json.dump(results, f, indent=2)\n\n    return results\n\n# Usage\nresults = batch_extract_courts(['civil', 'criminal', 'supreme'])\n</code></pre>"},{"location":"api_reference/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"api_reference/#standard-extraction-flow","title":"Standard Extraction Flow","text":"<pre><code>graph TD\n    A[User Input] --&gt; B{Use Custom URL?}\n    B --&gt;|Yes| C[Extract with Custom URL]\n    B --&gt;|No| D[CourtSearchBuilder]\n    D --&gt; E[Discover Court IDs]\n    E --&gt; F[Build Search Parameters]\n    F --&gt; G[Generate Search URL]\n    C --&gt; H[ParserAppealsAL]\n    G --&gt; H\n    H --&gt; I[Extract First Page]\n    I --&gt; J[Determine Total Pages]\n    J --&gt; K[Process All Pages]\n    K --&gt; L[Aggregate Results]\n    L --&gt; M[Generate Outputs]</code></pre>"},{"location":"api_reference/#component-dependencies","title":"Component Dependencies","text":"<pre><code>graph TD\n    A[configurable_court_extractor] --&gt; B[court_case_parser]\n    A --&gt; C[court_url_paginator]\n    B --&gt; D[BaseParser]\n    E[integrated_parser] --&gt; B\n    E --&gt; F[url_catcher_module]\n    G[extract_all_court_cases] --&gt; B\n    G --&gt; C</code></pre>"},{"location":"api_reference/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"api_reference/#custom-extraction-pipeline","title":"Custom Extraction Pipeline","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\nfrom opal.court_url_paginator import paginate_court_urls\nimport asyncio\nimport json\n\nclass AdvancedCourtExtractor:\n    def __init__(self):\n        self.builder = CourtSearchBuilder()\n        self.parser = ParserAppealsAL(headless=True, rate_limit_seconds=1)\n\n    def setup(self):\n        \"\"\"Initialize court IDs\"\"\"\n        self.builder.discover_court_ids(self.parser)\n\n    def create_search(self, court, filters):\n        \"\"\"Create configured search URL\"\"\"\n        self.builder.set_court(court)\n\n        if 'date_period' in filters:\n            self.builder.set_date_range(period=filters['date_period'])\n        if 'case_category' in filters:\n            self.builder.set_case_category(filters['case_category'])\n        if 'exclude_closed' in filters:\n            self.builder.set_exclude_closed(filters['exclude_closed'])\n\n        return self.builder.build_url()\n\n    def extract_with_progress(self, court, filters, max_pages=None):\n        \"\"\"Extract with progress reporting\"\"\"\n        search_url = self.create_search(court, filters)\n        page_urls = paginate_court_urls(search_url, self.parser)\n\n        if max_pages:\n            page_urls = page_urls[:max_pages]\n\n        all_cases = []\n        for i, url in enumerate(page_urls):\n            print(f\"Page {i+1}/{len(page_urls)}: \", end='', flush=True)\n\n            result = self.parser.parse_article(url)\n            if 'cases' in result:\n                cases = result['cases']\n                all_cases.extend(cases)\n                print(f\"{len(cases)} cases\")\n            else:\n                print(\"0 cases\")\n\n        return all_cases\n\n    def cleanup(self):\n        \"\"\"Clean up resources\"\"\"\n        self.parser._close_driver()\n\n# Usage\nextractor = AdvancedCourtExtractor()\nextractor.setup()\n\ntry:\n    cases = extractor.extract_with_progress(\n        'civil', \n        {'date_period': '1m', 'case_category': 'Appeal'},\n        max_pages=3\n    )\n    print(f\"Total extracted: {len(cases)} cases\")\nfinally:\n    extractor.cleanup()\n</code></pre>"},{"location":"api_reference/#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nfrom datetime import datetime, timedelta\nimport time\nimport json\n\nclass CourtCaseMonitor:\n    def __init__(self, courts=['civil', 'criminal', 'supreme']):\n        self.courts = courts\n        self.last_check = {}\n\n    def check_new_cases(self, hours_back=1):\n        \"\"\"Check for new cases in the last N hours\"\"\"\n        new_cases = {}\n\n        for court in self.courts:\n            print(f\"Checking {court} court...\")\n\n            try:\n                results = extract_court_cases_with_params(\n                    court=court,\n                    date_period='7d',  # Check recent cases\n                    max_pages=3,\n                    output_prefix=f\"monitor_{court}\"\n                )\n\n                if results and results['status'] == 'success':\n                    # Filter for truly new cases\n                    cutoff_time = datetime.now() - timedelta(hours=hours_back)\n                    recent_cases = []\n\n                    for case in results['cases']:\n                        try:\n                            filed_date = datetime.strptime(case['filed_date'], '%m/%d/%Y')\n                            if filed_date &gt; cutoff_time:\n                                recent_cases.append(case)\n                        except:\n                            continue\n\n                    new_cases[court] = recent_cases\n                    print(f\"  Found {len(recent_cases)} new cases\")\n                else:\n                    new_cases[court] = []\n\n            except Exception as e:\n                print(f\"  Error checking {court}: {e}\")\n                new_cases[court] = []\n\n        return new_cases\n\n    def monitor_continuously(self, check_interval_minutes=30):\n        \"\"\"Continuously monitor for new cases\"\"\"\n        while True:\n            print(f\"\\n--- Checking at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\")\n\n            new_cases = self.check_new_cases(hours_back=1)\n\n            # Report findings\n            total_new = sum(len(cases) for cases in new_cases.values())\n            if total_new &gt; 0:\n                print(f\"\ud83d\udea8 Found {total_new} new cases!\")\n                for court, cases in new_cases.items():\n                    if cases:\n                        print(f\"  {court}: {len(cases)} new cases\")\n            else:\n                print(\"\u2713 No new cases found\")\n\n            # Wait for next check\n            print(f\"Next check in {check_interval_minutes} minutes...\")\n            time.sleep(check_interval_minutes * 60)\n\n# Usage\nmonitor = CourtCaseMonitor()\nmonitor.check_new_cases(hours_back=24)  # One-time check\n# monitor.monitor_continuously()  # Continuous monitoring\n</code></pre>"},{"location":"api_reference/#best-practices","title":"Best Practices","text":""},{"location":"api_reference/#1-resource-management","title":"1. Resource Management","text":"<pre><code>from opal.court_case_parser import ParserAppealsAL\n\n# Always use try/finally for cleanup\nparser = ParserAppealsAL()\ntry:\n    result = parser.parse_article(url)\n    # Process result...\nfinally:\n    parser._close_driver()\n</code></pre>"},{"location":"api_reference/#2-error-handling","title":"2. Error Handling","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nimport logging\n\ndef robust_extract(court, **kwargs):\n    max_retries = 3\n\n    for attempt in range(max_retries):\n        try:\n            return extract_court_cases_with_params(court=court, **kwargs)\n        except Exception as e:\n            logging.warning(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(5)  # Wait before retry\n</code></pre>"},{"location":"api_reference/#3-configuration-management","title":"3. Configuration Management","text":"<pre><code>import json\n\n# Use configuration files\nconfig = {\n    \"courts\": [\"civil\", \"criminal\"],\n    \"default_date_period\": \"1m\",\n    \"max_pages\": 10,\n    \"rate_limit\": 2,\n    \"output_dir\": \"./results\"\n}\n\ndef extract_with_config(config_path):\n    with open(config_path) as f:\n        config = json.load(f)\n\n    results = {}\n    for court in config['courts']:\n        results[court] = extract_court_cases_with_params(\n            court=court,\n            date_period=config['default_date_period'],\n            max_pages=config['max_pages']\n        )\n    return results\n</code></pre>"},{"location":"api_reference/#4-logging-and-monitoring","title":"4. Logging and Monitoring","text":"<pre><code>import logging\nfrom datetime import datetime\n\n# Set up structured logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(f\"court_extraction_{datetime.now().strftime('%Y%m%d')}.log\"),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger(__name__)\n\n# Log extraction metrics\ndef log_extraction_stats(results):\n    if results and results['status'] == 'success':\n        logger.info(f\"Extraction successful: {results['total_cases']} cases, \"\n                   f\"{results['pages_processed']} pages, \"\n                   f\"completed at {results['extraction_time']}\")\n    else:\n        logger.error(\"Extraction failed\")\n</code></pre> <p>This API reference provides the correct imports, class names, and usage patterns for all OPAL components based on the actual codebase implementation.</p>"},{"location":"about/contributing/","title":"Contributing to OPAL","text":"<p>We welcome contributions to OPAL! This guide will help you get started.</p>"},{"location":"about/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/yourusername/opal.git\ncd opal\n</code></pre></li> <li>Create a new branch for your feature:    <pre><code>git checkout -b feature-name\n</code></pre></li> </ol>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\npip install -e .\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install pytest black flake8\n</code></pre></p> </li> </ol>"},{"location":"about/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"about/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add docstrings to all functions and classes</li> <li>Keep lines under 88 characters (Black's default)</li> </ul>"},{"location":"about/contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new features</li> <li>Ensure all tests pass before submitting</li> <li>Run tests with: <code>pytest tests/</code></li> </ul>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for new features</li> <li>Include docstrings in your code</li> <li>Update README if needed</li> </ul>"},{"location":"about/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"about/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code follows the style guidelines</li> <li>Update documentation as needed</li> <li>Add tests for new functionality</li> <li>Ensure all tests pass</li> <li>Update the CHANGELOG.md with your changes</li> </ol>"},{"location":"about/contributing/#adding-new-parsers","title":"Adding New Parsers","text":"<p>When contributing a new parser:</p> <ol> <li>Follow the BaseParser structure</li> <li>Include comprehensive error handling</li> <li>Add documentation to the parser class</li> <li>Create an example in the documentation</li> <li>Test with various edge cases</li> </ol>"},{"location":"about/contributing/#reporting-issues","title":"Reporting Issues","text":"<ul> <li>Use GitHub Issues to report bugs</li> <li>Include detailed reproduction steps</li> <li>Provide error messages and logs</li> <li>Specify your Python version and OS</li> </ul>"},{"location":"about/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers and help them get started</li> <li>Focus on constructive criticism</li> <li>Respect differing viewpoints</li> </ul>"},{"location":"about/contributing/#questions","title":"Questions?","text":"<p>Feel free to open an issue for any questions about contributing!</p>"},{"location":"about/license/","title":"License","text":"<p>OPAL is released under the MIT License.</p>"},{"location":"about/license/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2025 Gabriel Cab\u00e1n Cubero / Alabama Forward</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/license/#what-this-means","title":"What this means","text":"<ul> <li>You can use OPAL for commercial and non-commercial purposes</li> <li>You can modify the code to suit your needs</li> <li>You can distribute the software</li> <li>You must include the copyright notice in copies</li> <li>The software is provided \"as is\" without warranty</li> </ul>"},{"location":"about/license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>OPAL uses the following open-source libraries:</p> <ul> <li>BeautifulSoup4 - MIT License</li> <li>Requests - Apache License 2.0</li> <li>Selenium - Apache License 2.0</li> <li>MkDocs - BSD License</li> <li>Material for MkDocs - MIT License</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to OPAL, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"api/base-parser/","title":"BaseParser API Reference","text":"<p>The <code>BaseParser</code> class provides the foundation for all OPAL parsers.</p>"},{"location":"api/base-parser/#class-definition","title":"Class Definition","text":"<pre><code>class BaseParser:\n    def __init__(self, url, suffix=\"\", max_pages=5)\n</code></pre>"},{"location":"api/base-parser/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str required Base URL to scrape <code>suffix</code> str <code>\"\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"api/base-parser/#attributes","title":"Attributes","text":""},{"location":"api/base-parser/#headers","title":"headers","text":"<p><pre><code>self.headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\n</code></pre> HTTP headers for requests</p>"},{"location":"api/base-parser/#logger","title":"logger","text":"<p><pre><code>self.logger: logging.Logger\n</code></pre> Logger instance for debugging</p>"},{"location":"api/base-parser/#methods","title":"Methods","text":""},{"location":"api/base-parser/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Main method to extract articles. Must be implemented by subclasses.</p> <p>Returns: List of dictionaries containing article data</p>"},{"location":"api/base-parser/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extract article URLs from a page. Must be implemented by subclasses.</p> <p>Parameters: - <code>page_url</code>: URL of the page to extract links from</p> <p>Returns: List of article URLs</p>"},{"location":"api/base-parser/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parse individual article data. Must be implemented by subclasses.</p> <p>Parameters: - <code>article_url</code>: URL of the article to parse</p> <p>Returns: Dictionary with article data</p>"},{"location":"api/base-parser/#save_to_json","title":"save_to_json()","text":"<p><pre><code>def save_to_json(self, data: List[Dict], filename: str = \"opal_output.json\")\n</code></pre> Save extracted data to JSON file.</p> <p>Parameters: - <code>data</code>: List of article dictionaries - <code>filename</code>: Output filename</p>"},{"location":"api/base-parser/#usage-example","title":"Usage Example","text":"<pre><code>from opal.BaseParser import BaseParser\n\nclass MyParser(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"MyParser\"\n\n    def extract_article_data(self):\n        # Implementation\n        pass\n\n    def get_article_links(self, page_url):\n        # Implementation\n        pass\n\n    def parse_article(self, article_url):\n        # Implementation\n        pass\n\n# Use the parser\nparser = MyParser(\"https://example.com\", suffix=\"/articles\", max_pages=10)\ndata = parser.extract_article_data()\nparser.save_to_json(data)\n</code></pre>"},{"location":"api/court_url_paginator/","title":"Court URL Paginator","text":"<p>The Court URL Paginator module (<code>opal.court_url_paginator</code>) provides utilities for handling pagination in the Alabama Appeals Court Public Portal. It includes functions for parsing, building, and generating paginated URLs.</p>"},{"location":"api/court_url_paginator/#overview","title":"Overview","text":"<p>The Alabama Appeals Court portal (<code>publicportal.alappeals.gov</code>) uses URL-encoded pagination parameters. This module handles:</p> <ul> <li>Parsing page numbers from encoded URLs</li> <li>Building URLs for specific pages</li> <li>Extracting total page count from initial loads</li> <li>Generating complete sets of paginated URLs</li> <li>Validating Appeals Court portal URLs</li> </ul>"},{"location":"api/court_url_paginator/#functions","title":"Functions","text":""},{"location":"api/court_url_paginator/#parse_court_urlurl","title":"<code>parse_court_url(url)</code>","text":"<p>Extracts current page number and total pages from a court URL.</p> <pre><code>from opal.court_url_paginator import parse_court_url\n\nurl = \"https://publicportal.alappeals.gov/portal/search/case/results?...\"\ncurrent_page, total_pages = parse_court_url(url)\n\nprint(f\"Current page: {current_page}, Total pages: {total_pages}\")\n# Current page: 0, Total pages: 5\n</code></pre> <p>Parameters: - <code>url</code> (str): The court URL to parse</p> <p>Returns: - <code>Tuple[Optional[int], Optional[int]]</code>: (current_page, total_pages) or (None, None) if parsing fails</p> <p>URL Pattern Parsed: The function looks for these patterns in decoded URLs: - <code>page~(.*?number~(\\d+)</code> - extracts current page number - <code>totalPages~(\\d+)</code> - extracts total page count</p>"},{"location":"api/court_url_paginator/#build_court_urlbase_url-page_number","title":"<code>build_court_url(base_url, page_number)</code>","text":"<p>Constructs a URL for a specific page number.</p> <pre><code>from opal.court_url_paginator import build_court_url\n\nbase_url = \"https://publicportal.alappeals.gov/portal/search/case/results?...\"\npage_2_url = build_court_url(base_url, 2)\n</code></pre> <p>Parameters: - <code>base_url</code> (str): The original court URL (any page) - <code>page_number</code> (int): The desired page number (0-indexed)</p> <p>Returns: - <code>str</code>: URL for the specified page</p> <p>Implementation: Uses regex to replace the page number in the pattern: <code>page~%28.*?number~X</code></p>"},{"location":"api/court_url_paginator/#extract_total_pages_from_first_loadurl-parser","title":"<code>extract_total_pages_from_first_load(url, parser)</code>","text":"<p>Extracts the total number of pages by loading the first page and checking for JavaScript updates.</p> <pre><code>from opal.court_url_paginator import extract_total_pages_from_first_load\nfrom opal.parser_appeals_al import ParserAppealsAL\n\nparser = ParserAppealsAL()\ntotal_pages = extract_total_pages_from_first_load(court_url, parser)\nprint(f\"Total pages: {total_pages}\")\n</code></pre> <p>Parameters: - <code>url</code> (str): Initial URL (typically page 0) - <code>parser</code>: ParserAppealsAL instance to make the request</p> <p>Returns: - <code>int</code>: Total number of pages (1 if extraction fails)</p> <p>Process: 1. Makes request using the parser 2. Waits for JavaScript to update the URL 3. Parses the updated URL for total page count 4. Falls back to 1 if unable to determine</p>"},{"location":"api/court_url_paginator/#paginate_court_urlsbase_url-parsernone","title":"<code>paginate_court_urls(base_url, parser=None)</code>","text":"<p>Generates a list of URLs for all pages in the search results.</p> <pre><code>from opal.court_url_paginator import paginate_court_urls\nfrom opal.parser_appeals_al import ParserAppealsAL\n\nparser = ParserAppealsAL()\n\n# With parser for dynamic total page detection\nurls = paginate_court_urls(first_url, parser)\n\n# Without parser (uses URL info only)\nurls = paginate_court_urls(first_url)\n\nfor i, url in enumerate(urls):\n    print(f\"Page {i}: {url}\")\n</code></pre> <p>Parameters: - <code>base_url</code> (str): Initial court search URL - <code>parser</code> (optional): ParserAppealsAL instance for dynamic page detection</p> <p>Returns: - <code>List[str]</code>: List of URLs for all pages (0-indexed)</p> <p>Logic: 1. Try to parse total pages from URL 2. If not available and parser provided, load first page to detect 3. Generate URLs for all pages (0 to total_pages-1) 4. Return just base URL if pagination cannot be determined</p>"},{"location":"api/court_url_paginator/#is_court_urlurl","title":"<code>is_court_url(url)</code>","text":"<p>Validates if a URL is from the Alabama Appeals Court portal.</p> <pre><code>from opal.court_url_paginator import is_court_url\n\nif is_court_url(url):\n    print(\"Valid Appeals Court URL\")\nelse:\n    print(\"Not an Appeals Court URL\")\n</code></pre> <p>Parameters: - <code>url</code> (str): URL to validate</p> <p>Returns: - <code>bool</code>: True if URL contains both <code>publicportal.alappeals.gov</code> and <code>/portal/search/case/results</code></p>"},{"location":"api/court_url_paginator/#url-structure","title":"URL Structure","text":"<p>Appeals Court URLs use encoded pagination parameters:</p> <pre><code>https://publicportal.alappeals.gov/portal/search/case/results?searchParams=...page~%28size~25~number~0~totalElements~125~totalPages~5%29\n</code></pre> <p>Key components: - <code>page~%28</code> - Start of page parameter block - <code>size~25</code> - Results per page - <code>number~0</code> - Current page (0-indexed) - <code>totalElements~125</code> - Total result count - <code>totalPages~5</code> - Total number of pages</p>"},{"location":"api/court_url_paginator/#integration-examples","title":"Integration Examples","text":""},{"location":"api/court_url_paginator/#with-parserappealsal","title":"With ParserAppealsAL","text":"<pre><code>from opal.parser_appeals_al import ParserAppealsAL\nfrom opal.court_url_paginator import paginate_court_urls, extract_total_pages_from_first_load\n\nparser = ParserAppealsAL()\n\n# Get total pages dynamically\ntotal_pages = extract_total_pages_from_first_load(search_url, parser)\nprint(f\"Found {total_pages} pages\")\n\n# Generate all page URLs\nall_urls = paginate_court_urls(search_url, parser)\n\n# Process each page\nall_cases = []\nfor i, url in enumerate(all_urls):\n    print(f\"Processing page {i+1}/{len(all_urls)}\")\n    cases = parser.extract_page_data(url)\n    all_cases.extend(cases)\n</code></pre>"},{"location":"api/court_url_paginator/#with-configurable-court-extractor","title":"With Configurable Court Extractor","text":"<p>The configurable court extractor uses these functions internally:</p> <pre><code># Internal usage in configurable_court_extractor.py\ndef _process_paginated_results(self, first_page_url):\n    # Generate URLs for all pages\n    page_urls = paginate_court_urls(first_page_url, self.parser)\n\n    # Process each page\n    for url in page_urls:\n        self._process_page(url)\n</code></pre>"},{"location":"api/court_url_paginator/#manual-pagination-handling","title":"Manual Pagination Handling","text":"<pre><code>from opal.court_url_paginator import parse_court_url, build_court_url\n\n# Parse current state\ncurrent_page, total_pages = parse_court_url(search_url)\n\nif total_pages and total_pages &gt; 1:\n    # Process remaining pages\n    for page_num in range(current_page + 1, total_pages):\n        next_url = build_court_url(search_url, page_num)\n        # Process next_url...\n</code></pre>"},{"location":"api/court_url_paginator/#error-handling","title":"Error Handling","text":"<p>The paginator functions are designed to fail gracefully:</p> <ul> <li><code>parse_court_url</code>: Returns (None, None) if parsing fails</li> <li><code>extract_total_pages_from_first_load</code>: Returns 1 if extraction fails</li> <li><code>build_court_url</code>: Returns original URL if building fails</li> <li><code>paginate_court_urls</code>: Returns single-item list with base URL if pagination fails</li> </ul> <pre><code># Safe usage pattern\nfrom opal.court_url_paginator import paginate_court_urls\n\ntry:\n    urls = paginate_court_urls(court_url, parser)\n    if len(urls) == 1:\n        print(\"Single page or pagination detection failed\")\nexcept Exception as e:\n    print(f\"Pagination error: {e}\")\n    urls = [court_url]  # Fallback to original URL\n</code></pre>"},{"location":"api/court_url_paginator/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>URL parsing is fast and doesn't require network requests</li> <li>Dynamic page detection requires loading the first page</li> <li>Consider caching total page counts for repeated searches</li> <li>Use with rate limiting to avoid overwhelming the server</li> </ul>"},{"location":"api/court_url_paginator/#debugging","title":"Debugging","text":"<p>Enable debug output by checking the console messages:</p> <pre><code>from opal.court_url_paginator import extract_total_pages_from_first_load\n\n# Function prints debug messages:\n# \"Detected X total pages from URL\"\n# \"Error parsing URL: ...\"\n# \"Error extracting total pages: ...\"\n\ntotal_pages = extract_total_pages_from_first_load(url, parser)\n</code></pre>"},{"location":"api/court_url_paginator/#limitations","title":"Limitations","text":"<ol> <li>Appeals Court Specific: Only works with <code>publicportal.alappeals.gov</code> URLs</li> <li>JavaScript Dependency: Requires browser/parser for dynamic page detection</li> <li>URL Structure Dependency: May break if portal changes URL encoding</li> <li>0-Based Indexing: Page numbers are 0-indexed (page 0 is first page)</li> <li>Session Dependency: URLs may be session-based and expire</li> </ol>"},{"location":"api/court_url_paginator/#complete-example","title":"Complete Example","text":"<pre><code>from opal.court_url_paginator import (\n    is_court_url, \n    parse_court_url,\n    paginate_court_urls,\n    extract_total_pages_from_first_load\n)\nfrom opal.parser_appeals_al import ParserAppealsAL\n\ndef process_all_appeals_court_pages(search_url):\n    # Validate URL\n    if not is_court_url(search_url):\n        raise ValueError(\"Not a valid Appeals Court URL\")\n\n    # Parse initial URL\n    current_page, total_pages = parse_court_url(search_url)\n    print(f\"Starting from page {current_page}, total: {total_pages}\")\n\n    # Setup parser\n    parser = ParserAppealsAL()\n\n    # Get total pages if not in URL\n    if total_pages is None:\n        total_pages = extract_total_pages_from_first_load(search_url, parser)\n        print(f\"Detected {total_pages} total pages\")\n\n    # Generate all URLs\n    all_urls = paginate_court_urls(search_url, parser)\n\n    # Process each page\n    results = []\n    for i, url in enumerate(all_urls):\n        print(f\"Processing page {i}/{len(all_urls)-1}\")\n        page_data = parser.extract_page_data(url)\n        results.extend(page_data)\n\n    return results\n\n# Usage\nsearch_url = \"https://publicportal.alappeals.gov/portal/search/case/results?...\"\nall_cases = process_all_appeals_court_pages(search_url)\nprint(f\"Extracted {len(all_cases)} total cases\")\n</code></pre>"},{"location":"api/court_url_paginator/#key-differences-from-other-court-systems","title":"Key Differences from Other Court Systems","text":"<p>This module is specifically designed for the Alabama Appeals Court portal, which differs from other Alabama court systems:</p> <ul> <li>URL Domain: <code>publicportal.alappeals.gov</code> (not <code>alacourt.gov</code>)</li> <li>Pagination: URL-encoded parameters (not JavaScript/AJAX)</li> <li>Page Indexing: 0-based (page 0 is first page)</li> <li>Search Path: <code>/portal/search/case/results</code> (not <code>/ajax/courts.aspx</code>)</li> </ul> <p>Make sure you're using the correct parser and URLs for the Appeals Court system.</p>"},{"location":"api/data_structures/","title":"Data Structures","text":"<p>This document describes the data structures returned by OPAL parsers and extractors.</p>"},{"location":"api/data_structures/#court-parser-data-structures","title":"Court Parser Data Structures","text":""},{"location":"api/data_structures/#individual-court-case","title":"Individual Court Case","text":"<p>Each court case is represented as a dictionary with the following structure:</p> <pre><code>{\n    \"court\": str,              # Court name (e.g., \"Alabama Civil Appeals\")\n    \"case_number\": {\n        \"text\": str,           # Case number (e.g., \"2190259\")\n        \"link\": str            # URL to case details\n    },\n    \"case_title\": str,         # Full case title\n    \"classification\": str,     # Case type (e.g., \"Appeal\", \"Certiorari\")\n    \"filed_date\": str,         # Filing date (MM/DD/YYYY format)\n    \"status\": str              # Case status (e.g., \"Open\", \"Closed\")\n}\n</code></pre>"},{"location":"api/data_structures/#court-search-results","title":"Court Search Results","text":"<p>When using the court parser or configurable extractor, the complete results structure is:</p> <pre><code>{\n    \"status\": str,                    # \"success\" or \"error\"\n    \"search_parameters\": {            # Parameters used for search\n        \"court\": str,\n        \"date_period\": str,\n        \"start_date\": str | None,\n        \"end_date\": str | None,\n        \"case_number\": str | None,\n        \"case_title\": str | None,\n        \"case_category\": str | None,\n        \"exclude_closed\": bool\n    },\n    \"total_cases\": int,               # Total number of cases found\n    \"extraction_date\": str,           # Date of extraction (YYYY-MM-DD)\n    \"extraction_time\": str,           # Time of extraction (HH:MM:SS)\n    \"pages_processed\": int,           # Number of pages processed\n    \"cases\": List[Dict]               # List of case dictionaries (see above)\n}\n</code></pre>"},{"location":"api/data_structures/#paginated-results","title":"Paginated Results","text":"<p>For paginated court results:</p> <pre><code>{\n    \"page\": int,                      # Current page number\n    \"total_pages\": int,               # Total number of pages\n    \"cases_on_page\": int,             # Number of cases on this page\n    \"cases\": List[Dict]               # List of cases for this page\n}\n</code></pre>"},{"location":"api/data_structures/#news-parser-data-structures","title":"News Parser Data Structures","text":""},{"location":"api/data_structures/#news-article","title":"News Article","text":"<p>News articles from alabamanewscenter.com:</p> <pre><code>{\n    \"title\": str,                     # Article title\n    \"link\": str,                      # Article URL\n    \"date\": str,                      # Publication date\n    \"author\": str | None,             # Article author\n    \"summary\": str | None,            # Article summary/excerpt\n    \"content\": str,                   # Full article content\n    \"tags\": List[str],                # Article tags/categories\n    \"image_url\": str | None           # Main article image URL\n}\n</code></pre>"},{"location":"api/data_structures/#news-search-results","title":"News Search Results","text":"<pre><code>{\n    \"status\": str,                    # \"success\" or \"error\"\n    \"search_term\": str | None,        # Search term used (if any)\n    \"category\": str | None,           # Category filter (if any)\n    \"total_articles\": int,            # Total articles found\n    \"articles\": List[Dict]            # List of article dictionaries\n}\n</code></pre>"},{"location":"api/data_structures/#integrated-parser-results","title":"Integrated Parser Results","text":"<p>When using the integrated parser, results include a parser type indicator:</p> <pre><code>{\n    \"parser_type\": str,               # \"court\" or \"news\"\n    \"url\": str,                       # Original URL processed\n    \"data\": Dict                      # Parser-specific results (see above)\n}\n</code></pre>"},{"location":"api/data_structures/#csv-output-formats","title":"CSV Output Formats","text":""},{"location":"api/data_structures/#court-cases-csv","title":"Court Cases CSV","text":"<p>When exporting court cases to CSV:</p> Column Description Example court Court name Alabama Civil Appeals case_number Case number text 2190259 case_number_link URL to case https://... case_title Full case title Smith v. Jones classification Case type Appeal filed_date Filing date 01/15/2024 status Case status Open"},{"location":"api/data_structures/#news-articles-csv","title":"News Articles CSV","text":"<p>When exporting news articles to CSV:</p> Column Description Example title Article title Breaking News... link Article URL https://... date Publication date 2024-01-15 author Author name John Doe summary Article excerpt This article... tags Comma-separated tags politics,local"},{"location":"api/data_structures/#error-response-structure","title":"Error Response Structure","text":"<p>When errors occur:</p> <pre><code>{\n    \"status\": \"error\",\n    \"error_type\": str,                # Type of error\n    \"error_message\": str,             # Detailed error message\n    \"timestamp\": str,                 # When error occurred\n    \"context\": Dict | None            # Additional error context\n}\n</code></pre>"},{"location":"api/data_structures/#metadata-fields","title":"Metadata Fields","text":"<p>Common metadata fields across parsers:</p> <pre><code>{\n    \"extraction_date\": str,           # YYYY-MM-DD format\n    \"extraction_time\": str,           # HH:MM:SS format\n    \"parser_version\": str,            # OPAL version\n    \"processing_time_seconds\": float, # Time taken to process\n    \"source_url\": str                 # Original URL processed\n}\n</code></pre>"},{"location":"api/data_structures/#type-definitions","title":"Type Definitions","text":"<p>For TypeScript or type-aware Python development:</p> <pre><code>from typing import TypedDict, List, Optional, Union\n\nclass CaseNumber(TypedDict):\n    text: str\n    link: str\n\nclass CourtCase(TypedDict):\n    court: str\n    case_number: CaseNumber\n    case_title: str\n    classification: str\n    filed_date: str\n    status: str\n\nclass SearchParameters(TypedDict):\n    court: str\n    date_period: Optional[str]\n    start_date: Optional[str]\n    end_date: Optional[str]\n    case_number: Optional[str]\n    case_title: Optional[str]\n    case_category: Optional[str]\n    exclude_closed: bool\n\nclass CourtSearchResults(TypedDict):\n    status: str\n    search_parameters: SearchParameters\n    total_cases: int\n    extraction_date: str\n    extraction_time: str\n    pages_processed: int\n    cases: List[CourtCase]\n\nclass NewsArticle(TypedDict):\n    title: str\n    link: str\n    date: str\n    author: Optional[str]\n    summary: Optional[str]\n    content: str\n    tags: List[str]\n    image_url: Optional[str]\n</code></pre>"},{"location":"api/data_structures/#data-validation","title":"Data Validation","text":""},{"location":"api/data_structures/#required-fields","title":"Required Fields","text":"<p>Court Cases: All fields are required except <code>case_number.link</code> may be empty for some cases.</p> <p>News Articles: Required fields are <code>title</code>, <code>link</code>, <code>date</code>, and <code>content</code>. Other fields may be null/empty.</p>"},{"location":"api/data_structures/#date-formats","title":"Date Formats","text":"<ul> <li>Court filing dates: <code>MM/DD/YYYY</code> format</li> <li>News publication dates: <code>YYYY-MM-DD</code> format</li> <li>Extraction timestamps: ISO 8601 format</li> </ul>"},{"location":"api/data_structures/#status-values","title":"Status Values","text":"<p>Court case status values: - <code>Open</code> - Active case - <code>Closed</code> - Completed case - <code>Unknown</code> - Status not determined</p>"},{"location":"api/data_structures/#classification-values","title":"Classification Values","text":"<p>Common court case classifications: - <code>Appeal</code> - <code>Certiorari</code> - <code>Original Proceeding</code> - <code>Petition</code> - <code>Certified Question</code> - <code>Other</code></p>"},{"location":"api/data_structures/#usage-examples","title":"Usage Examples","text":""},{"location":"api/data_structures/#accessing-court-case-data","title":"Accessing Court Case Data","text":"<pre><code>results = parser.extract_all_court_cases()\n\nfor case in results['cases']:\n    print(f\"Case: {case['case_number']['text']}\")\n    print(f\"Title: {case['case_title']}\")\n    print(f\"Filed: {case['filed_date']}\")\n    print(f\"Status: {case['status']}\")\n\n    # Access case details URL\n    if case['case_number']['link']:\n        print(f\"Details: {case['case_number']['link']}\")\n</code></pre>"},{"location":"api/data_structures/#working-with-search-parameters","title":"Working with Search Parameters","text":"<pre><code># Access search parameters used\nparams = results['search_parameters']\nif params['date_period'] == 'custom':\n    print(f\"Date range: {params['start_date']} to {params['end_date']}\")\n\n# Check filters applied\nif params['exclude_closed']:\n    print(\"Closed cases were excluded\")\n</code></pre>"},{"location":"api/data_structures/#error-handling","title":"Error Handling","text":"<pre><code>if results['status'] == 'error':\n    print(f\"Error: {results['error_message']}\")\n    if 'context' in results:\n        print(f\"Context: {results['context']}\")\nelse:\n    process_cases(results['cases'])\n</code></pre>"},{"location":"api/parser-1819/","title":"Parser1819 API Reference","text":"<p>Parser for 1819 News website.</p>"},{"location":"api/parser-1819/#class-definition","title":"Class Definition","text":"<pre><code>class Parser1819(BaseParser):\n    def __init__(self, url=\"https://1819news.com/\", suffix=\"/news/item\", max_pages=5)\n</code></pre>"},{"location":"api/parser-1819/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str <code>\"https://1819news.com/\"</code> Base URL for 1819 News <code>suffix</code> str <code>\"/news/item\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"api/parser-1819/#methods","title":"Methods","text":""},{"location":"api/parser-1819/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Extracts articles from multiple pages of 1819 News.</p> <p>Returns: List of article dictionaries with keys: - <code>title</code>: Article headline - <code>content</code>: Full article text - <code>date</code>: Publication date - <code>author</code>: Article author - <code>url</code>: Article URL - <code>tags</code>: List of article tags</p>"},{"location":"api/parser-1819/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extracts article URLs from a 1819 News page.</p> <p>Parameters: - <code>page_url</code>: URL of the page to scrape</p> <p>Returns: List of article URLs matching the suffix pattern</p>"},{"location":"api/parser-1819/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parses individual 1819 News article.</p> <p>Parameters: - <code>article_url</code>: URL of the article</p> <p>Returns: Dictionary with article data</p>"},{"location":"api/parser-1819/#usage-example","title":"Usage Example","text":"<pre><code>from opal.Parser1819 import Parser1819\n\n# Create parser instance\nparser = Parser1819(\n    url=\"https://1819news.com/\",\n    suffix=\"/news/item\",\n    max_pages=10\n)\n\n# Extract articles\narticles = parser.extract_article_data()\n\n# Save to JSON\nparser.save_to_json(articles, \"1819_news_articles.json\")\n\n# Access article data\nfor article in articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"Date: {article['date']}\")\n    print(f\"Author: {article['author']}\")\n</code></pre>"},{"location":"api/parser-1819/#output-format","title":"Output Format","text":"<pre><code>{\n  \"title\": \"Alabama Legislature Passes New Education Bill\",\n  \"content\": \"Full article text...\",\n  \"date\": \"2024-01-15\",\n  \"author\": \"John Smith\",\n  \"url\": \"https://1819news.com/news/item/education-bill-2024\",\n  \"tags\": [\"education\", \"legislature\", \"alabama\"]\n}\n</code></pre>"},{"location":"api/parser-1819/#notes","title":"Notes","text":"<ul> <li>Uses BeautifulSoup for HTML parsing</li> <li>Handles pagination automatically</li> <li>Filters links by suffix to ensure only articles are scraped</li> <li>Includes error handling for missing elements</li> </ul>"},{"location":"api/parser-appeals-al/","title":"ParserAppealsAL API Reference","text":"<p>Parser for Alabama Appeals Court Public Portal.</p>"},{"location":"api/parser-appeals-al/#class-definition","title":"Class Definition","text":"<pre><code>class ParserAppealsAL(BaseParser):\n    def __init__(self, url=\"https://publicportal.alappeals.gov/portal/search/case/results\")\n</code></pre>"},{"location":"api/parser-appeals-al/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str Portal URL Base URL for court portal <code>headless</code> bool True Run browser in headless mode <code>rate_limit_seconds</code> float 1.0 Delay between requests"},{"location":"api/parser-appeals-al/#methods","title":"Methods","text":""},{"location":"api/parser-appeals-al/#setup_driver","title":"setup_driver()","text":"<p><pre><code>def setup_driver(self) -&gt; webdriver.Chrome\n</code></pre> Sets up Chrome WebDriver with appropriate options.</p> <p>Returns: Configured Chrome WebDriver instance</p>"},{"location":"api/parser-appeals-al/#extract_court_data","title":"extract_court_data()","text":"<p><pre><code>def extract_court_data(self) -&gt; List[Dict]\n</code></pre> Main method to extract court case data.</p> <p>Returns: List of court case dictionaries</p>"},{"location":"api/parser-appeals-al/#parse_case_row","title":"parse_case_row()","text":"<p><pre><code>def parse_case_row(self, row: WebElement) -&gt; Dict\n</code></pre> Parses individual case row from search results.</p> <p>Parameters: - <code>row</code>: Selenium WebElement representing a case row</p> <p>Returns: Dictionary with case information</p>"},{"location":"api/parser-appeals-al/#get_case_details","title":"get_case_details()","text":"<p><pre><code>def get_case_details(self, case_url: str) -&gt; Dict\n</code></pre> Fetches detailed information for a specific case.</p> <p>Parameters: - <code>case_url</code>: URL to the case details page</p> <p>Returns: Dictionary with detailed case information</p>"},{"location":"api/parser-appeals-al/#usage-example","title":"Usage Example","text":"<pre><code>from opal.ParserAppealsAL import ParserAppealsAL\n\n# Create parser instance\nparser = ParserAppealsAL()\n\n# Extract court cases\ncases = parser.extract_court_data()\n\n# Save to JSON\nparser.save_to_json(cases, \"court_cases.json\")\n\n# Process cases\nfor case in cases:\n    print(f\"Case: {case['case_number']}\")\n    print(f\"Title: {case['case_title']}\")\n    print(f\"Status: {case['status']}\")\n</code></pre>"},{"location":"api/parser-appeals-al/#output-format","title":"Output Format","text":"<pre><code>{\n  \"case_number\": \"2024-CV-001234\",\n  \"case_title\": \"State of Alabama v. John Doe\",\n  \"court\": \"Alabama Court of Civil Appeals\",\n  \"date_filed\": \"2024-01-10\",\n  \"status\": \"Active\",\n  \"judge\": \"Hon. Jane Smith\",\n  \"parties\": {\n    \"appellant\": \"John Doe\",\n    \"appellee\": \"State of Alabama\"\n  },\n  \"attorneys\": [\n    {\n      \"name\": \"James Johnson\",\n      \"role\": \"Attorney for Appellant\"\n    }\n  ],\n  \"docket_entries\": [\n    {\n      \"date\": \"2024-01-10\",\n      \"description\": \"Notice of Appeal Filed\",\n      \"document_url\": \"https://publicportal.alappeals.gov/document/12345\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/parser-appeals-al/#special-features","title":"Special Features","text":""},{"location":"api/parser-appeals-al/#selenium-webdriver","title":"Selenium WebDriver","text":"<ul> <li>Automatically installs ChromeDriver</li> <li>Handles JavaScript-rendered content</li> <li>Supports dynamic page interactions</li> </ul>"},{"location":"api/parser-appeals-al/#error-handling","title":"Error Handling","text":"<ul> <li>Retries failed page loads</li> <li>Handles stale element exceptions</li> <li>Logs detailed error information</li> </ul>"},{"location":"api/parser-appeals-al/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Includes delays between requests</li> <li>Respects server load</li> </ul>"},{"location":"api/parser-appeals-al/#integration-with-other-components","title":"Integration with Other Components","text":""},{"location":"api/parser-appeals-al/#with-configurable-court-extractor","title":"With Configurable Court Extractor","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nfrom opal.court_case_parser import ParserAppealsAL\n\n# The configurable extractor uses ParserAppealsAL internally\nresults = extract_court_cases_with_params(\n    court=\"civil\",\n    date_period=\"1m\",\n    max_pages=5\n)\n\n# Direct parser usage\nparser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\nresult = parser.parse_article(url)\n</code></pre>"},{"location":"api/parser-appeals-al/#with-court-url-paginator","title":"With Court URL Paginator","text":"<pre><code>from opal.court_url_paginator import paginate_court_urls\nfrom opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL()\n\n# Get all page URLs (requires parser for dynamic page detection)\npage_urls = paginate_court_urls(search_url, parser)\n\n# Process each page\nall_cases = []\nfor url in page_urls:\n    result = parser.parse_article(url)\n    if 'cases' in result:\n        all_cases.extend(result['cases'])\n\nparser._close_driver()\n</code></pre>"},{"location":"api/parser-appeals-al/#with-integrated-parser","title":"With Integrated Parser","text":"<pre><code>from opal.integrated_parser import IntegratedParser\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Integrated parser requires parser class as parameter\nintegrated = IntegratedParser(ParserAppealsAL)\nresult = integrated.process_site(\"https://publicportal.alappeals.gov/...\")\n\n# Result contains processed data\nif result:\n    data = json.loads(result)\n</code></pre>"},{"location":"api/parser-appeals-al/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/parser-appeals-al/#custom-chrome-options","title":"Custom Chrome Options","text":"<pre><code>from selenium import webdriver\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Note: Chrome options are set in the _setup_driver method\n# To customize, you would need to modify the parser's _setup_driver method\nparser = ParserAppealsAL(headless=False)  # Run with visible browser\n</code></pre>"},{"location":"api/parser-appeals-al/#error-handling-integration","title":"Error Handling Integration","text":"<pre><code>from opal.court_case_parser import ParserAppealsAL\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\nparser = ParserAppealsAL(\n    headless=False,  # For debugging\n    rate_limit_seconds=2  # Slower for observation (int, not float)\n)\n\ntry:\n    result = parser.parse_article(url)\n    cases = result.get('cases', [])\nexcept Exception as e:\n    logging.error(f\"Extraction failed: {e}\")\n    # Handle error appropriately\nfinally:\n    parser._close_driver()\n</code></pre>"},{"location":"api/parser-appeals-al/#notes","title":"Notes","text":"<ul> <li>Requires Chrome browser installed</li> <li>Uses Selenium for JavaScript support</li> <li>Handles pagination automatically</li> <li>Extracts both case list and detailed case information</li> <li>Includes comprehensive error handling for web automation</li> <li>Integrates with all other OPAL components</li> <li>Supports extensive customization through configuration</li> </ul>"},{"location":"api/parser-daily-news/","title":"ParserDailyNews API Reference","text":"<p>Parser for Alabama Daily News website.</p>"},{"location":"api/parser-daily-news/#class-definition","title":"Class Definition","text":"<pre><code>class ParserDailyNews(BaseParser):\n    def __init__(self, url=\"https://www.aldailynews.com/\", suffix=\"/news/item\", max_pages=5)\n</code></pre>"},{"location":"api/parser-daily-news/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str <code>\"https://www.aldailynews.com/\"</code> Base URL for Alabama Daily News <code>suffix</code> str <code>\"/news/item\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"api/parser-daily-news/#methods","title":"Methods","text":""},{"location":"api/parser-daily-news/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Extracts articles from Alabama Daily News.</p> <p>Returns: List of article dictionaries with keys: - <code>title</code>: Article headline - <code>content</code>: Full article text - <code>date</code>: Publication date - <code>author</code>: Article author - <code>url</code>: Article URL - <code>category</code>: Article category</p>"},{"location":"api/parser-daily-news/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extracts article URLs from an Alabama Daily News page.</p> <p>Parameters: - <code>page_url</code>: URL of the page to scrape</p> <p>Returns: List of article URLs</p>"},{"location":"api/parser-daily-news/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parses individual Alabama Daily News article.</p> <p>Parameters: - <code>article_url</code>: URL of the article</p> <p>Returns: Dictionary with article data</p>"},{"location":"api/parser-daily-news/#usage-example","title":"Usage Example","text":"<pre><code>from opal.ParserDailyNews import ParserDailyNews\n\n# Create parser instance\nparser = ParserDailyNews(\n    url=\"https://www.aldailynews.com/\",\n    suffix=\"/news/item\",\n    max_pages=5\n)\n\n# Extract articles\narticles = parser.extract_article_data()\n\n# Save to JSON\nparser.save_to_json(articles, \"adn_articles.json\")\n\n# Process articles\nfor article in articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"Category: {article['category']}\")\n    print(f\"Date: {article['date']}\")\n</code></pre>"},{"location":"api/parser-daily-news/#output-format","title":"Output Format","text":"<pre><code>{\n  \"title\": \"Governor Announces Infrastructure Plan\",\n  \"content\": \"Full article text...\",\n  \"date\": \"2024-01-15\",\n  \"author\": \"Jane Doe\",\n  \"url\": \"https://www.aldailynews.com/news/item/infrastructure-plan\",\n  \"category\": \"Politics\"\n}\n</code></pre>"},{"location":"api/parser-daily-news/#notes","title":"Notes","text":"<ul> <li>Handles Alabama Daily News specific HTML structure</li> <li>Extracts category information when available</li> <li>Supports pagination through page parameters</li> <li>Includes robust error handling for missing elements</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/","title":"BaseParser: Web Scraping Fundamentals Guide","text":""},{"location":"developer/BaseParser_web_scraping_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Core Concepts</li> <li>HTTP Requests</li> <li>HTML Parsing</li> <li>Beautiful Soup</li> <li>BaseParser Architecture</li> <li>Implementation Examples</li> <li>Best Practices</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#introduction","title":"Introduction","text":"<p>The <code>BaseParser</code> class is the foundation of OPAL's web scraping system. It provides a standardized interface for extracting structured data from websites, whether they're news articles or court records. This guide explains the core concepts behind web scraping and how BaseParser implements them.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#core-concepts","title":"Core Concepts","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-is-web-scraping","title":"What is Web Scraping?","text":"<p>Web scraping is the process of extracting data from websites programmatically. It involves:</p> <ol> <li>Fetching - Downloading the HTML content from a web server</li> <li>Parsing - Analyzing the HTML structure to find specific data</li> <li>Extracting - Pulling out the desired information</li> <li>Structuring - Organizing the data into a useful format (like JSON)</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#the-web-scraping-pipeline","title":"The Web Scraping Pipeline","text":"<pre><code>URL \u2192 HTTP Request \u2192 HTML Response \u2192 Parse HTML \u2192 Extract Data \u2192 Structure Output\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#http-requests","title":"HTTP Requests","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-are-http-requests","title":"What are HTTP Requests?","text":"<p>HTTP (HyperText Transfer Protocol) requests are how programs communicate with web servers. When you visit a website, your browser sends an HTTP request to the server, which responds with the HTML content.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#key-components-of-http-requests","title":"Key Components of HTTP Requests","text":"<ol> <li>Method: Usually GET for retrieving data</li> <li>URL: The address of the resource</li> <li>Headers: Metadata about the request (User-Agent, Accept types, etc.)</li> <li>Response: The server's reply containing status code and content</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#example-from-baseparser","title":"Example from BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers\"\"\"\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, timeout=5)  # HTTP GET request\n            response.raise_for_status()  # Check for HTTP errors\n            responses.append(response.text)  # Store HTML content\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#common-http-status-codes","title":"Common HTTP Status Codes","text":"<ul> <li>200: Success - the page loaded correctly</li> <li>404: Not Found - the page doesn't exist</li> <li>403: Forbidden - access denied</li> <li>500: Server Error - problem on the website's end</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#html-parsing","title":"HTML Parsing","text":""},{"location":"developer/BaseParser_web_scraping_guide/#understanding-html-structure","title":"Understanding HTML Structure","text":"<p>HTML (HyperText Markup Language) is the standard markup language for web pages. It uses a tree-like structure of nested elements:</p> <pre><code>&lt;html&gt;\n  &lt;body&gt;\n    &lt;div class=\"article\"&gt;\n      &lt;h1&gt;Article Title&lt;/h1&gt;\n      &lt;p class=\"author\"&gt;By John Doe&lt;/p&gt;\n      &lt;div class=\"content\"&gt;\n        &lt;p&gt;First paragraph...&lt;/p&gt;\n        &lt;p&gt;Second paragraph...&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#the-document-object-model-dom","title":"The Document Object Model (DOM)","text":"<p>The DOM represents HTML as a tree structure where: - Each HTML tag is a node - Nodes can have attributes (class, id, href) - Nodes can contain text or other nodes - Nodes have parent-child relationships</p>"},{"location":"developer/BaseParser_web_scraping_guide/#parsing-strategy","title":"Parsing Strategy","text":"<ol> <li>Identify patterns: Find consistent HTML structures</li> <li>Use selectors: Target specific elements by tag, class, or ID</li> <li>Navigate relationships: Move between parent/child/sibling elements</li> <li>Extract data: Get text content or attribute values</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#beautiful-soup","title":"Beautiful Soup","text":""},{"location":"developer/BaseParser_web_scraping_guide/#what-is-beautiful-soup","title":"What is Beautiful Soup?","text":"<p>Beautiful Soup is a Python library designed for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data in a more Pythonic way.</p>"},{"location":"developer/BaseParser_web_scraping_guide/#key-features","title":"Key Features","text":"<ol> <li>Automatic encoding detection: Handles different character encodings</li> <li>Lenient parsing: Works with poorly formatted HTML</li> <li>Powerful searching: Find elements using various methods</li> <li>Tree navigation: Move through the document structure easily</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#beautiful-soup-methods","title":"Beautiful Soup Methods","text":""},{"location":"developer/BaseParser_web_scraping_guide/#finding-elements","title":"Finding Elements","text":"<pre><code># Find single elements\nsoup.find('tag')                    # First occurrence of tag\nsoup.find('tag', class_='classname') # First tag with specific class\nsoup.find('tag', {'attribute': 'value'}) # First tag with attribute\n\n# Find multiple elements\nsoup.find_all('tag')                # All occurrences of tag\nsoup.find_all(['tag1', 'tag2'])    # All occurrences of multiple tags\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#extracting-data","title":"Extracting Data","text":"<pre><code># Get text content\nelement.text          # All text including nested elements\nelement.get_text()    # Same as .text but with options\nelement.string        # Direct text only (no nested elements)\n\n# Get attributes\nelement.get('href')   # Get specific attribute\nelement['class']      # Get class attribute (returns list)\nelement.attrs         # Get all attributes as dictionary\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#real-examples-from-opal-parsers","title":"Real Examples from OPAL Parsers","text":""},{"location":"developer/BaseParser_web_scraping_guide/#parser1819-news-article-extraction","title":"Parser1819 - News Article Extraction","text":"<pre><code>def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Extract title from &lt;title&gt; tag\n    title_tag = soup.title\n    if title_tag:\n        article['title'] = title_tag.string.strip()\n\n    # Extract author from specific div structure\n    author_date_div = soup.find('div', class_='author-date')\n    if author_date_div:\n        author_link = author_date_div.find('a')\n        if author_link:\n            article['author'] = author_link.text.strip()\n\n    # Extract all paragraphs\n    paragraphs = soup.find_all(['p'])\n    for p in paragraphs:\n        text = p.get_text().strip()\n        # Process paragraph text...\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#parserappealsal-table-data-extraction","title":"ParserAppealsAL - Table Data Extraction","text":"<pre><code>def parse_table_row(self, row) -&gt; Optional[Dict]:\n    cells = row.find_all('td')  # Find all table cells\n    if len(cells) &lt; 6:\n        return None\n\n    # Extract text from specific cells\n    court = cells[0].get_text(strip=True)\n\n    # Extract both text and link from anchor tag\n    case_number_elem = cells[1].find('a')\n    if case_number_elem:\n        case_number = {\n            \"text\": case_number_elem.get_text(strip=True),\n            \"link\": case_number_elem.get('href', '')\n        }\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#baseparser-architecture","title":"BaseParser Architecture","text":""},{"location":"developer/BaseParser_web_scraping_guide/#abstract-base-class-design","title":"Abstract Base Class Design","text":"<p>BaseParser uses Python's ABC (Abstract Base Class) to define a contract that all parsers must follow:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseParser(ABC):\n    \"\"\"Base class defining the interface for all parsers\"\"\"\n\n    @abstractmethod\n    def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n        \"\"\"Each parser must implement this method\"\"\"\n        pass\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#why-use-abstract-base-classes","title":"Why Use Abstract Base Classes?","text":"<ol> <li>Consistency: Ensures all parsers have required methods</li> <li>Polymorphism: Different parsers can be used interchangeably</li> <li>Documentation: Clear contract for what parsers must implement</li> <li>Error Prevention: Catches missing implementations at instantiation</li> </ol>"},{"location":"developer/BaseParser_web_scraping_guide/#baseparser-methods","title":"BaseParser Methods","text":""},{"location":"developer/BaseParser_web_scraping_guide/#make_request","title":"make_request()","text":"<ul> <li>Purpose: Fetch HTML content from URLs</li> <li>Input: List of URLs</li> <li>Output: HTML content and successful URLs</li> <li>Error Handling: Continues on failure, reports errors</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#parse_article-abstract","title":"parse_article() (abstract)","text":"<ul> <li>Purpose: Extract data from single HTML page</li> <li>Input: HTML content and URL</li> <li>Output: Dictionary of extracted data</li> <li>Implementation: Must be defined by each parser subclass</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#parse_articles","title":"parse_articles()","text":"<ul> <li>Purpose: Coordinate parsing of multiple URLs</li> <li>Input: List of URLs</li> <li>Output: JSON string of all parsed data</li> <li>Process: Fetches HTML, calls parse_article(), combines results</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#implementation-examples","title":"Implementation Examples","text":""},{"location":"developer/BaseParser_web_scraping_guide/#creating-a-new-parser","title":"Creating a New Parser","text":"<pre><code>from opal.parser_module import BaseParser\nfrom bs4 import BeautifulSoup\n\nclass MyNewsParser(BaseParser):\n    \"\"\"Custom parser for a specific news site\"\"\"\n\n    def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Initialize data structure\n        article = {\n            'url': url,\n            'title': '',\n            'author': '',\n            'date': '',\n            'content': []\n        }\n\n        # Extract title\n        title_element = soup.find('h1', class_='article-title') #These classes are website specific\n        if title_element:\n            article['title'] = title_elem.get_text(strip=True)\n\n        # Extract author\n        author_element = soup.find('span', class_='byline')\n        if author_element:\n            article['author'] = author_elem.get_text(strip=True)\n\n        # Extract content paragraphs\n        content_div = soup.find('div', class_='article-body')\n        if content_div:\n            paragraphs = content_div.find_all('p')\n            article['content'] = [p.get_text(strip=True) for p in paragraphs]\n\n        return article\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#handling-complex-html-structures","title":"Handling Complex HTML Structures","text":"<p>Sometimes data is nested or spread across multiple elements:</p> <pre><code># Handle nested structures\narticle_div = soup.find('div', class_='article')\nif article_div:\n    # Navigate to nested elements\n    header = article_div.find('header')\n    if header:\n        title = header.find('h1')\n        meta = header.find('div', class_='meta')\n\n    # Find siblings\n    content = article_div.find_next_sibling('div', class_='content')\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<pre><code>def parse_article(self, html: str, url: str) -&gt; Dict[str, Any]:\n    try:\n        soup = BeautifulSoup(html, 'html.parser')\n        article = {'url': url}\n\n        # Always check if elements exist\n        title_elem = soup.find('h1')\n        if title_elem:\n            article['title'] = title_elem.get_text(strip=True)\n        else:\n            article['title'] = 'No title found'\n\n        # Handle missing attributes safely\n        link_elem = soup.find('a', class_='author-link')\n        if link_elem:\n            article['author_url'] = link_elem.get('href', '')\n\n        return article\n\n    except Exception as e:\n        # Return partial data rather than failing completely\n        return {\n            'url': url,\n            'error': str(e),\n            'partial_data': True\n        }\n</code></pre>"},{"location":"developer/BaseParser_web_scraping_guide/#best-practices","title":"Best Practices","text":""},{"location":"developer/BaseParser_web_scraping_guide/#1-respect-website-policies","title":"1. Respect Website Policies","text":"<ul> <li>Check robots.txt (example: https://1819news.com/robots.txt)</li> <li>Add delays between requests</li> <li>Use appropriate User-Agent headers</li> <li>Don't overwhelm servers</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<ul> <li>Expect missing elements</li> <li>Provide default values</li> <li>Log errors for debugging</li> <li>Continue processing other data</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#3-write-maintainable-code","title":"3. Write Maintainable Code","text":"<ul> <li>Use descriptive variable names</li> <li>Comment complex selections</li> <li>Create reusable helper functions</li> <li>Test with various page structures</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#4-optimize-performance","title":"4. Optimize Performance","text":"<ul> <li>Reuse parser instances</li> <li>Batch process URLs</li> <li>Cache results when appropriate</li> <li>Close resources properly</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#5-structure-data-consistently","title":"5. Structure Data Consistently","text":"<ul> <li>Use consistent field names</li> <li>Provide empty defaults</li> <li>Validate data types</li> <li>Document output format</li> </ul>"},{"location":"developer/BaseParser_web_scraping_guide/#common-challenges-and-solutions","title":"Common Challenges and Solutions","text":""},{"location":"developer/BaseParser_web_scraping_guide/#dynamic-content","title":"Dynamic Content","text":"<p>Problem: Content loaded by JavaScript isn't in initial HTML Solution: Use Selenium (like ParserAppealsAL) for JavaScript rendering</p>"},{"location":"developer/BaseParser_web_scraping_guide/#changing-html-structure","title":"Changing HTML Structure","text":"<p>Problem: Website updates break selectors Solution: Use multiple fallback selectors, test regularly</p>"},{"location":"developer/BaseParser_web_scraping_guide/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: Too many requests trigger blocking Solution: Add delays, rotate User-Agents, respect rate limits</p>"},{"location":"developer/BaseParser_web_scraping_guide/#encoding-issues","title":"Encoding Issues","text":"<p>Problem: Special characters appear corrupted Solution: Beautiful Soup handles most encoding automatically</p>"},{"location":"developer/BaseParser_web_scraping_guide/#conclusion","title":"Conclusion","text":"<p>The BaseParser provides a robust foundation for web scraping by: - Standardizing the parsing interface - Handling HTTP requests with error recovery - Leveraging Beautiful Soup for HTML parsing - Supporting both simple and complex extraction needs</p> <p>Whether scraping news articles or court records, understanding these fundamentals enables you to create effective parsers that extract structured data from any website.</p>"},{"location":"developer/ParserAppealsAL_documentation/","title":"ParserAppealsAL Documentation","text":""},{"location":"developer/ParserAppealsAL_documentation/#overview","title":"Overview","text":"<p><code>ParserAppealsAL</code> is a specialized parser designed to extract court case data from the Alabama Appeals Court Public Portal. Unlike traditional web scrapers that use simple HTTP requests, this parser employs Selenium WebDriver to handle JavaScript-rendered content, making it capable of extracting data from dynamic web applications.</p>"},{"location":"developer/ParserAppealsAL_documentation/#key-features","title":"Key Features","text":"<ul> <li>JavaScript Support: Uses Selenium WebDriver to render JavaScript-heavy pages</li> <li>Automatic Browser Management: Handles Chrome driver setup and teardown</li> <li>Rate Limiting: Built-in configurable delays between requests to avoid overwhelming the server</li> <li>Table Parsing: Specialized logic for extracting structured data from HTML tables</li> <li>Error Handling: Robust error handling with graceful fallbacks</li> <li>Headless Operation: Can run with or without a visible browser window</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#architecture","title":"Architecture","text":""},{"location":"developer/ParserAppealsAL_documentation/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser (Abstract Base Class)\n    \u2514\u2500\u2500 ParserAppealsAL\n</code></pre> <p>ParserAppealsAL inherits from the <code>BaseParser</code> base class, which defines the common interface for all parsers in the OPAL system. It overrides key methods to provide court-specific functionality.</p>"},{"location":"developer/ParserAppealsAL_documentation/#dependencies","title":"Dependencies","text":"<pre><code># Core Dependencies\nselenium &gt;= 4.0.0          # Browser automation\nwebdriver-manager &gt;= 4.0.0 # Automatic ChromeDriver management\nbeautifulsoup4            # HTML parsing\nrequests                  # HTTP requests (inherited from base)\n\n# Standard Library\njson                      # JSON data handling\ntime                      # Rate limiting\ndatetime                  # Timestamp generation\ntyping                    # Type hints\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#implementation-guide","title":"Implementation Guide","text":""},{"location":"developer/ParserAppealsAL_documentation/#1-basic-structure","title":"1. Basic Structure","text":"<p>To implement your own court parser based on ParserAppealsAL, start with this structure:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nfrom your_project.parser_module import BaseParser\n\nclass YourCourtParser(BaseParser):\n    def __init__(self, headless=True, rate_limit_seconds=3):\n        super().__init__()\n        self.headless = headless\n        self.rate_limit_seconds = rate_limit_seconds\n        self.driver = None\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#2-core-methods","title":"2. Core Methods","text":""},{"location":"developer/ParserAppealsAL_documentation/#__init__self-headless-bool-true-rate_limit_seconds-int-3","title":"<code>__init__(self, headless: bool = True, rate_limit_seconds: int = 3)</code>","text":"<p>Initializes the parser with configuration options.</p> <p>Parameters: - <code>headless</code>: Run Chrome in headless mode (no visible window) - <code>rate_limit_seconds</code>: Delay between requests to avoid rate limiting</p>"},{"location":"developer/ParserAppealsAL_documentation/#_setup_driverself","title":"<code>_setup_driver(self)</code>","text":"<p>Sets up the Chrome WebDriver with appropriate options:</p> <pre><code>def _setup_driver(self):\n    chrome_options = Options()\n    if self.headless:\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920,1080\")\n\n    service = Service(ChromeDriverManager().install())\n    self.driver = webdriver.Chrome(service=service, options=chrome_options)\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#make_requestself-url-str-timeout-int-30-optionalstr","title":"<code>make_request(self, url: str, timeout: int = 30) -&gt; Optional[str]</code>","text":"<p>Overrides the base class method to use Selenium instead of requests library.</p> <p>Key Features: - Lazy driver initialization - Waits for specific elements to load - Implements rate limiting - Returns page source HTML</p> <pre><code>def make_request(self, url, timeout=30):\n    if not self.driver:\n        self._setup_driver()\n\n    self.driver.get(url)\n\n    # Wait for your specific element\n    WebDriverWait(self.driver, timeout).until(\n        EC.presence_of_element_located((By.CSS_SELECTOR, \"table\"))\n    )\n\n    time.sleep(self.rate_limit_seconds)\n    return self.driver.page_source\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#parse_table_rowself-row-optionaldict","title":"<code>parse_table_row(self, row) -&gt; Optional[Dict]</code>","text":"<p>Extracts data from a single table row. This method is specific to the table structure of your court portal.</p> <p>Expected Table Structure: 1. Court Name 2. Case Number (with optional link) 3. Case Title 4. Classification 5. Filed Date 6. Status</p> <p>Returns: <pre><code>{\n    \"court\": \"Court of Civil Appeals\",\n    \"case_number\": {\n        \"text\": \"2230123\",\n        \"link\": \"/case/details/...\"\n    },\n    \"case_title\": \"Smith v. Jones\",\n    \"classification\": \"Civil\",\n    \"filed_date\": \"06/11/2024\",\n    \"status\": \"Active\"\n}\n</code></pre></p>"},{"location":"developer/ParserAppealsAL_documentation/#parse_articleself-url-str-dict","title":"<code>parse_article(self, url: str) -&gt; Dict</code>","text":"<p>Main parsing method that processes a single page of court results.</p> <p>Process: 1. Loads the page using <code>make_request</code> 2. Parses HTML with BeautifulSoup 3. Finds the main data table 4. Extracts data from each row 5. Returns structured results</p>"},{"location":"developer/ParserAppealsAL_documentation/#parse_all_casesself-base_url-str-page_urls-liststr-dict","title":"<code>parse_all_cases(self, base_url: str, page_urls: List[str]) -&gt; Dict</code>","text":"<p>Processes multiple pages of results and combines them.</p> <p>Returns: <pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        # List of case dictionaries\n    ]\n}\n</code></pre></p>"},{"location":"developer/ParserAppealsAL_documentation/#3-integration-with-opal-system","title":"3. Integration with OPAL System","text":"<p>The parser integrates with OPAL through the <code>IntegratedParser</code> class:</p> <pre><code>from opal.integrated_parser import IntegratedParser\nfrom your_parser import YourCourtParser\n\n# Create parser instance\nparser = IntegratedParser(YourCourtParser)\n\n# Process court data\nresult = parser.process_site(\n    base_url=\"https://your-court-portal.gov/search\",\n    suffix=\"\",  # Not used for court parsers\n    max_pages=None  # Will process all available pages\n)\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#4-url-pagination","title":"4. URL Pagination","text":"<p>Court portals often use complex URL parameters for pagination. The system includes helper functions in <code>court_url_paginator.py</code>:</p> <ul> <li><code>parse_court_url()</code>: Extracts page number and total pages from URL</li> <li><code>build_court_url()</code>: Constructs URLs for specific pages</li> <li><code>paginate_court_urls()</code>: Generates list of all page URLs</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#5-best-practices","title":"5. Best Practices","text":"<ol> <li>Error Handling: Always wrap operations in try-except blocks</li> <li>Resource Management: Ensure driver is closed in finally blocks</li> <li>Rate Limiting: Respect server limits to avoid IP bans</li> <li>Dynamic Waits: Use WebDriverWait instead of fixed sleep times when possible</li> <li>Memory Management: Close driver after processing to free resources</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#6-testing","title":"6. Testing","text":"<p>Create test scripts to validate your parser:</p> <pre><code>from your_parser import YourCourtParser\n\ndef test_single_page():\n    parser = YourCourtParser(headless=True)\n    result = parser.parse_article(\"https://court-url.gov/page1\")\n\n    assert result[\"cases\"]\n    assert len(result[\"cases\"]) &gt; 0\n\n    # Validate case structure\n    case = result[\"cases\"][0]\n    assert \"court\" in case\n    assert \"case_number\" in case\n    assert \"case_title\" in case\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#customization-guide","title":"Customization Guide","text":""},{"location":"developer/ParserAppealsAL_documentation/#adapting-for-different-court-systems","title":"Adapting for Different Court Systems","text":"<ol> <li>Table Structure: Modify <code>parse_table_row()</code> to match your court's table columns</li> <li>Wait Conditions: Update the element selector in <code>make_request()</code> </li> <li>URL Patterns: Adjust pagination logic in helper functions</li> <li>Data Fields: Add or remove fields based on available data</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#common-modifications","title":"Common Modifications","text":"<ol> <li> <p>Different Table Selectors: <pre><code># Instead of generic \"table\"\nWebDriverWait(self.driver, timeout).until(\n    EC.presence_of_element_located((By.ID, \"case-results-table\"))\n)\n</code></pre></p> </li> <li> <p>Additional Data Extraction: <pre><code># Add judge information if available\njudge = cells[6].get_text(strip=True) if len(cells) &gt; 6 else \"\"\n</code></pre></p> </li> <li> <p>Custom Headers: <pre><code># Some courts require authentication headers\nself.driver.add_cookie({\"name\": \"session\", \"value\": \"your-session-id\"})\n</code></pre></p> </li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/ParserAppealsAL_documentation/#common-issues","title":"Common Issues","text":"<ol> <li>ChromeDriver Not Found: </li> <li>Solution: webdriver-manager should handle this automatically</li> <li> <p>Manual fix: Download ChromeDriver matching your Chrome version</p> </li> <li> <p>Elements Not Loading:</p> </li> <li>Increase timeout in WebDriverWait</li> <li>Check if element selectors have changed</li> <li> <p>Verify JavaScript is executing properly</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Increase <code>rate_limit_seconds</code></li> <li>Implement exponential backoff</li> <li> <p>Consider using proxy rotation</p> </li> <li> <p>Memory Leaks:</p> </li> <li>Ensure driver is closed after use</li> <li>Implement periodic driver restarts for long runs</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Headless Mode: Significantly faster than visible browser</li> <li>Parallel Processing: Not recommended due to rate limits</li> <li>Caching: Consider caching parsed results to avoid re-parsing</li> <li>Resource Usage: Each driver instance uses ~100-200MB RAM</li> </ul>"},{"location":"developer/ParserAppealsAL_documentation/#example-output","title":"Example Output","text":"<pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        {\n            \"court\": \"Court of Civil Appeals\",\n            \"case_number\": {\n                \"text\": \"CL-2024-000123\",\n                \"link\": \"/portal/case/details/123\"\n            },\n            \"case_title\": \"Smith v. Jones Corporation\",\n            \"classification\": \"Civil Appeal\",\n            \"filed_date\": \"01/10/2025\",\n            \"status\": \"Pending\"\n        },\n        {\n            \"court\": \"Court of Criminal Appeals\",\n            \"case_number\": {\n                \"text\": \"CR-2024-000456\",\n                \"link\": \"/portal/case/details/456\"\n            },\n            \"case_title\": \"State of Alabama v. Doe\",\n            \"classification\": \"Criminal Appeal\",\n            \"filed_date\": \"01/09/2025\",\n            \"status\": \"Active\"\n        }\n    ]\n}\n</code></pre>"},{"location":"developer/ParserAppealsAL_documentation/#security-considerations","title":"Security Considerations","text":"<ol> <li>Input Validation: Always validate URLs before processing</li> <li>Sandbox Mode: Chrome runs with --no-sandbox for compatibility</li> <li>Credential Storage: Never hardcode credentials in parser</li> <li>SSL Verification: Selenium handles SSL by default</li> </ol>"},{"location":"developer/ParserAppealsAL_documentation/#future-enhancements","title":"Future Enhancements","text":"<p>Consider these improvements for production use:</p> <ol> <li>Retry Logic: Implement automatic retries for failed requests</li> <li>Progress Tracking: Add callbacks for progress updates</li> <li>Data Validation: Implement schema validation for parsed data</li> <li>Export Formats: Support multiple output formats (CSV, Excel)</li> <li>Incremental Updates: Track previously parsed cases to avoid duplicates</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/","title":"Alabama Appeals Court Public Portal Scraper - Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#overview","title":"Overview","text":"<p>Create a court case scraper extension for OPAL that extracts tabular data from the Alabama Appeals Court Public Portal. The scraper must handle JavaScript-rendered content, complex URL-based pagination, and preserve both text and link references.</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-by-step-implementation-instructions","title":"Step-by-Step Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-1-update-dependencies","title":"Step 1: Update Dependencies","text":"<p>Add the following to <code>requirements.txt</code> and <code>pyproject.toml</code>: - <code>selenium&gt;=4.0.0</code> or <code>playwright&gt;=1.40.0</code> (for JavaScript rendering) - <code>webdriver-manager&gt;=4.0.0</code> (if using Selenium for automatic driver management)</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-2-create-court-case-parser-module","title":"Step 2: Create Court Case Parser Module","text":"<p>Create a new file <code>opal/court_case_parser.py</code> with the following specifications:</p> <ol> <li>Import necessary libraries:</li> <li>Selenium/Playwright for JavaScript rendering</li> <li>BeautifulSoup for HTML parsing</li> <li> <p>Standard libraries for URL manipulation and JSON output</p> </li> <li> <p>Create <code>CourtCaseParser</code> class that extends <code>BaseParser</code>:</p> </li> <li>Override <code>make_request()</code> to use Selenium/Playwright instead of requests</li> <li>Implement JavaScript rendering with appropriate wait conditions</li> <li> <p>Add rate limiting (minimum 2-3 seconds between requests)</p> </li> <li> <p>Implement <code>parse_table_row()</code> method to extract:</p> </li> <li>Court name from <code>&lt;td class=\"text-start\"&gt;</code> (column 1)</li> <li>Case number text and href from <code>&lt;a href=\"/portal/court/...\"&gt;</code> (column 2)</li> <li>Case title from <code>&lt;td class=\"text-start\"&gt;</code> (column 3)</li> <li>Classification from <code>&lt;td class=\"text-start\"&gt;</code> (column 4)</li> <li>Filed date from <code>&lt;td class=\"text-start\"&gt;</code> (column 5)</li> <li>Open/Closed status from <code>&lt;td class=\"text-start\"&gt;</code> (column 6)</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-3-create-custom-url-pagination-handler","title":"Step 3: Create Custom URL Pagination Handler","text":"<p>Create <code>opal/court_url_paginator.py</code> with:</p> <ol> <li>URL parser function to:</li> <li>Extract and decode the complex URL parameters</li> <li>Identify current page number from <code>page~(number~X)</code></li> <li> <p>Extract total pages from <code>totalPages~X</code></p> </li> <li> <p>URL builder function to:</p> </li> <li>Take base URL and page number</li> <li>Update <code>page~(number~X)</code> parameter</li> <li>Maintain all other search parameters</li> <li> <p>Handle special encoding (<code>~</code>, <code>%2a2f</code>, etc.)</p> </li> <li> <p>Pagination iterator that:</p> </li> <li>Starts at page 0</li> <li>Continues until reaching <code>totalPages</code></li> <li>Yields properly formatted URLs for each page</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-4-implement-data-extraction-logic","title":"Step 4: Implement Data Extraction Logic","text":"<p>In <code>CourtCaseParser</code>, create <code>parse_all_cases()</code> method that:</p> <ol> <li>Initialize browser driver (Selenium/Playwright)</li> <li>Load first page and extract total pages from URL</li> <li>For each page:</li> <li>Navigate to page URL</li> <li>Wait for table to load (use explicit waits)</li> <li>Extract all table rows</li> <li>Parse each row using <code>parse_table_row()</code></li> <li>Store results with preserved link references</li> <li>Close browser driver when complete</li> <li>Return combined results from all pages as single dataset</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-5-define-output-format","title":"Step 5: Define Output Format","text":"<p>Structure the output JSON as: <pre><code>{\n  \"status\": \"success\",\n  \"total_cases\": 317,\n  \"extraction_date\": \"2025-06-11\",\n  \"cases\": [\n    {\n      \"court\": \"Alabama Supreme Court\",\n      \"case_number\": {\n        \"text\": \"SC-2025-0424\",\n        \"link\": \"/portal/court/68f021c4-6a44-4735-9a76-5360b2e8af13/case/d024d958-58a1-41c9-9fae-39c645c7977e\"\n      },\n      \"case_title\": \"Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.\",\n      \"classification\": \"Appeal - Civil - Injunction Other\",\n      \"filed_date\": \"06/10/2025\",\n      \"status\": \"Open\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-6-integrate-with-opal-cli","title":"Step 6: Integrate with OPAL CLI","text":"<p>Modify existing OPAL files:</p> <ol> <li>Update <code>opal/__init__.py</code>:</li> <li>Add <code>from .court_case_parser import CourtCaseParser</code></li> <li> <p>Add <code>from .court_url_paginator import paginate_court_urls</code></p> </li> <li> <p>Update <code>opal/integrated_parser.py</code>:</p> </li> <li>Add conditional logic to handle court case URLs differently</li> <li> <p>Use <code>paginate_court_urls</code> instead of <code>get_all_news_urls</code> for court sites</p> </li> <li> <p>Update <code>opal/main.py</code>:</p> </li> <li>Add <code>--parser court</code> option to argparse choices</li> <li>Add court parser to the parser selection logic</li> <li>Adjust output filename format for court data</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-7-handle-technical-requirements","title":"Step 7: Handle Technical Requirements","text":"<p>Implement the following in <code>CourtCaseParser</code>:</p> <ol> <li>JavaScript rendering:</li> <li>Wait for table element to be present</li> <li>Wait for data rows to load</li> <li> <p>Handle any loading spinners or dynamic content</p> </li> <li> <p>Error handling:</p> </li> <li>Timeout exceptions for slow page loads</li> <li>Missing table elements</li> <li>Network errors</li> <li> <p>Browser crashes</p> </li> <li> <p>Rate limiting:</p> </li> <li>Add configurable delay between page requests (default 3 seconds)</li> <li>Respect server response times</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-8-testing-urls","title":"Step 8: Testing URLs","text":"<p>Use these URLs for testing: - First page: <code>https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</code> - Second page: Same URL but with <code>page~(number~1)</code> and updated <code>totalElements~317~totalPages~13</code></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-9-final-integration","title":"Step 9: Final Integration","text":"<ol> <li>Test the complete flow with: <code>python -m opal --url [court_url] --parser court</code></li> <li>Ensure output file is created with court case data in tabular format</li> <li>Verify all pages are scraped and combined into single result set</li> <li>Confirm case number links are preserved in the output</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#expected-deliverables","title":"Expected Deliverables","text":"<ol> <li><code>opal/court_case_parser.py</code> - Main parser for court data</li> <li><code>opal/court_url_paginator.py</code> - URL pagination handler</li> <li>Updated <code>opal/__init__.py</code>, <code>opal/integrated_parser.py</code>, and <code>opal/main.py</code></li> <li>Updated <code>requirements.txt</code> and <code>pyproject.toml</code> with new dependencies</li> <li>JSON output file with all court cases in structured format</li> </ol>"},{"location":"developer/architecture/","title":"Architecture","text":"<p>OPAL follows a modular architecture that makes it easy to add new parsers and extend functionality.</p>"},{"location":"developer/architecture/#core-components","title":"Core Components","text":""},{"location":"developer/architecture/#baseparser","title":"BaseParser","text":"<p>The foundation of all parsers, providing: - Common web scraping functionality - Error handling and retry logic - Logging infrastructure - Output formatting</p>"},{"location":"developer/architecture/#parser-classes","title":"Parser Classes","text":"<p>Each website has its own parser class that inherits from BaseParser: - <code>Parser1819</code>: For 1819 News - <code>ParserDailyNews</code>: For Alabama Daily News - <code>ParserAppealsAL</code>: For Alabama Appeals Court</p>"},{"location":"developer/architecture/#main-module","title":"Main Module","text":"<p>The <code>__main__.py</code> module handles: - Command-line argument parsing - Parser instantiation - Execution flow - Output management</p>"},{"location":"developer/architecture/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser\n\u251c\u2500\u2500 Parser1819\n\u251c\u2500\u2500 ParserDailyNews\n\u2514\u2500\u2500 ParserAppealsAL\n</code></pre>"},{"location":"developer/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Input: User provides URL and parser type via CLI</li> <li>Initialization: Main module creates parser instance</li> <li>Scraping: Parser fetches and processes web pages</li> <li>Extraction: Parser extracts structured data</li> <li>Output: Data saved to JSON file</li> </ol>"},{"location":"developer/architecture/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"developer/architecture/#template-method-pattern","title":"Template Method Pattern","text":"<p>BaseParser defines the scraping workflow: <pre><code>class BaseParser:\n    def scrape(self):\n        self.setup()\n        data = self.extract_data()\n        self.save_output(data)\n</code></pre></p>"},{"location":"developer/architecture/#factory-pattern","title":"Factory Pattern","text":"<p>Parser selection based on command-line argument: <pre><code>parsers = {\n    'Parser1819': Parser1819,\n    'ParserDailyNews': ParserDailyNews,\n    'court': ParserAppealsAL\n}\nparser_class = parsers[args.parser]\n</code></pre></p>"},{"location":"developer/architecture/#extension-points","title":"Extension Points","text":""},{"location":"developer/architecture/#adding-new-parsers","title":"Adding New Parsers","text":"<ol> <li>Create new class inheriting from BaseParser</li> <li>Implement required methods:</li> <li><code>extract_article_data()</code></li> <li><code>get_article_links()</code></li> <li><code>parse_article()</code></li> <li>Register in main module</li> </ol>"},{"location":"developer/architecture/#customizing-output","title":"Customizing Output","text":"<p>Override <code>format_output()</code> method to customize data structure.</p>"},{"location":"developer/architecture/#adding-features","title":"Adding Features","text":"<ul> <li>Authentication: Add login methods</li> <li>Caching: Implement request caching</li> <li>Rate limiting: Add delay mechanisms</li> </ul>"},{"location":"developer/configurable_court_extractor_design/","title":"Configurable Court Extractor Design","text":""},{"location":"developer/configurable_court_extractor_design/#problem-statement","title":"Problem Statement","text":"<p>A former version of <code>extract_all_court_cases.py</code> had hardcoded search parameters in the URL, making it inflexible for different search criteria. Users couldn't dynamically change: - Date ranges - Case number filters - Case title filters - Whether to include/exclude closed cases</p>"},{"location":"developer/configurable_court_extractor_design/#solution-overview","title":"Solution Overview","text":"<p>I designed a configurable court extractor that separates URL construction from data extraction, allowing users to specify search parameters via command line arguments or function parameters.</p>"},{"location":"developer/configurable_court_extractor_design/#architecture","title":"Architecture","text":""},{"location":"developer/configurable_court_extractor_design/#1-courtsearchbuilder-class","title":"1. CourtSearchBuilder Class","text":"<p>Purpose: Encapsulates the complex URL building logic for Alabama Appeals Court searches.</p>"},{"location":"developer/configurable_court_extractor_design/#why-i-designed-it-this-way","title":"Why I designed it this way:","text":"<ul> <li> <p>Separation of Concerns: URL building is separate from data extraction</p> </li> <li> <p>Maintainability: Changes to URL structure only affect one class</p> </li> <li> <p>Reusability: Can be used by different scripts or tools</p> </li> <li> <p>Readability: Clear methods for each search parameter</p> </li> </ul> <pre><code>class CourtSearchBuilder:\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n        self.court_id = \"68f021c4-6a44-4735-9a76-5360b2e8af13\"\n        self.reset_params()\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#2-key-methods-explained","title":"2. Key Methods Explained","text":""},{"location":"developer/configurable_court_extractor_design/#set_date_range","title":"<code>set_date_range()</code>","text":"<p>Purpose: Handle different date range options Design rationale: - Supports both predefined periods (<code>-1y</code>, <code>-6m</code>) and custom date ranges - Automatically converts dates to the portal's expected format (<code>*2f</code> encoding) - Provides sensible defaults</p>"},{"location":"developer/configurable_court_extractor_design/#build_criteria_string","title":"<code>build_criteria_string()</code>","text":"<p>Purpose: Construct the complex URL-encoded criteria parameter Design rationale: - Handles the intricate URL encoding required by the portal - Builds the nested parameter structure programmatically - Reduces human error in URL construction</p>"},{"location":"developer/configurable_court_extractor_design/#build_url","title":"<code>build_url()</code>","text":"<p>Purpose: Create complete search URLs with pagination Design rationale: - Updates page numbers dynamically - Maintains other search parameters across pages - Returns ready-to-use URLs</p>"},{"location":"developer/configurable_court_extractor_design/#configuration-options","title":"Configuration Options","text":""},{"location":"developer/configurable_court_extractor_design/#court-selection","title":"Court Selection","text":"<pre><code># Available courts\ncourts = {\n    'civil': 'Alabama Civil Court of Appeals',\n    'criminal': 'Alabama Court of Criminal Appeals', \n    'supreme': 'Alabama Supreme Court'\n}\n\n# Select court\nsearch_builder.set_court('civil')  # or 'criminal', 'supreme'\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-number-formats","title":"Case Number Formats","text":"<pre><code># Open-ended search\nsearch_builder.set_case_number_filter('2024-001')\n\n# Court-specific formats\nsearch_builder.set_case_number_filter('CL-2024-0001')  # Civil Appeals\nsearch_builder.set_case_number_filter('CR-2024-0001')  # Criminal Appeals  \nsearch_builder.set_case_number_filter('SC-2024-0001')  # Supreme Court\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-categories","title":"Case Categories","text":"<pre><code># For Civil Appeals and Criminal Appeals\ncategories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n\n# For Supreme Court (includes additional option)\nsupreme_categories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n\n# Set category\nsearch_builder.set_case_category('Appeal')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#date-filters","title":"Date Filters","text":"<pre><code># Predefined periods (matching portal options)\nsearch_builder.set_date_range(period='7d')   # Last 7 days\nsearch_builder.set_date_range(period='1m')   # Last month\nsearch_builder.set_date_range(period='3m')   # Last 3 months\nsearch_builder.set_date_range(period='6m')   # Last 6 months\nsearch_builder.set_date_range(period='1y')   # Last year\n\n# Custom date range\nsearch_builder.set_date_range('2024-01-01', '2024-12-31', 'custom')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-title-and-status-filters","title":"Case Title and Status Filters","text":"<pre><code># Filter by case title (partial match)\nsearch_builder.set_case_title_filter('Smith v Jones')\n\n# Exclude closed cases\nsearch_builder.set_exclude_closed(True)\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#command-line-interface","title":"Command Line Interface","text":"<p>Why I included CLI arguments: - User-friendly: No need to modify code for different searches - Scriptable: Can be integrated into automated workflows - Documented: Built-in help shows all options</p>"},{"location":"developer/configurable_court_extractor_design/#usage-examples","title":"Usage Examples","text":""},{"location":"developer/configurable_court_extractor_design/#option-1-use-built-in-search-parameters-recommended","title":"Option 1: Use Built-in Search Parameters (Recommended)","text":"<pre><code># Extract all cases from last year (default from all courts)\npython configurable_court_extractor.py\n\n# Extract cases from Alabama Supreme Court only\npython configurable_court_extractor.py --court supreme\n\n# Extract cases from last 7 days from Criminal Appeals\npython configurable_court_extractor.py --court criminal --date-period 7d\n\n# Extract Appeal cases from Civil Court\npython configurable_court_extractor.py --court civil --case-category Appeal\n\n# Extract cases with custom date range from Supreme Court\npython configurable_court_extractor.py --court supreme --date-period custom --start-date 2024-01-01 --end-date 2024-06-30\n\n# Filter by specific case number format\npython configurable_court_extractor.py --court civil --case-number \"CL-2024-\"\n\n# Filter by case title in Criminal Appeals\npython configurable_court_extractor.py --court criminal --case-title \"State v\"\n\n# Exclude closed cases from Supreme Court\npython configurable_court_extractor.py --court supreme --exclude-closed\n\n# Extract Certified Questions from Supreme Court (unique to Supreme Court)\npython configurable_court_extractor.py --court supreme --case-category \"Certified Question\"\n\n# Comprehensive search with multiple filters\npython configurable_court_extractor.py --court civil --case-category Appeal --date-period 3m --exclude-closed --output-prefix \"civil_appeals_q1\"\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#option-2-use-pre-built-url-with-embedded-search-terms","title":"Option 2: Use Pre-built URL with Embedded Search Terms","text":"<p>\u26a0\ufe0f  WARNING: Custom URLs are temporary and session-based. They may stop working when the website session expires.</p> <pre><code># Use your existing URL with search terms already embedded\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29\"\n\n# Use custom URL with limited pages and custom output prefix\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\" --max-pages 5 --output-prefix \"my_custom_search\"\n\n# Any URL from the portal search interface works\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=YOUR_CUSTOM_SEARCH_CRITERIA\"\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#hybrid-approach","title":"Hybrid Approach","text":"<pre><code># You can also programmatically call the function with a custom URL\nfrom configurable_court_extractor import extract_court_cases_with_params\n\n# Use your existing URL\nyour_url = \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\nresult = extract_court_cases_with_params(custom_url=your_url, max_pages=10)\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#dynamic-court-id-discovery","title":"Dynamic Court ID Discovery","text":""},{"location":"developer/configurable_court_extractor_design/#the-problem-with-dynamic-ids","title":"The Problem with Dynamic IDs","text":"<p>Modern web applications often generate session-specific or dynamic identifiers that change between visits. The Alabama Appeals Court portal appears to use dynamic court IDs that are assigned during the user's session rather than being static, predictable values.</p>"},{"location":"developer/configurable_court_extractor_design/#solution","title":"Solution","text":"<p>Chosen Solution: Automatic Discovery The <code>discover_court_ids()</code> method navigates to the court's search interface and programmatically extracts the current court IDs by:</p> <ol> <li>Loading the search page - Navigates to the main case search interface</li> <li>Inspecting form elements - Locates the court selection dropdown or form elements</li> <li>Extracting ID mappings - Parses the HTML to find court names and their corresponding dynamic IDs</li> <li>Caching for session - Stores the discovered IDs for the duration of the session</li> </ol> <p>Option 2: Manual Discovery If automatic discovery fails, users can:</p> <ol> <li>Inspect browser network traffic - Use browser developer tools to monitor the search requests</li> <li>Extract court ID from URL - Copy a working search URL and extract the court ID parameter</li> <li>Set manually - Use <code>set_court_id_manually()</code> to override the discovered ID</li> </ol> <p>Option 3: URL Bypass (Fallback) When court ID discovery completely fails, users can:</p> <ol> <li>Use browser to build URL - Manually configure search on the website</li> <li>Copy complete URL - Get the full URL with embedded parameters</li> <li>Use --url option - Pass the pre-built URL directly, bypassing all parameter building</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#implementation-benefits","title":"Implementation Benefits","text":"<ol> <li>Resilient to changes - Automatically adapts to new court ID schemes</li> <li>Fallback options - Multiple strategies when automatic discovery fails</li> <li>User-friendly - Handles complexity behind the scenes</li> <li>Transparent - Shows discovered IDs to user for verification</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#usage-examples-with-dynamic-ids","title":"Usage Examples with Dynamic IDs","text":"<pre><code># Let the system discover court IDs automatically\npython configurable_court_extractor.py --court civil --date-period 1m\n\n# If discovery fails, fall back to custom URL\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n\n# For debugging: manually set a court ID\nsearch_builder = CourtSearchBuilder()\nsearch_builder.set_court_id_manually('civil', 'discovered-session-id-12345')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"developer/configurable_court_extractor_design/#url-encoding-strategy","title":"URL Encoding Strategy","text":"<p>The Alabama Appeals Court portal uses a complex nested URL structure: <pre><code>?criteria=~%28advanced~false~courtID~%27{court_id}~page~%28...%29~sort~%28...%29~case~%28...%29%29\n</code></pre></p> <p>My approach:</p> <ol> <li>Build parameters as nested dictionaries</li> <li>Convert to the portal's specific encoding format</li> <li>Handle special characters and escaping automatically</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#error-handling","title":"Error Handling","text":"<p>Graceful degradation: - If total page count can't be determined, process incrementally - Continue processing if individual pages fail - Provide detailed error messages with stack traces</p>"},{"location":"developer/configurable_court_extractor_design/#performance-considerations","title":"Performance Considerations","text":"<p>Rate limiting:  - Configurable delays between requests - Respectful of server resources</p> <p>Memory efficiency: - Process pages incrementally - Don't load all data into memory at once</p> <p>Progress reporting: - Real-time feedback on processing status - Clear indication of completion</p>"},{"location":"developer/configurable_court_extractor_design/#advantages-over-former-implementation","title":"Advantages Over Former Implementation","text":""},{"location":"developer/configurable_court_extractor_design/#1-flexibility","title":"1. Flexibility","text":"<ul> <li>Before: Fixed search parameters in hardcoded URL</li> <li>After: Configurable search criteria via parameters OR custom URLs</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#2-maintainability","title":"2. Maintainability","text":"<ul> <li>Before: URL changes require code modification</li> <li>After: URL structure centralized in builder class with dynamic discovery</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#3-usability","title":"3. Usability","text":"<ul> <li>Before: Developers need to understand complex URL structure</li> <li>After: Simple method calls and CLI arguments</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#4-reusability","title":"4. Reusability","text":"<ul> <li>Before: Single-purpose script</li> <li>After: Reusable components for different use cases</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#5-documentation","title":"5. Documentation","text":"<ul> <li>Before: Search parameters hidden in URL</li> <li>After: Clear parameter documentation and examples</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#6-resilience-to-changes","title":"6. Resilience to Changes","text":"<ul> <li>Before: Hardcoded court IDs break when website changes</li> <li>After: Automatic discovery adapts to dynamic court ID schemes</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#7-multiple-fallback-options","title":"7. Multiple Fallback Options","text":"<ul> <li>Before: Script fails completely if URL structure changes</li> <li>After: Automatic discovery \u2192 manual discovery \u2192 custom URL bypass</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#integration-with-existing-code","title":"Integration with Existing Code","text":"<p>The new extractor coexists with the current implementation: - Uses the same <code>ParserAppealsAL</code> class - Produces the same JSON/CSV output format - Follows the same error handling patterns</p>"},{"location":"developer/configurable_court_extractor_design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"developer/configurable_court_extractor_design/#advanced-features","title":"Advanced Features","text":"<ul> <li>Save/load search configurations</li> <li>Scheduled extractions</li> <li>Differential updates (only new cases)</li> <li>Export to additional formats (Excel, XML)</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Parallel page processing</li> <li>Caching of search results</li> <li>Resume interrupted extractions</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#code-structure","title":"Code Structure","text":"<pre><code>configurable_court_extractor.py\n\u251c\u2500\u2500 CourtSearchBuilder class\n\u2502   \u251c\u2500\u2500 Parameter management methods\n\u2502   \u251c\u2500\u2500 URL building methods\n\u2502   \u2514\u2500\u2500 Validation methods\n\u251c\u2500\u2500 extract_court_cases_with_params() function\n\u2502   \u251c\u2500\u2500 Search execution logic\n\u2502   \u251c\u2500\u2500 Progress reporting\n\u2502   \u2514\u2500\u2500 Output generation\n\u2514\u2500\u2500 main() function\n    \u251c\u2500\u2500 CLI argument parsing\n    \u251c\u2500\u2500 Parameter validation\n    \u2514\u2500\u2500 Function orchestration\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#why-this-design-is-better","title":"Why This Design is Better","text":"<ol> <li>Single Responsibility: Each class/function has one clear purpose</li> <li>Open/Closed Principle: Easy to extend without modifying existing code</li> <li>DRY (Don't Repeat Yourself): URL logic is centralized</li> <li>User-Centered: Designed around user needs, not technical constraints</li> <li>Testable: Components can be unit tested independently</li> <li>Documented: Self-documenting code with clear method names</li> </ol> <p>This design transforms a rigid, single-purpose script into a flexible, user-friendly tool that can adapt to various research needs while maintaining the reliability and performance of the original implementation.</p>"},{"location":"developer/configurable_court_extractor_design/#complete-implementation-code","title":"Complete Implementation Code","text":"<p>Below is the full implementation of the configurable court extractor:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nConfigurable Court Case Extractor for Alabama Appeals Court\nAllows users to set custom search parameters OR use pre-built URLs\n\"\"\"\nimport json\nimport argparse\nfrom datetime import datetime, timedelta\nfrom urllib.parse import quote\nfrom opal.court_case_parser import ParserAppealsAL\nfrom opal.court_url_paginator import parse_court_url\n\n\nclass CourtSearchBuilder:\n    \"\"\"Builder class for constructing Alabama Court search URLs with court-specific parameters\"\"\"\n\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n\n        # Court definitions with their specific IDs and configurations\n        # NOTE: Court IDs may be dynamically assigned by the website\n        # These IDs should be discovered through session initialization\n        self.courts = {\n            'civil': {\n                'name': 'Alabama Civil Court of Appeals',\n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'CL',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n            },\n            'criminal': {\n                'name': 'Alabama Court of Criminal Appeals', \n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'CR',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n            },\n            'supreme': {\n                'name': 'Alabama Supreme Court',\n                'id': None,  # Will be discovered dynamically\n                'case_prefix': 'SC',\n                'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n            }\n        }\n\n        # Date period mappings\n        self.date_periods = {\n            '7d': '-7d',\n            '1m': '-1m', \n            '3m': '-3m',\n            '6m': '-6m',\n            '1y': '-1y',\n            'custom': 'custom'\n        }\n\n        self.current_court = 'civil'  # Default court\n        self.session_initialized = False\n        self.reset_params()\n\n    def reset_params(self):\n        \"\"\"Reset all parameters to defaults\"\"\"\n        court_info = self.courts[self.current_court]\n        self.params = {\n            'advanced': 'false',\n            'courtID': court_info['id'],  # May be None until discovered\n            'page': {\n                'size': 500,\n                'number': 0,\n                'totalElements': 0,\n                'totalPages': 0\n            },\n            'sort': {\n                'sortBy': 'caseHeader.filedDate',\n                'sortDesc': 'true'\n            },\n            'case': {\n                'caseCategoryID': 1000000,  # All categories\n                'caseNumberQueryTypeID': 10463,  # Contains\n                'caseTitleQueryTypeID': 300054,  # Contains\n                'filedDateChoice': '1y',  # Last year\n                'filedDateStart': '',\n                'filedDateEnd': '',\n                'excludeClosed': 'false'\n            }\n        }\n\n    def discover_court_ids(self, parser_instance):\n        \"\"\"\n        Discover court IDs by navigating to the website and inspecting the court selection interface\n\n        Args:\n            parser_instance: Instance of ParserAppealsAL with active WebDriver\n        \"\"\"\n        try:\n            # Navigate to the main search page\n            search_page_url = \"https://publicportal.alappeals.gov/portal/search/case\"\n            parser_instance.driver.get(search_page_url)\n\n            # Wait for the page to load\n            from selenium.webdriver.support.ui import WebDriverWait\n            from selenium.webdriver.support import expected_conditions as EC\n            from selenium.webdriver.common.by import By\n\n            wait = WebDriverWait(parser_instance.driver, 10)\n\n            # Look for court selection dropdown or options\n            # This is a placeholder - actual implementation would need to inspect the HTML structure\n            court_selector = wait.until(EC.presence_of_element_located((By.ID, \"court-selector\")))\n\n            # Extract court options and their IDs\n            # Implementation would parse the HTML to find court names and their corresponding IDs\n            court_options = court_selector.find_elements(By.TAG_NAME, \"option\")\n\n            for option in court_options:\n                court_name = option.text.lower()\n                court_id = option.get_attribute(\"value\")\n\n                # Map court names to our court keys\n                if \"civil\" in court_name and \"appeals\" in court_name:\n                    self.courts['civil']['id'] = court_id\n                elif \"criminal\" in court_name and \"appeals\" in court_name:\n                    self.courts['criminal']['id'] = court_id\n                elif \"supreme\" in court_name:\n                    self.courts['supreme']['id'] = court_id\n\n            self.session_initialized = True\n            print(\"Successfully discovered court IDs:\")\n            for court_key, court_info in self.courts.items():\n                print(f\"  {court_info['name']}: {court_info['id']}\")\n\n        except Exception as e:\n            print(f\"Warning: Could not discover court IDs automatically: {e}\")\n            print(\"Try running your search on the website and searching by URL populated by the search\")\n\n    def set_court_id_manually(self, court_key, court_id):\n        \"\"\"\n        Set court ID manually\n\n        Args:\n            court_key: 'civil', 'criminal', or 'supreme'\n            court_id: The discovered court ID string\n        \"\"\"\n        if court_key in self.courts:\n            self.courts[court_key]['id'] = court_id\n            print(f\"Manually set {court_key} court ID to: {court_id}\")\n        else:\n            raise ValueError(f\"Invalid court key: {court_key}\")\n\n    def set_court(self, court_key):\n        \"\"\"\n        Set the court to search\n\n        Args:\n            court_key: 'civil', 'criminal', or 'supreme'\n        \"\"\"\n        if court_key not in self.courts:\n            raise ValueError(f\"Invalid court: {court_key}. Must be one of {list(self.courts.keys())}\")\n\n        self.current_court = court_key\n        self.reset_params()  # Reset params with new court ID\n\n    def get_court_info(self):\n        \"\"\"Get information about the current court\"\"\"\n        return self.courts[self.current_court]\n\n    def validate_case_category(self, category):\n        \"\"\"Validate that the category is available for the current court\"\"\"\n        court_info = self.courts[self.current_court]\n        if category not in court_info['categories']:\n            raise ValueError(f\"Category '{category}' not available for {court_info['name']}. \"\n                           f\"Available: {court_info['categories']}\")\n        return True\n\n    def format_case_number_suggestion(self, year=None):\n        \"\"\"Suggest proper case number format for current court\"\"\"\n        court_info = self.courts[self.current_court]\n        current_year = year or datetime.now().year\n        return f\"{court_info['case_prefix']}-{current_year}-####\"\n\n    def set_date_range(self, start_date=None, end_date=None, period='1y'):\n        \"\"\"\n        Set the date range for case searches\n\n        Args:\n            start_date: Start date (YYYY-MM-DD) or None\n            end_date: End date (YYYY-MM-DD) or None  \n            period: Predefined period ('7d', '1m', '3m', '6m', '1y', 'custom')\n        \"\"\"\n        if period == 'custom' and start_date and end_date:\n            # Convert dates to the format expected by the portal\n            self.params['case']['filedDateChoice'] = 'custom'\n            self.params['case']['filedDateStart'] = start_date.replace('-', '*2f')\n            self.params['case']['filedDateEnd'] = end_date.replace('-', '*2f')\n        else:\n            # Use predefined period - validate it exists\n            if period not in self.date_periods:\n                raise ValueError(f\"Invalid date period: {period}. Must be one of {list(self.date_periods.keys())}\")\n\n            self.params['case']['filedDateChoice'] = self.date_periods[period]\n\n            # Calculate dates for display purposes\n            today = datetime.now()\n            if period == '7d':\n                start = today - timedelta(days=7)\n            elif period == '1m':\n                start = today - timedelta(days=30)\n            elif period == '3m':\n                start = today - timedelta(days=90)\n            elif period == '6m':\n                start = today - timedelta(days=180)\n            elif period == '1y':\n                start = today - timedelta(days=365)\n            else:\n                start = today - timedelta(days=365)\n\n            self.params['case']['filedDateStart'] = start.strftime('%m*2f%d*2f%Y')\n            self.params['case']['filedDateEnd'] = today.strftime('%m*2f%d*2f%Y')\n\n    def set_case_category(self, category_name=None):\n        \"\"\"\n        Set case category filter\n\n        Args:\n            category_name: Category name ('Appeal', 'Certiorari', 'Original Proceeding', \n                          'Petition', 'Certified Question') or None for all\n        \"\"\"\n        if category_name is None:\n            self.params['case']['caseCategoryID'] = 1000000  # All categories\n            return\n\n        # Validate category is available for current court\n        self.validate_case_category(category_name)\n\n        # Map category names to IDs (these would need to be discovered from the portal)\n        category_map = {\n            'Appeal': 1000001,\n            'Certiorari': 1000002, \n            'Original Proceeding': 1000003,\n            'Petition': 1000004,\n            'Certified Question': 1000005  # Supreme Court only\n        }\n\n        if category_name in category_map:\n            self.params['case']['caseCategoryID'] = category_map[category_name]\n        else:\n            raise ValueError(f\"Unknown category: {category_name}\")\n\n    def set_case_number_filter(self, case_number=None, query_type=10463):\n        \"\"\"\n        Set case number filter\n\n        Args:\n            case_number: Case number to search for\n            query_type: Query type (10463=contains, check portal for others)\n        \"\"\"\n        self.params['case']['caseNumberQueryTypeID'] = query_type\n        if case_number:\n            self.params['case']['caseNumber'] = case_number\n\n    def set_case_title_filter(self, title=None, query_type=300054):\n        \"\"\"\n        Set case title filter\n\n        Args:\n            title: Title text to search for\n            query_type: Query type (300054=contains, check portal for others)\n        \"\"\"\n        self.params['case']['caseTitleQueryTypeID'] = query_type\n        if title:\n            self.params['case']['caseTitle'] = title\n\n    def set_exclude_closed(self, exclude=False):\n        \"\"\"\n        Set whether to exclude closed cases\n\n        Args:\n            exclude: True to exclude closed cases, False to include all\n        \"\"\"\n        self.params['case']['excludeClosed'] = 'true' if exclude else 'false'\n\n    def set_sort_order(self, sort_by='caseHeader.filedDate', descending=True):\n        \"\"\"\n        Set sort order for results\n\n        Args:\n            sort_by: Field to sort by\n            descending: True for descending, False for ascending\n        \"\"\"\n        self.params['sort']['sortBy'] = sort_by\n        self.params['sort']['sortDesc'] = 'true' if descending else 'false'\n\n    def set_page_info(self, page_number=0, page_size=25, total_elements=0, total_pages=0):\n        \"\"\"Set pagination information\"\"\"\n        self.params['page'].update({\n            'number': page_number,\n            'size': page_size,\n            'totalElements': total_elements,\n            'totalPages': total_pages\n        })\n\n    def build_criteria_string(self):\n        \"\"\"Build the criteria string for the URL\"\"\"\n        criteria_parts = []\n\n        # Basic parameters\n        criteria_parts.append(f\"advanced~{self.params['advanced']}\")\n        criteria_parts.append(f\"courtID~%27{self.params['courtID']}\")\n\n        # Page parameters\n        page = self.params['page']\n        page_str = f\"page~%28size~{page['size']}~number~{page['number']}~totalElements~{page['totalElements']}~totalPages~{page['totalPages']}%29\"\n        criteria_parts.append(page_str)\n\n        # Sort parameters\n        sort = self.params['sort']\n        sort_str = f\"sort~%28sortBy~%27{sort['sortBy']}~sortDesc~{sort['sortDesc']}%29\"\n        criteria_parts.append(sort_str)\n\n        # Case parameters\n        case = self.params['case']\n        case_parts = []\n        case_parts.append(f\"caseCategoryID~{case['caseCategoryID']}\")\n        case_parts.append(f\"caseNumberQueryTypeID~{case['caseNumberQueryTypeID']}\")\n        case_parts.append(f\"caseTitleQueryTypeID~{case['caseTitleQueryTypeID']}\")\n        case_parts.append(f\"filedDateChoice~%27{case['filedDateChoice']}\")\n        case_parts.append(f\"filedDateStart~%27{case['filedDateStart']}\")\n        case_parts.append(f\"filedDateEnd~%27{case['filedDateEnd']}\")\n        case_parts.append(f\"excludeClosed~{case['excludeClosed']}\")\n\n        # Add optional case filters\n        if 'caseNumber' in case:\n            case_parts.append(f\"caseNumber~{quote(case['caseNumber'])}\")\n        if 'caseTitle' in case:\n            case_parts.append(f\"caseTitle~{quote(case['caseTitle'])}\")\n\n        case_str = f\"case~%28{'~'.join(case_parts)}%29\"\n        criteria_parts.append(case_str)\n\n        return f\"~%28{'~'.join(criteria_parts)}%29\"\n\n    def build_url(self, page_number=0):\n        \"\"\"Build complete search URL\"\"\"\n        # Update page number\n        self.set_page_info(page_number=page_number, \n                          page_size=self.params['page']['size'],\n                          total_elements=self.params['page']['totalElements'],\n                          total_pages=self.params['page']['totalPages'])\n\n        criteria = self.build_criteria_string()\n        return f\"{self.base_url}?criteria={criteria}\"\n\n\ndef extract_court_cases_with_params(\n    court='civil',\n    date_period='1y',\n    start_date=None,\n    end_date=None,\n    case_number=None,\n    case_title=None,\n    case_category=None,\n    exclude_closed=False,\n    max_pages=None,\n    output_prefix=\"court_cases\",\n    custom_url=None\n):\n    \"\"\"\n    Extract court cases with configurable search parameters OR a pre-built URL\n\n    Args:\n        court: Court to search ('civil', 'criminal', 'supreme') - ignored if custom_url provided\n        date_period: Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - ignored if custom_url provided\n        start_date: Start date for custom range (YYYY-MM-DD) - ignored if custom_url provided\n        end_date: End date for custom range (YYYY-MM-DD) - ignored if custom_url provided\n        case_number: Filter by case number (partial match) - ignored if custom_url provided\n        case_title: Filter by case title (partial match) - ignored if custom_url provided\n        case_category: Filter by category ('Appeal', 'Certiorari', etc.) - ignored if custom_url provided\n        exclude_closed: Whether to exclude closed cases - ignored if custom_url provided\n        max_pages: Maximum pages to process (None for all)\n        output_prefix: Prefix for output files\n        custom_url: Pre-built search URL with embedded parameters (overrides all other search params)\n    \"\"\"\n\n    if custom_url:\n        # Use the provided URL directly\n        print(\"Using custom URL with embedded search parameters\")\n        print(\"\u26a0\ufe0f  WARNING: Custom URLs contain session-specific parameters that expire.\")\n        print(\"   This URL will only work temporarily and may become invalid after your browser session ends.\")\n        print(\"   For reliable, repeatable searches, use the CLI search parameters instead of --url option.\")\n        print()\n        base_url = custom_url\n        court_name = \"Custom Search\"  # Generic name since we don't know the court\n    else:\n        # Build search URL from parameters\n        search_builder = CourtSearchBuilder()\n\n        # Create parser instance early for court ID discovery\n        parser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\n\n        # Discover court IDs if not already done\n        if not search_builder.session_initialized:\n            print(\"Discovering court IDs from website...\")\n            search_builder.discover_court_ids(parser)\n\n        # Set court\n        search_builder.set_court(court)\n        court_info = search_builder.get_court_info()\n        court_name = court_info['name']\n\n        # Verify court ID was discovered\n        if court_info['id'] is None:\n            raise ValueError(f\"Could not discover court ID for {court_name}. \"\n                           \"Try using the --url option with a pre-built search URL instead.\")\n\n        # Set date range\n        if date_period == 'custom':\n            if not start_date or not end_date:\n                raise ValueError(\"Custom date range requires both start_date and end_date\")\n            search_builder.set_date_range(start_date, end_date, 'custom')\n        else:\n            search_builder.set_date_range(period=date_period)\n\n        # Set filters\n        if case_number:\n            search_builder.set_case_number_filter(case_number)\n        if case_title:\n            search_builder.set_case_title_filter(case_title)\n        if case_category:\n            search_builder.set_case_category(case_category)\n\n        search_builder.set_exclude_closed(exclude_closed)\n\n        # Build initial URL\n        base_url = search_builder.build_url(0)\n\n    print(\"Alabama Appeals Court - Configurable Data Extraction\")\n    print(\"=\" * 55)\n    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Date period: {date_period}\")\n    if date_period == 'custom':\n        print(f\"Date range: {start_date} to {end_date}\")\n    if case_number:\n        print(f\"Case number filter: {case_number}\")\n    if case_title:\n        print(f\"Case title filter: {case_title}\")\n    print(f\"Exclude closed: {exclude_closed}\")\n    print(f\"Max pages: {max_pages or 'All available'}\")\n    print()\n\n    # Create parser instance (may have been created earlier for court ID discovery)\n    if 'parser' not in locals():\n        parser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\n\n    try:\n        # First, get the first page to determine total pages\n        print(\"Loading first page to determine total results...\")\n        result = parser.parse_article(base_url)\n\n        if \"cases\" not in result or not result['cases']:\n            print(\"No cases found with the specified criteria.\")\n            return\n\n        # Try to get total pages from the URL after JavaScript execution\n        if hasattr(parser, 'driver') and parser.driver:\n            current_url = parser.driver.current_url\n            _, total_pages = parse_court_url(current_url)\n\n        if not total_pages:\n            # Estimate based on first page results\n            total_pages = 1\n            print(f\"Could not determine total pages, will process incrementally\")\n        else:\n            print(f\"Found {total_pages} total pages\")\n\n        # Apply max_pages limit\n        if max_pages and max_pages &lt; total_pages:\n            total_pages = max_pages\n            print(f\"Limited to {max_pages} pages\")\n\n        all_cases = []\n\n        # Process all pages\n        for page_num in range(total_pages):\n            print(f\"Processing page {page_num + 1}...\", end='', flush=True)\n\n            if page_num == 0:\n                # Use result from first page\n                page_result = result\n            else:\n                # Build URL for subsequent pages only if not using custom URL\n                if custom_url:\n                    # For custom URLs, we need to modify pagination manually\n                    # This is a simplified approach - in practice, you'd need to parse and modify the URL\n                    page_url = custom_url.replace('number~0', f'number~{page_num}')\n                else:\n                    page_url = search_builder.build_url(page_num)\n                page_result = parser.parse_article(page_url)\n\n            if \"cases\" in page_result and page_result['cases']:\n                all_cases.extend(page_result['cases'])\n                print(f\" Found {len(page_result['cases'])} cases\")\n            else:\n                print(\" No cases found\")\n                # If no cases on this page, we might have reached the end\n                break\n\n        # Create output data\n        output_data = {\n            \"status\": \"success\",\n            \"search_parameters\": {\n                \"court\": court if not custom_url else \"Custom URL\",\n                \"date_period\": date_period if not custom_url else \"Custom URL\",\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n                \"case_number_filter\": case_number,\n                \"case_title_filter\": case_title,\n                \"case_category\": case_category,\n                \"exclude_closed\": exclude_closed,\n                \"custom_url\": custom_url\n            },\n            \"total_cases\": len(all_cases),\n            \"extraction_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n            \"extraction_time\": datetime.now().strftime(\"%H:%M:%S\"),\n            \"pages_processed\": page_num + 1,\n            \"cases\": all_cases\n        }\n\n        # Save results\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        json_filename = f\"{output_prefix}_{timestamp}.json\"\n\n        with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n            json.dump(output_data, f, indent=4, ensure_ascii=False)\n\n        print(f\"\\n\u2713 Successfully extracted {len(all_cases)} court cases\")\n        print(f\"\u2713 Results saved to {json_filename}\")\n\n        # Create CSV if there are results\n        if all_cases:\n            csv_filename = f\"{output_prefix}_{timestamp}.csv\"\n            with open(csv_filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"Court,Case Number,Case Title,Classification,Filed Date,Status,Case Link\\n\")\n\n                for case in all_cases:\n                    court = case.get('court', '').replace(',', ';')\n                    case_num = case.get('case_number', {}).get('text', '').replace(',', ';')\n                    title = case.get('case_title', '').replace(',', ';').replace('\"', \"'\")\n                    classification = case.get('classification', '').replace(',', ';')\n                    filed = case.get('filed_date', '')\n                    status = case.get('status', '')\n                    link = f\"https://publicportal.alappeals.gov{case.get('case_number', {}).get('link', '')}\"\n\n                    f.write(f'\"{court}\",\"{case_num}\",\"{title}\",\"{classification}\",\"{filed}\",\"{status}\",\"{link}\"\\n')\n\n            print(f\"\u2713 CSV table saved to {csv_filename}\")\n\n        return output_data\n\n    except Exception as e:\n        print(f\"\\nError occurred: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None\n    finally:\n        parser._close_driver()\n        print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\ndef main():\n    \"\"\"Command line interface for the configurable court extractor\"\"\"\n    parser = argparse.ArgumentParser(description='Extract Alabama Court cases with configurable search parameters OR custom URL')\n\n    # URL option that overrides all search parameters\n    parser.add_argument('--url', help='Pre-built search URL with embedded parameters (overrides all search options)')\n\n    # Search parameter arguments (ignored if --url is provided)\n    parser.add_argument('--court', choices=['civil', 'criminal', 'supreme'], \n                       default='civil', help='Court to search (default: civil)')\n    parser.add_argument('--date-period', choices=['7d', '1m', '3m', '6m', '1y', 'custom'], \n                       default='1y', help='Date period for case search (default: 1y)')\n    parser.add_argument('--start-date', help='Start date for custom range (YYYY-MM-DD)')\n    parser.add_argument('--end-date', help='End date for custom range (YYYY-MM-DD)')\n    parser.add_argument('--case-number', help='Filter by case number (e.g., CL-2024-, CR-2024-, SC-2024-)')\n    parser.add_argument('--case-title', help='Filter by case title (partial match)')\n    parser.add_argument('--case-category', \n                       choices=['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question'],\n                       help='Filter by case category')\n    parser.add_argument('--exclude-closed', action='store_true', \n                       help='Exclude closed cases from results')\n\n    # Output options (always available)\n    parser.add_argument('--max-pages', type=int, \n                       help='Maximum number of pages to process (default: all)')\n    parser.add_argument('--output-prefix', default='court_cases',\n                       help='Prefix for output files (default: court_cases)')\n\n    args = parser.parse_args()\n\n    # If URL is provided, skip all parameter validation\n    if args.url:\n        print(\"Using custom URL - all search parameter options will be ignored\")\n        print(\"\u26a0\ufe0f  IMPORTANT: Custom URLs are session-based and temporary!\")\n        print(\"   Your URL may stop working when the court website session expires.\")\n        print(\"   Consider using CLI search parameters for reliable, repeatable searches.\")\n        print()\n        extract_court_cases_with_params(\n            custom_url=args.url,\n            max_pages=args.max_pages,\n            output_prefix=args.output_prefix\n        )\n        return\n\n    # Validate custom date range\n    if args.date_period == 'custom':\n        if not args.start_date or not args.end_date:\n            parser.error(\"Custom date period requires both --start-date and --end-date\")\n\n    # Validate case category for court\n    if args.case_category:\n        builder = CourtSearchBuilder()\n        builder.set_court(args.court)\n        try:\n            builder.validate_case_category(args.case_category)\n        except ValueError as e:\n            parser.error(str(e))\n\n    # Show case number format suggestion\n    if args.case_number:\n        builder = CourtSearchBuilder()\n        builder.set_court(args.court)\n        suggested_format = builder.format_case_number_suggestion()\n        print(f\"Case number format for {builder.get_court_info()['name']}: {suggested_format}\")\n\n    # Extract cases using search parameters\n    extract_court_cases_with_params(\n        court=args.court,\n        date_period=args.date_period,\n        start_date=args.start_date,\n        end_date=args.end_date,\n        case_number=args.case_number,\n        case_title=args.case_title,\n        case_category=args.case_category,\n        exclude_closed=args.exclude_closed,\n        max_pages=args.max_pages,\n        output_prefix=args.output_prefix\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"developer/configurable_court_extractor_design/#courtsearchbuilder-class","title":"CourtSearchBuilder Class","text":"<p>Line 12-17: Initialize with base URL and court ID constants. The <code>reset_params()</code> method sets up the default parameter structure.</p> <p>Line 19-46: The <code>reset_params()</code> method creates a nested dictionary structure that mirrors the complex URL parameters used by the Alabama Appeals Court portal.</p> <p>Line 48-74: <code>set_date_range()</code> handles both predefined periods and custom date ranges. It converts standard YYYY-MM-DD format to the portal's <code>*2f</code> encoding format.</p> <p>Line 140-169: <code>build_criteria_string()</code> is the core URL building logic. It constructs the complex nested parameter string with proper URL encoding.</p> <p>Line 171-180: <code>build_url()</code> ties everything together, updating pagination and returning the complete URL.</p>"},{"location":"developer/configurable_court_extractor_design/#main-extraction-function","title":"Main Extraction Function","text":"<p>Line 584-596: Function signature with comprehensive parameters including <code>custom_url</code> option for pre-built URLs.</p> <p>Line 614-647: Dual-mode logic - uses custom URL directly OR builds URL from search parameters.</p> <p>Line 649-661: Initial setup and parameter display for user feedback, with custom URL handling.</p> <p>Line 666-689: First page processing to determine total results and pages available.</p> <p>Line 694-713: Main processing loop that handles pagination dynamically for both custom URLs and built URLs.</p> <p>Line 719-738: Output data structure creation with search parameters preserved for reproducibility, including custom URL tracking.</p> <p>Line 740-760: File saving logic for both JSON and CSV formats.</p>"},{"location":"developer/configurable_court_extractor_design/#command-line-interface_1","title":"Command Line Interface","text":"<p>Line 781-807: Argument parser setup with <code>--url</code> option and all configuration options with help text.</p> <p>Line 811-819: Custom URL handling that bypasses all parameter validation when <code>--url</code> is provided.</p> <p>Line 821-854: Parameter validation and function execution for search-parameter mode.</p>"},{"location":"developer/configurable_court_extractor_design/#key-design-decisions-explained","title":"Key Design Decisions Explained","text":"<ol> <li>Builder Pattern: Separates URL construction complexity from business logic</li> <li>Dual-Mode Operation: Supports both parameter-based search and pre-built URL input</li> <li>Parameter Validation: Ensures required combinations are provided (custom dates)</li> <li>Progressive Enhancement: Starts with defaults, allows selective customization</li> <li>Error Recovery: Graceful handling when page counts can't be determined</li> <li>Output Consistency: Maintains same format as original extractor</li> <li>User Feedback: Real-time progress and parameter confirmation</li> <li>URL Flexibility: Custom URLs override all search parameters for maximum flexibility</li> </ol>"},{"location":"developer/court_scraper_analysis/","title":"Court Scraper Analysis","text":"<p>The ParserAppealsAL scraper is different from the news scrapers. This is because of the courts' use of JavaScript, which renders content dynamically. </p>"},{"location":"developer/court_scraper_analysis/#1-dynamic-javascript-rendered-content","title":"1. Dynamic JavaScript-Rendered Content","text":"<ul> <li>The site loads content dynamically, which means BeautifulSoup alone won't work</li> <li>You'll need Selenium or Playwright to render JavaScript before parsing</li> </ul>"},{"location":"developer/court_scraper_analysis/#2-complex-url-based-pagination","title":"2. Complex URL-Based Pagination","text":"<ul> <li>Pagination uses URL parameters (<code>page~(number~0)</code> becomes <code>page~(number~1)</code>)</li> <li>Total pages/elements are embedded in the URL (<code>totalElements~317~totalPages~13</code>)</li> <li>This is very different from the simple <code>/page/2</code> pattern in the news scrapers</li> </ul>"},{"location":"developer/court_scraper_analysis/#3-structured-table-data","title":"3. Structured Table Data","text":"<ul> <li>We needed 6 specific columns: Court, Case Number, Case Title, Classification, Filed Date, Open/Closed</li> <li>Case Numbers contained links we wanted to preserve (both text and href)</li> <li>We wanted all paginated results combined into one table</li> </ul>"},{"location":"developer/court_scraper_analysis/#4-custom-url-encoding","title":"4. Custom URL Encoding","text":"<ul> <li>The URLs use a unique encoding scheme with <code>~</code> and <code>%2a2f</code> (for slashes)</li> <li>Search parameters are complex with date ranges and multiple filters</li> </ul>"},{"location":"developer/court_scraper_analysis/#key-challenges","title":"Key Challenges:","text":"<ul> <li>Need to handle JavaScript rendering</li> <li>Must parse and manipulate the encoded URL format for pagination</li> <li>Need to extract both text and href attributes from case number links</li> <li>Different data structure (tabular vs. article paragraphs)</li> </ul>"},{"location":"developer/court_scraper_requirements/","title":"Court Case Scraper Extension Requirements","text":"<p>To help you build this court case scraper extension while maintaining OPAL's modularity, I'll need the following information:</p>"},{"location":"developer/court_scraper_requirements/#1-website-details","title":"1. Website Details","text":"<ul> <li>The URL of the court case website Example URL: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Example URLs of pages containing the tables you want to scrape Example URL after pagination of next batch of table elements: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~1~totalElements~317~totalPages~13%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Screenshots or HTML snippets of the table structure</li> </ul>"},{"location":"developer/court_scraper_requirements/#2-data-requirements","title":"2. Data Requirements","text":"<ul> <li>What specific data fields do you need from the tables? (case number, parties, dates, status, etc.) I will need to access the following html fields for data</li> </ul> <p>Column 1 Title: Court Court</p> <p>Column 1 Content Follows this pattern: Alabama Supreme Court</p> <p>Column 2 Title: Case Number Case Number</p> <p>Column 2 Content Follows this pattern:  SC-2025-0424 </p> <p>Column 3 Title: Case Title Case Title</p> <p>Column 3 Content Follows this pattern: Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.</p> <p>Column 4 Title: Classification Classification</p> <p>Column 4 Content Follows this pattern: Appeal - Civil - Injunction Other</p> <p>Column 5 Title: Filed Date Filed Date</p> <p>Column 5 Content Follows this pattern:  06/10/2025 </p> <p>Column 6 Title: Open / Closed Open / Closed</p> <p>Column 6 Content Follows this pattern:  Open </p> <ul> <li>Do you need data from multiple tables per page or one main table?</li> </ul> <p>I only want one table with all of the results of all of the pages at that url. Even if pagination is used to reduce the number of table elements that appear at a time, I want all the results in a single table.</p> <ul> <li>Any specific formatting requirements for the extracted data?</li> </ul>"},{"location":"developer/court_scraper_requirements/#3-navigation-pattern","title":"3. Navigation Pattern","text":"<ul> <li> <p>Does the site use pagination like the news sites? The pagination is not the same. The content is grouped into small chunks, but accessible at the same base url.</p> </li> <li> <p>Are there search/filter parameters in the URL? There are multiple search parameters in the URL. For example, here are the search terms for this url [case~%28caseCategoryID~1000000, caseNumberQueryTypeID~10463, aseTitleQueryTypeID~300054, iledDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025, excludeClosed~false%29%29]</p> </li> <li>Do you need to follow links within tables to get additional details?</li> </ul> <p>I do not want to follow the links within the table, but I do want to store the text and the reference embedded in the link.</p>"},{"location":"developer/court_scraper_requirements/#4-technical-considerations","title":"4. Technical Considerations","text":"<ul> <li>Does the site require authentication? No</li> <li>Is the content loaded dynamically (JavaScript) or static HTML? Dynamically</li> <li>Any rate limiting concerns we should be aware of? Please keep the rate limits low</li> </ul>"},{"location":"developer/court_scraper_requirements/#proposed-extension-architecture","title":"Proposed Extension Architecture","text":"<p>Based on OPAL's current architecture, here's how we'd extend it:</p> <ol> <li>Create a new parser class (e.g., <code>CourtCaseParser</code>) extending <code>NewsParser</code> in <code>parser_module.py</code></li> <li>Adapt or create a new URL discovery function if the pagination pattern differs from the news sites</li> <li>Modify the CLI in <code>main.py</code> to add the court parser option</li> <li>Ensure the output format makes sense for tabular data (might need to adjust from the line-by-line article format)</li> </ol>"},{"location":"developer/court_scraper_requirements/#next-steps","title":"Next Steps","text":"<p>Please provide: 1. The court website URL 2. Description of the table structure you need to parse 3. Any specific requirements or constraints</p> <p>This will help me design the extension to fit seamlessly with your existing OPAL architecture.</p>"},{"location":"developer/creating-parsers/","title":"Creating New Parsers","text":"<p>This guide explains how to create a new parser for OPAL to support additional websites.</p>"},{"location":"developer/creating-parsers/#overview","title":"Overview","text":"<p>All parsers inherit from the <code>BaseParser</code> class, which provides common functionality for web scraping.</p>"},{"location":"developer/creating-parsers/#step-1-create-parser-class","title":"Step 1: Create Parser Class","text":"<p>Create a new Python file in the <code>opal</code> directory:</p> <pre><code>from opal.BaseParser import BaseParser\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass ParserExample(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"ExampleParser\"\n</code></pre>"},{"location":"developer/creating-parsers/#step-2-implement-required-methods","title":"Step 2: Implement Required Methods","text":""},{"location":"developer/creating-parsers/#get_article_links","title":"get_article_links()","text":"<p>Extract article URLs from the main page:</p> <pre><code>def get_article_links(self, page_url):\n    \"\"\"Extract article links from a page.\"\"\"\n    response = requests.get(page_url, headers=self.headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    links = []\n    for article in soup.find_all('article'):\n        link = article.find('a')\n        if link and link.get('href'):\n            full_url = self.url + link['href']\n            links.append(full_url)\n\n    return links\n</code></pre>"},{"location":"developer/creating-parsers/#parse_article","title":"parse_article()","text":"<p>Extract data from individual articles:</p> <pre><code>def parse_article(self, article_url):\n    \"\"\"Parse individual article.\"\"\"\n    response = requests.get(article_url, headers=self.headers)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    return {\n        'title': soup.find('h1').text.strip(),\n        'content': soup.find('div', class_='content').text.strip(),\n        'date': soup.find('time')['datetime'],\n        'author': soup.find('span', class_='author').text.strip(),\n        'url': article_url\n    }\n</code></pre>"},{"location":"developer/creating-parsers/#extract_article_data","title":"extract_article_data()","text":"<p>Main method that orchestrates the scraping:</p> <pre><code>def extract_article_data(self):\n    \"\"\"Main extraction method.\"\"\"\n    all_articles = []\n\n    for page in range(1, self.max_pages + 1):\n        page_url = f\"{self.url}/page/{page}\"\n        links = self.get_article_links(page_url)\n\n        for link in links:\n            try:\n                article_data = self.parse_article(link)\n                all_articles.append(article_data)\n            except Exception as e:\n                self.logger.error(f\"Error parsing {link}: {e}\")\n\n    return all_articles\n</code></pre>"},{"location":"developer/creating-parsers/#step-3-handle-special-cases","title":"Step 3: Handle Special Cases","text":""},{"location":"developer/creating-parsers/#javascript-rendered-content","title":"JavaScript-Rendered Content","text":"<p>Use Selenium for dynamic content:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndef setup_driver(self):\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless')\n    service = Service(ChromeDriverManager().install())\n    return webdriver.Chrome(service=service, options=options)\n</code></pre>"},{"location":"developer/creating-parsers/#pagination","title":"Pagination","text":"<p>Handle different pagination styles:</p> <pre><code>def get_next_page_url(self, current_page):\n    # URL parameter style\n    return f\"{self.url}?page={current_page}\"\n\n    # Or path style\n    return f\"{self.url}/page/{current_page}\"\n\n    # Or offset style\n    offset = (current_page - 1) * 20\n    return f\"{self.url}?offset={offset}\"\n</code></pre>"},{"location":"developer/creating-parsers/#step-4-register-parser","title":"Step 4: Register Parser","text":"<p>Add your parser to <code>__main__.py</code>:</p> <pre><code>from opal.ParserExample import ParserExample\n\n# In the parser selection logic\nif args.parser == 'example':\n    parser = ParserExample(args.url, args.suffix, args.max_pages)\n</code></pre>"},{"location":"developer/creating-parsers/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Always wrap parsing logic in try-except blocks</li> <li>Logging: Use self.logger for debugging information</li> <li>Headers: Use appropriate User-Agent headers</li> <li>Rate Limiting: Add delays between requests if needed</li> <li>Testing: Test with various edge cases (empty content, missing elements)</li> </ol>"},{"location":"developer/creating-parsers/#example-complete-parser","title":"Example: Complete Parser","text":"<pre><code>from opal.BaseParser import BaseParser\nfrom bs4 import BeautifulSoup\nimport requests\nimport time\n\nclass ParserNewsSite(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"NewsSiteParser\"\n\n    def get_article_links(self, page_url):\n        response = requests.get(page_url, headers=self.headers)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        links = []\n        for item in soup.select('.article-item'):\n            link = item.select_one('a.title-link')\n            if link:\n                full_url = self.url + link['href']\n                links.append(full_url)\n\n        return links\n\n    def parse_article(self, article_url):\n        response = requests.get(article_url, headers=self.headers)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        return {\n            'title': soup.select_one('h1.article-title').text.strip(),\n            'content': soup.select_one('.article-body').text.strip(),\n            'date': soup.select_one('time.publish-date')['datetime'],\n            'author': soup.select_one('.author-name').text.strip(),\n            'url': article_url,\n            'tags': [tag.text for tag in soup.select('.tag')]\n        }\n\n    def extract_article_data(self):\n        all_articles = []\n\n        for page in range(1, self.max_pages + 1):\n            page_url = f\"{self.url}/articles?page={page}\"\n            self.logger.info(f\"Scraping page {page}\")\n\n            links = self.get_article_links(page_url)\n\n            for link in links:\n                try:\n                    article = self.parse_article(link)\n                    all_articles.append(article)\n                    time.sleep(1)  # Be respectful\n                except Exception as e:\n                    self.logger.error(f\"Error: {e}\")\n\n        return all_articles\n</code></pre>"},{"location":"developer/creating-parsers/#testing-your-parser","title":"Testing Your Parser","text":"<pre><code># Test with a small number of pages first\npython -m opal --url https://example.com --parser example --max_pages 2\n\n# Check the output\ncat opal_output.json | python -m json.tool\n</code></pre>"},{"location":"developer/error_handling/","title":"Error Handling","text":"<p>OPAL includes comprehensive error handling mechanisms to ensure robust data extraction. This document covers error types, handling strategies, and troubleshooting guidance.</p>"},{"location":"developer/error_handling/#error-categories","title":"Error Categories","text":""},{"location":"developer/error_handling/#1-selenium-webdriver-errors","title":"1. Selenium WebDriver Errors","text":""},{"location":"developer/error_handling/#timeoutexception","title":"TimeoutException","text":"<p>Occurs when elements don't load within expected timeframes.</p> <p>Common Causes: - Slow network connection - Page taking longer than usual to load - Element selectors changed on the website</p> <p>Handling Strategy: <pre><code>from selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support.ui import WebDriverWait\n\ntry:\n    element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, \"target-element\"))\n    )\nexcept TimeoutException:\n    logger.warning(\"Element not found within timeout, retrying...\")\n    # Implement retry logic or fallback\n</code></pre></p>"},{"location":"developer/error_handling/#staleelementreferenceexception","title":"StaleElementReferenceException","text":"<p>Occurs when trying to interact with elements that are no longer attached to the DOM.</p> <p>Handling Strategy: <pre><code>from selenium.common.exceptions import StaleElementReferenceException\n\ndef safe_element_interaction(driver, locator, action):\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            element = driver.find_element(*locator)\n            return action(element)\n        except StaleElementReferenceException:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)  # Wait before retry\n</code></pre></p>"},{"location":"developer/error_handling/#webdriverexception","title":"WebDriverException","text":"<p>General driver-related errors including crashes.</p> <p>Handling Strategy: <pre><code>from selenium.common.exceptions import WebDriverException\n\ndef restart_driver_on_failure(parser_instance):\n    try:\n        # Perform driver operation\n        return parser_instance.extract_data()\n    except WebDriverException as e:\n        logger.error(f\"Driver failed: {e}\")\n        parser_instance._restart_driver()\n        return parser_instance.extract_data()  # Retry once\n</code></pre></p>"},{"location":"developer/error_handling/#2-network-errors","title":"2. Network Errors","text":""},{"location":"developer/error_handling/#connection-timeouts","title":"Connection Timeouts","text":"<p>Network connectivity issues or server unresponsiveness.</p> <p>Handling Strategy: - Implement exponential backoff - Retry with longer timeouts - Check network connectivity</p> <pre><code>import time\nimport random\n\ndef retry_with_backoff(func, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except (requests.ConnectionError, requests.Timeout) as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = (2 ** attempt) + random.uniform(0, 1)\n            logger.warning(f\"Network error, retrying in {wait_time:.2f}s\")\n            time.sleep(wait_time)\n</code></pre>"},{"location":"developer/error_handling/#http-status-errors","title":"HTTP Status Errors","text":"<p>Server returning error status codes (404, 500, etc.)</p> <pre><code>def handle_http_errors(response):\n    if response.status_code == 404:\n        logger.error(\"Page not found - URL may have changed\")\n        return None\n    elif response.status_code &gt;= 500:\n        logger.error(\"Server error - may be temporary\")\n        raise ServerError(\"Server returned error status\")\n    elif response.status_code != 200:\n        logger.warning(f\"Unexpected status code: {response.status_code}\")\n</code></pre>"},{"location":"developer/error_handling/#3-parsing-errors","title":"3. Parsing Errors","text":""},{"location":"developer/error_handling/#elementnotfound","title":"ElementNotFound","text":"<p>Target HTML elements are missing or have changed.</p> <p>Handling Strategy: <pre><code>def safe_find_element(driver, primary_selector, fallback_selectors=None):\n    \"\"\"Try multiple selectors for robustness\"\"\"\n    selectors = [primary_selector] + (fallback_selectors or [])\n\n    for selector in selectors:\n        try:\n            return driver.find_element(*selector)\n        except NoSuchElementException:\n            continue\n\n    logger.error(\"No matching elements found with any selector\")\n    return None\n</code></pre></p>"},{"location":"developer/error_handling/#datavalidation","title":"DataValidation","text":"<p>Extracted data doesn't match expected format.</p> <pre><code>def validate_case_data(case_data):\n    \"\"\"Validate extracted court case data\"\"\"\n    required_fields = ['case_number', 'case_title', 'court', 'status']\n\n    for field in required_fields:\n        if field not in case_data or not case_data[field]:\n            logger.warning(f\"Missing required field: {field}\")\n            return False\n\n    # Validate date format\n    if case_data.get('filed_date'):\n        try:\n            datetime.strptime(case_data['filed_date'], '%m/%d/%Y')\n        except ValueError:\n            logger.warning(f\"Invalid date format: {case_data['filed_date']}\")\n            case_data['filed_date'] = None\n\n    return True\n</code></pre>"},{"location":"developer/error_handling/#4-session-management-errors","title":"4. Session Management Errors","text":""},{"location":"developer/error_handling/#session-expiration","title":"Session Expiration","text":"<p>Court system URLs contain session tokens that expire.</p> <p>Detection: <pre><code>def is_session_expired(driver):\n    \"\"\"Check if current session has expired\"\"\"\n    try:\n        # Look for session expired indicators\n        expired_indicators = [\n            \"Session has expired\",\n            \"Please log in again\",\n            \"Invalid session\"\n        ]\n\n        page_text = driver.page_source.lower()\n        return any(indicator.lower() in page_text for indicator in expired_indicators)\n    except:\n        return False\n</code></pre></p> <p>Handling: <pre><code>def handle_session_expiration(custom_url):\n    \"\"\"Provide guidance for expired sessions\"\"\"\n    logger.error(\"Session appears to have expired\")\n    logger.info(\"Custom URLs are session-based and expire after ~30 minutes\")\n    logger.info(\"Please create a new search and provide the fresh URL\")\n\n    # Suggest alternative\n    logger.info(\"Or use the configurable extractor to build a new search:\")\n    logger.info(\"python -m opal.configurable_court_extractor --court civil --date-period 7d\")\n</code></pre></p>"},{"location":"developer/error_handling/#5-configuration-errors","title":"5. Configuration Errors","text":""},{"location":"developer/error_handling/#invalid-parameters","title":"Invalid Parameters","text":"<p>User-provided parameters don't match expected values.</p> <pre><code>def validate_court_type(court_type):\n    \"\"\"Validate court type parameter\"\"\"\n    valid_courts = ['civil', 'criminal', 'supreme']\n    if court_type not in valid_courts:\n        raise ValueError(f\"Invalid court type: {court_type}. Must be one of {valid_courts}\")\n\ndef validate_date_period(date_period):\n    \"\"\"Validate date period parameter\"\"\"\n    valid_periods = ['7d', '1m', '3m', '6m', '1y', 'custom']\n    if date_period not in valid_periods:\n        raise ValueError(f\"Invalid date period: {date_period}. Must be one of {valid_periods}\")\n</code></pre>"},{"location":"developer/error_handling/#error-recovery-strategies","title":"Error Recovery Strategies","text":""},{"location":"developer/error_handling/#1-graceful-degradation","title":"1. Graceful Degradation","text":"<p>When non-critical operations fail, continue with reduced functionality:</p> <pre><code>def extract_with_fallbacks(driver):\n    \"\"\"Extract data with fallback strategies\"\"\"\n    results = []\n\n    try:\n        # Primary extraction method\n        results = extract_full_data(driver)\n    except Exception as e:\n        logger.warning(f\"Primary extraction failed: {e}\")\n\n        try:\n            # Fallback to basic extraction\n            results = extract_basic_data(driver)\n            logger.info(\"Using fallback extraction method\")\n        except Exception as e2:\n            logger.error(f\"Fallback extraction also failed: {e2}\")\n            # Return partial results if any\n            results = extract_minimal_data(driver)\n\n    return results\n</code></pre>"},{"location":"developer/error_handling/#2-partial-success-handling","title":"2. Partial Success Handling","text":"<p>Continue processing even when some operations fail:</p> <pre><code>def process_all_pages(page_urls):\n    \"\"\"Process all pages, continuing on individual failures\"\"\"\n    successful_pages = 0\n    failed_pages = 0\n    all_results = []\n\n    for i, url in enumerate(page_urls):\n        try:\n            logger.info(f\"Processing page {i+1}/{len(page_urls)}\")\n            page_results = extract_page_data(url)\n            all_results.extend(page_results)\n            successful_pages += 1\n        except Exception as e:\n            logger.error(f\"Failed to process page {i+1}: {e}\")\n            failed_pages += 1\n            continue  # Continue with next page\n\n    logger.info(f\"Completed: {successful_pages} successful, {failed_pages} failed\")\n    return all_results\n</code></pre>"},{"location":"developer/error_handling/#3-state-recovery","title":"3. State Recovery","text":"<p>Save progress to recover from failures:</p> <pre><code>import json\nimport os\n\nclass StatefulExtractor:\n    def __init__(self, state_file=\"extraction_state.json\"):\n        self.state_file = state_file\n        self.state = self.load_state()\n\n    def load_state(self):\n        \"\"\"Load previous state if exists\"\"\"\n        if os.path.exists(self.state_file):\n            with open(self.state_file, 'r') as f:\n                return json.load(f)\n        return {\"completed_pages\": [], \"results\": []}\n\n    def save_state(self):\n        \"\"\"Save current state\"\"\"\n        with open(self.state_file, 'w') as f:\n            json.dump(self.state, f)\n\n    def extract_with_recovery(self, page_urls):\n        \"\"\"Extract data with state recovery\"\"\"\n        for url in page_urls:\n            if url in self.state[\"completed_pages\"]:\n                logger.info(f\"Skipping already processed page: {url}\")\n                continue\n\n            try:\n                results = extract_page_data(url)\n                self.state[\"results\"].extend(results)\n                self.state[\"completed_pages\"].append(url)\n                self.save_state()  # Save after each page\n            except Exception as e:\n                logger.error(f\"Failed to process {url}: {e}\")\n                continue\n\n        # Clean up state file on completion\n        if os.path.exists(self.state_file):\n            os.remove(self.state_file)\n\n        return self.state[\"results\"]\n</code></pre>"},{"location":"developer/error_handling/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"developer/error_handling/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass StructuredLogger:\n    def __init__(self, name):\n        self.logger = logging.getLogger(name)\n\n    def log_extraction_start(self, court_type, parameters):\n        \"\"\"Log extraction start with context\"\"\"\n        self.logger.info(\"Extraction started\", extra={\n            \"event\": \"extraction_start\",\n            \"court_type\": court_type,\n            \"parameters\": parameters,\n            \"timestamp\": datetime.utcnow().isoformat()\n        })\n\n    def log_error(self, error_type, error_message, context=None):\n        \"\"\"Log errors with structured data\"\"\"\n        self.logger.error(\"Error occurred\", extra={\n            \"event\": \"error\",\n            \"error_type\": error_type,\n            \"error_message\": str(error_message),\n            \"context\": context or {},\n            \"timestamp\": datetime.utcnow().isoformat()\n        })\n</code></pre>"},{"location":"developer/error_handling/#error-metrics","title":"Error Metrics","text":"<p>Track error rates and types:</p> <pre><code>from collections import defaultdict\nimport time\n\nclass ErrorMetrics:\n    def __init__(self):\n        self.error_counts = defaultdict(int)\n        self.start_time = time.time()\n\n    def record_error(self, error_type):\n        \"\"\"Record an error occurrence\"\"\"\n        self.error_counts[error_type] += 1\n\n    def get_error_summary(self):\n        \"\"\"Get summary of all errors\"\"\"\n        total_errors = sum(self.error_counts.values())\n        runtime = time.time() - self.start_time\n\n        return {\n            \"total_errors\": total_errors,\n            \"error_rate\": total_errors / (runtime / 60),  # errors per minute\n            \"error_breakdown\": dict(self.error_counts),\n            \"runtime_minutes\": runtime / 60\n        }\n</code></pre>"},{"location":"developer/error_handling/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"developer/error_handling/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"developer/error_handling/#1-element-not-found-errors","title":"1. \"Element not found\" errors","text":"<p>Symptoms: NoSuchElementException, TimeoutException Causes: Website changes, slow loading, wrong selectors Solutions: - Check if website structure changed - Increase timeout values - Use more robust selectors - Add wait conditions</p>"},{"location":"developer/error_handling/#2-empty-results-returned","title":"2. Empty results returned","text":"<p>Symptoms: No data extracted, empty result sets Causes: Page not loading, changed selectors, session issues Solutions: - Run in non-headless mode to see browser - Check page source for expected elements - Verify URL is correct and accessible - Check for session expiration</p>"},{"location":"developer/error_handling/#3-driver-crashes-or-hangs","title":"3. Driver crashes or hangs","text":"<p>Symptoms: WebDriverException, processes not terminating Causes: Memory issues, driver version problems, resource limits Solutions: - Update ChromeDriver - Increase system resources - Add proper cleanup in finally blocks - Use context managers for driver lifecycle</p>"},{"location":"developer/error_handling/#4-slow-extraction-performance","title":"4. Slow extraction performance","text":"<p>Symptoms: Very slow processing, timeouts Causes: Network issues, server rate limiting, inefficient code Solutions: - Adjust rate limiting parameters - Optimize element selection - Use headless mode - Check network connectivity</p>"},{"location":"developer/error_handling/#debug-mode-usage","title":"Debug Mode Usage","text":"<p>Enable comprehensive debugging:</p> <pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Run extraction with debugging\nfrom opal.configurable_court_extractor import extract_court_cases_with_params\n\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='7d',\n    max_pages=3\n    # The function internally uses ParserAppealsAL with headless=True by default\n    # To debug, modify the function to set headless=False\n)\n\n# Add breakpoints for inspection\nimport pdb; pdb.set_trace()\n</code></pre>"},{"location":"developer/error_handling/#health-checks","title":"Health Checks","text":"<p>Implement health checks for long-running extractions:</p> <pre><code>def health_check(driver):\n    \"\"\"Check if extraction environment is healthy\"\"\"\n    checks = {\n        \"driver_responsive\": False,\n        \"page_loaded\": False,\n        \"elements_present\": False\n    }\n\n    try:\n        # Check driver responsiveness\n        driver.current_url\n        checks[\"driver_responsive\"] = True\n\n        # Check page loaded\n        driver.execute_script(\"return document.readyState\") == \"complete\"\n        checks[\"page_loaded\"] = True\n\n        # Check for expected elements\n        driver.find_element(By.TAG_NAME, \"body\")\n        checks[\"elements_present\"] = True\n\n    except Exception as e:\n        logger.warning(f\"Health check failed: {e}\")\n\n    return all(checks.values()), checks\n</code></pre>"},{"location":"developer/error_handling/#best-practices","title":"Best Practices","text":"<ol> <li>Always use timeouts - Never wait indefinitely</li> <li>Implement retry logic - Network issues are common</li> <li>Log comprehensively - Errors and successful operations</li> <li>Validate data - Check extracted data makes sense</li> <li>Handle partial failures - Don't let one failure stop everything</li> <li>Clean up resources - Always close drivers and files</li> <li>Provide user feedback - Show progress and error context</li> <li>Plan for recovery - Save state for long operations</li> </ol>"},{"location":"developer/error_handling/#error-response-formats","title":"Error Response Formats","text":"<p>Standardized error responses for consistency:</p> <pre><code>def create_error_response(error_type, message, context=None):\n    \"\"\"Create standardized error response\"\"\"\n    return {\n        \"status\": \"error\",\n        \"error_type\": error_type,\n        \"error_message\": message,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"context\": context or {}\n    }\n\n# Usage examples\ntimeout_error = create_error_response(\n    \"TimeoutError\",\n    \"Page failed to load within 30 seconds\",\n    {\"url\": \"https://example.com\", \"timeout\": 30}\n)\n\nsession_error = create_error_response(\n    \"SessionExpired\",\n    \"Court session has expired\",\n    {\"suggestion\": \"Create a new search\"}\n)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/","title":"User-Agent Headers Guide","text":""},{"location":"developer/user_agent_headers_guide/#what-is-a-user-agent","title":"What is a User-Agent?","text":"<p>User-Agent headers are strings that identify the client (browser, bot, or application) making an HTTP request to a web server.</p> <p>It's an HTTP header that tells the server: - What software is making the request - What version it is - What operating system it's running on</p>"},{"location":"developer/user_agent_headers_guide/#examples-of-user-agent-strings","title":"Examples of User-Agent Strings","text":""},{"location":"developer/user_agent_headers_guide/#chrome-browser","title":"Chrome Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#python-requests-default","title":"Python Requests (default)","text":"<pre><code>python-requests/2.28.0\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#googlebot","title":"Googlebot","text":"<pre><code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#firefox-browser","title":"Firefox Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#safari-browser","title":"Safari Browser","text":"<pre><code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#why-user-agents-matter","title":"Why User-Agents Matter","text":"<ol> <li>Server Behavior: Websites may serve different content based on User-Agent</li> <li>Access Control: Some sites block requests with suspicious or missing User-Agents</li> <li>Analytics: Helps websites understand their traffic</li> <li>Bot Detection: Sites use it to identify and potentially block scrapers</li> <li>Content Optimization: Sites may serve mobile vs desktop versions</li> </ol>"},{"location":"developer/user_agent_headers_guide/#setting-user-agent-in-python","title":"Setting User-Agent in Python","text":""},{"location":"developer/user_agent_headers_guide/#basic-example","title":"Basic Example","text":"<pre><code>import requests\n\n# Without User-Agent (might be blocked)\nresponse = requests.get('https://example.com')\n\n# With User-Agent (appears as a browser)\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#advanced-example-with-multiple-headers","title":"Advanced Example with Multiple Headers","text":"<pre><code>headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Accept-Encoding': 'gzip, deflate',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n}\n\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#common-user-agent-patterns","title":"Common User-Agent Patterns","text":""},{"location":"developer/user_agent_headers_guide/#desktop-browsers","title":"Desktop Browsers","text":"<pre><code># Windows Chrome\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n\n# macOS Safari\n'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'\n\n# Linux Firefox\n'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#mobile-browsers","title":"Mobile Browsers","text":"<pre><code># iPhone Safari\n'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n\n# Android Chrome\n'Mozilla/5.0 (Linux; Android 11; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#custom-bot-honest-approach","title":"Custom Bot (Honest Approach)","text":"<pre><code>'OPAL-Bot/1.0 (+https://github.com/yourusername/opal)'\n'MyCompany-Scraper/2.1 (contact@mycompany.com)'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#implementing-user-agents-in-opal","title":"Implementing User-Agents in OPAL","text":""},{"location":"developer/user_agent_headers_guide/#enhanced-baseparser","title":"Enhanced BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers with proper headers\"\"\"\n\n    # Realistic browser headers\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.9',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'DNT': '1',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n    }\n\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, headers=headers, timeout=5)\n            response.raise_for_status()\n            responses.append(response.text)\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n\n    return responses, successful_urls\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#user-agent-rotation","title":"User-Agent Rotation","text":"<pre><code>import random\n\nclass RotatingUserAgentParser(BaseParser):\n    def __init__(self):\n        super().__init__()\n        self.user_agents = [\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0',\n        ]\n\n    def get_random_user_agent(self):\n        return random.choice(self.user_agents)\n\n    def make_request(self, urls):\n        # Use different User-Agent for each request\n        headers = {'User-Agent': self.get_random_user_agent()}\n        # ... rest of request logic\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#best-practices","title":"Best Practices","text":""},{"location":"developer/user_agent_headers_guide/#1-be-strategic","title":"1. Be Strategic","text":"<ul> <li>Use real User-Agents: Copy from actual browsers</li> <li>Stay current: Browser versions change frequently</li> <li>Match behavior: If you claim to be Chrome, act like Chrome</li> </ul>"},{"location":"developer/user_agent_headers_guide/#2-be-respectful","title":"2. Be Respectful","text":"<ul> <li>Respect robots.txt: Even with a browser User-Agent</li> <li>Rate limit: Don't overwhelm servers</li> <li>Be honest when possible: Some sites appreciate transparent bots</li> </ul>"},{"location":"developer/user_agent_headers_guide/#3-be-consistent","title":"3. Be Consistent","text":"<ul> <li>Use complete headers: Include Accept, Accept-Language, etc.</li> <li>Maintain session: Use the same User-Agent throughout a session</li> <li>Handle responses: Check if the site is behaving differently</li> </ul>"},{"location":"developer/user_agent_headers_guide/#4-be-prepared","title":"4. Be Prepared","text":"<ul> <li>Rotate User-Agents: Avoid detection patterns</li> <li>Handle blocks: Have fallback strategies</li> <li>Monitor changes: Sites may update their detection methods</li> </ul>"},{"location":"developer/user_agent_headers_guide/#user-agent-detection-techniques","title":"User-Agent Detection Techniques","text":"<p>Websites can detect fake User-Agents by:</p> <ol> <li>Header Analysis: Checking if browser behavior matches the User-Agent</li> <li>Missing Headers: Looking for headers real browsers always send</li> <li>JavaScript Testing: Testing browser capabilities that match the claimed version</li> <li>Request Patterns: Analyzing timing and request sequences</li> <li>Feature Detection: Checking for browser-specific features</li> </ol>"},{"location":"developer/user_agent_headers_guide/#common-mistakes","title":"Common Mistakes","text":""},{"location":"developer/user_agent_headers_guide/#1-outdated-user-agents","title":"1. Outdated User-Agents","text":"<pre><code># Bad - very old browser version\n'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 Chrome/45.0.2454.85'\n\n# Good - recent browser version\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/91.0.4472.124'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#2-inconsistent-headers","title":"2. Inconsistent Headers","text":"<pre><code># Bad - claims to be Chrome but uses Firefox Accept header\nheaders = {\n    'User-Agent': 'Chrome/91.0.4472.124',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'  # Firefox style\n}\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#3-missing-common-headers","title":"3. Missing Common Headers","text":"<pre><code># Bad - only User-Agent\nheaders = {'User-Agent': 'Mozilla/5.0...'}\n\n# Good - realistic browser headers\nheaders = {\n    'User-Agent': 'Mozilla/5.0...',\n    'Accept': 'text/html,application/xhtml+xml...',\n    'Accept-Language': 'en-US,en;q=0.9',\n    'Accept-Encoding': 'gzip, deflate, br',\n}\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#testing-user-agents","title":"Testing User-Agents","text":""},{"location":"developer/user_agent_headers_guide/#check-what-youre-sending","title":"Check What You're Sending","text":"<pre><code>import requests\n\n# Test your headers\nresponse = requests.get('https://httpbin.org/headers', headers=your_headers)\nprint(response.json())\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#verify-server-response","title":"Verify Server Response","text":"<pre><code># Check if the site is treating you differently\nresponse_bot = requests.get(url)  # Default requests User-Agent\nresponse_browser = requests.get(url, headers=browser_headers)\n\nif response_bot.content != response_browser.content:\n    print(\"Site serves different content based on User-Agent\")\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#tools-and-resources","title":"Tools and Resources","text":"<ul> <li>Browser DevTools: Copy real User-Agent strings from Network tab</li> <li>User-Agent Databases: Sites like whatismybrowser.com</li> <li>Header Checkers: Use httpbin.org to test your headers</li> <li>Browser Testing: Use Selenium to see what real browsers send</li> </ul>"},{"location":"developer/user_agent_headers_guide/#conclusion","title":"Conclusion","text":"<p>User-Agent headers are a crucial part of web scraping that can mean the difference between successful data extraction and being blocked. Use them thoughtfully and responsibly to build robust scrapers that respect both the technical and ethical aspects of web crawling.</p>"},{"location":"developer/workflows/","title":"Workflows","text":"<p>This document describes the internal workflows and processing patterns used by OPAL parsers, particularly focusing on the court parsing system.</p>"},{"location":"developer/workflows/#court-id-discovery-workflow","title":"Court ID Discovery Workflow","text":"<p>The CourtSearchBuilder automatically discovers available courts and their IDs from the Alabama Appeals Court portal:</p> <pre><code>graph TD\n    A[Navigate to Search Page] --&gt; B[Wait for Page Load]\n    B --&gt; C{Court Elements Found?}\n    C --&gt;|Yes| D[Extract Court Options]\n    C --&gt;|No| E[Use Fallback IDs]\n    D --&gt; F[Parse Court Names and IDs]\n    F --&gt; G[Store Court Mapping]\n    E --&gt; G</code></pre>"},{"location":"developer/workflows/#implementation-details","title":"Implementation Details","text":"<ol> <li>Navigation: Load <code>https://publicportal.alappeals.gov/portal/search/case</code></li> <li>Element Detection: Look for court selection elements</li> <li>Data Extraction: Parse option elements for court names and values</li> <li>Fallback Strategy: Use known court ID for civil court if discovery fails</li> </ol> <pre><code># Actual implementation in CourtSearchBuilder\ndef discover_court_ids(self, parser_instance):\n    try:\n        # Navigate to search page\n        parser_instance.driver.get(\"https://publicportal.alappeals.gov/portal/search/case\")\n\n        # Wait for page load\n        time.sleep(3)\n\n        # Try to find court selection elements\n        court_elements = parser_instance.driver.find_elements(\n            By.XPATH, \n            \"//select[@id='court'] | //select[contains(@name, 'court')] | //div[contains(@class, 'court')]\"\n        )\n\n        if court_elements:\n            # Extract court IDs from dropdown options\n            for element in court_elements:\n                options = element.find_elements(By.TAG_NAME, \"option\")\n                for option in options:\n                    court_name = option.text.lower()\n                    court_id = option.get_attribute(\"value\")\n\n                    # Map court names to internal keys\n                    if \"civil\" in court_name and \"appeals\" in court_name:\n                        self.courts['civil']['id'] = court_id\n                    elif \"criminal\" in court_name and \"appeals\" in court_name:\n                        self.courts['criminal']['id'] = court_id\n                    elif \"supreme\" in court_name:\n                        self.courts['supreme']['id'] = court_id\n        else:\n            # Use known working ID for civil court\n            self.courts['civil']['id'] = '68f021c4-6a44-4735-9a76-5360b2e8af13'\n\n    except Exception as e:\n        # Fallback to known civil court ID\n        self.courts['civil']['id'] = '68f021c4-6a44-4735-9a76-5360b2e8af13'\n</code></pre>"},{"location":"developer/workflows/#dynamic-pagination-workflow","title":"Dynamic Pagination Workflow","text":"<p>The system handles JavaScript-based pagination:</p> <pre><code>graph TD\n    A[Load First Page] --&gt; B[Extract Current URL]\n    B --&gt; C[Parse Page Info from URL]\n    C --&gt; D{Total Pages &gt; 1?}\n    D --&gt;|Yes| E[Generate Page URLs]\n    D --&gt;|No| F[Process Single Page]\n    E --&gt; G[Process Each Page]\n    G --&gt; H[Aggregate Results]\n    F --&gt; H</code></pre>"},{"location":"developer/workflows/#steps","title":"Steps:","text":"<ol> <li>Initial Load: Load search results first page</li> <li>URL Analysis: Extract pagination data from JavaScript-updated URL</li> <li>Page Generation: Create URLs for all result pages</li> <li>Sequential Processing: Load and extract data from each page</li> <li>Result Aggregation: Combine all extracted cases</li> </ol>"},{"location":"developer/workflows/#session-management-workflow","title":"Session Management Workflow","text":"<p>Handling session-based URLs and expiration:</p> <pre><code>graph TD\n    A[Receive Custom URL] --&gt; B{Check URL Age}\n    B --&gt;|Fresh| C[Use URL Directly]\n    B --&gt;|Possibly Expired| D[Warn User]\n    D --&gt; E{User Proceeds?}\n    E --&gt;|Yes| C\n    E --&gt;|No| F[Request New Search]\n    C --&gt; G[Process Results]\n    G --&gt; H{Session Valid?}\n    H --&gt;|Yes| I[Continue Processing]\n    H --&gt;|No| J[Session Expired Error]</code></pre>"},{"location":"developer/workflows/#key-considerations","title":"Key Considerations:","text":"<ul> <li>URLs contain session tokens that expire after ~30 minutes</li> <li>System warns users about potentially expired URLs</li> <li>Provides clear error messages when sessions expire</li> <li>Suggests creating new search for expired sessions</li> </ul>"},{"location":"developer/workflows/#error-recovery-workflow","title":"Error Recovery Workflow","text":"<p>Robust error handling throughout the extraction process:</p> <pre><code>graph TD\n    A[Start Extraction] --&gt; B{Error Occurred?}\n    B --&gt;|No| C[Continue Processing]\n    B --&gt;|Yes| D{Error Type}\n    D --&gt;|Timeout| E[Retry with Backoff]\n    D --&gt;|Element Not Found| F[Skip and Log]\n    D --&gt;|Driver Crash| G[Restart Driver]\n    D --&gt;|Network Error| H[Wait and Retry]\n    E --&gt; I{Retry Successful?}\n    I --&gt;|Yes| C\n    I --&gt;|No| J[Log Error and Continue]\n    F --&gt; J\n    G --&gt; K[Reinitialize and Resume]\n    H --&gt; I</code></pre>"},{"location":"developer/workflows/#error-types-and-handling","title":"Error Types and Handling:","text":"<ol> <li>Selenium Timeouts: Retry with exponential backoff</li> <li>Stale Elements: Re-locate elements before interaction</li> <li>Driver Crashes: Restart driver and resume from last page</li> <li>Network Errors: Implement retry logic with delays</li> </ol>"},{"location":"developer/workflows/#data-extraction-workflow","title":"Data Extraction Workflow","text":"<p>Extracting case information from court pages:</p> <pre><code>graph TD\n    A[Page Loaded] --&gt; B[Locate Case Table]\n    B --&gt; C[Find All Case Rows]\n    C --&gt; D[For Each Row]\n    D --&gt; E[Extract Case Number]\n    E --&gt; F[Extract Case Link]\n    F --&gt; G[Extract Case Title]\n    G --&gt; H[Extract Metadata]\n    H --&gt; I[Validate Data]\n    I --&gt; J{Valid?}\n    J --&gt;|Yes| K[Add to Results]\n    J --&gt;|No| L[Log Warning]\n    K --&gt; M{More Rows?}\n    L --&gt; M\n    M --&gt;|Yes| D\n    M --&gt;|No| N[Return Results]</code></pre>"},{"location":"developer/workflows/#data-points-extracted","title":"Data Points Extracted:","text":"<ul> <li>Case number and detail link</li> <li>Case title</li> <li>Court name</li> <li>Classification/type</li> <li>Filing date</li> <li>Current status</li> </ul>"},{"location":"developer/workflows/#integration-workflow","title":"Integration Workflow","text":"<p>How OPAL components work together:</p> <pre><code>graph TD\n    A[User Request] --&gt; B{Use Custom URL?}\n    B --&gt;|Yes| C[extract_court_cases_with_params with custom_url]\n    B --&gt;|No| D[CourtSearchBuilder]\n    D --&gt; E[discover_court_ids with ParserAppealsAL]\n    E --&gt; F[Build Search Parameters]\n    F --&gt; G[build_url]\n    C --&gt; H[ParserAppealsAL.parse_article]\n    G --&gt; H\n    H --&gt; I[court_url_paginator functions]\n    I --&gt; J[Process All Pages]\n    J --&gt; K[Aggregate Results]\n    K --&gt; L[Generate JSON/CSV Output]</code></pre>"},{"location":"developer/workflows/#command-line-workflow","title":"Command-Line Workflow","text":"<p>Processing flow for CLI usage:</p> <pre><code>graph TD\n    A[Parse CLI Arguments] --&gt; B{URL Provided?}\n    B --&gt;|Yes| C[Use Custom URL Path]\n    B --&gt;|No| D[Validate Search Parameters]\n    D --&gt; E[extract_court_cases_with_params]\n    C --&gt; E\n    E --&gt; F[CourtSearchBuilder.discover_court_ids]\n    F --&gt; G[Build Search URL]\n    G --&gt; H[ParserAppealsAL Processing]\n    H --&gt; I[Paginate Results]\n    I --&gt; J[Save JSON Output]\n    J --&gt; K[Save CSV Output]\n    K --&gt; L[Display Summary]</code></pre>"},{"location":"developer/workflows/#performance-optimization-workflow","title":"Performance Optimization Workflow","text":"<p>Strategies for efficient extraction:</p> <ol> <li>Batch Processing</li> <li>Process multiple pages concurrently where possible</li> <li> <p>Aggregate results before writing to disk</p> </li> <li> <p>Caching Strategy</p> </li> <li>Cache court IDs after discovery</li> <li> <p>Reuse WebDriver instance across pages</p> </li> <li> <p>Rate Limiting</p> </li> <li>Implement delays between requests</li> <li> <p>Respect server resources</p> </li> <li> <p>Memory Management</p> </li> <li>Process large result sets in chunks</li> <li>Clear driver cache periodically</li> </ol>"},{"location":"developer/workflows/#testing-workflow","title":"Testing Workflow","text":"<p>Ensuring reliability:</p> <pre><code>graph TD\n    A[Unit Tests] --&gt; B[Integration Tests]\n    B --&gt; C[End-to-End Tests]\n    C --&gt; D{All Pass?}\n    D --&gt;|Yes| E[Deploy]\n    D --&gt;|No| F[Fix Issues]\n    F --&gt; A</code></pre>"},{"location":"developer/workflows/#test-categories","title":"Test Categories:","text":"<ol> <li>Unit Tests: Individual function testing</li> <li>Integration Tests: Component interaction</li> <li>End-to-End Tests: Full extraction workflows</li> <li>Performance Tests: Load and speed testing</li> </ol>"},{"location":"developer/workflows/#debugging-workflow","title":"Debugging Workflow","text":"<p>Troubleshooting extraction issues:</p> <ol> <li> <p>Enable Debug Logging <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Run in Non-Headless Mode <pre><code>parser = ParserAppealsAL(headless=False)\n</code></pre></p> </li> <li> <p>Add Breakpoints</p> </li> <li>Inspect page state</li> <li>Verify element selection</li> <li> <p>Check data extraction</p> </li> <li> <p>Save Screenshots <pre><code>driver.save_screenshot(\"debug_state.png\")\n</code></pre></p> </li> <li> <p>Export Page Source <pre><code>with open(\"page_source.html\", \"w\") as f:\n    f.write(driver.page_source)\n</code></pre></p> </li> </ol>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>OPAL can be configured through command-line arguments, environment variables, and parser-specific options.</p>"},{"location":"getting-started/configuration/#command-line-arguments","title":"Command-Line Arguments","text":""},{"location":"getting-started/configuration/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>--url</code>: The base URL to scrape</li> <li><code>--parser</code>: The parser to use (Parser1819, ParserDailyNews, court)</li> </ul>"},{"location":"getting-started/configuration/#optional-arguments","title":"Optional Arguments","text":"<ul> <li><code>--suffix</code>: URL suffix for news articles (default: '')</li> <li><code>--max_pages</code>: Maximum number of pages to scrape (default: 5)</li> <li><code>--output</code>: Output file path (default: opal_output.json)</li> <li><code>--log-level</code>: Logging level (DEBUG, INFO, WARNING, ERROR)</li> </ul>"},{"location":"getting-started/configuration/#parser-specific-configuration","title":"Parser-Specific Configuration","text":""},{"location":"getting-started/configuration/#news-parsers-parser1819-parserdailynews","title":"News Parsers (Parser1819, ParserDailyNews)","text":"<ul> <li>Require <code>--suffix</code> parameter for article URLs</li> <li>Support pagination with <code>--max_pages</code></li> </ul>"},{"location":"getting-started/configuration/#court-parser-parserappealsal","title":"Court Parser (ParserAppealsAL)","text":"<p>Basic configuration: - Automatically handles Chrome WebDriver setup - Processes all available court cases - No pagination parameters needed</p> <p>Advanced configuration options: - <code>headless</code> (bool): Run browser in headless mode (default: True) - <code>rate_limit_seconds</code> (int): Delay between requests (default: 3)</p> <pre><code>from opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL(\n    headless=True,              # Run without visible browser\n    rate_limit_seconds=2        # 2 second delay between pages\n)\n</code></pre>"},{"location":"getting-started/configuration/#chrome-webdriver-options","title":"Chrome WebDriver Options","text":"<p>The court parser configures Chrome with these options: - <code>--disable-gpu</code>: Disable GPU hardware acceleration - <code>--no-sandbox</code>: Required for some environments - <code>--disable-dev-shm-usage</code>: Overcome limited resource problems - <code>--window-size=1920,1080</code>: Set browser window size</p> <p>Custom WebDriver options can be set:</p> <pre><code>from selenium import webdriver\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Custom Chrome options would need to be set in the parser's _setup_driver method\n# The parser uses webdriver_manager for automatic ChromeDriver management\n</code></pre>"},{"location":"getting-started/configuration/#configurable-court-extractor","title":"Configurable Court Extractor","text":"<p>The configurable court extractor supports additional parameters through the main function:</p> <pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\nresults = extract_court_cases_with_params(\n    court=\"civil\",              # Court selection\n    date_period=\"1m\",           # Date filtering\n    case_category=\"Appeal\",     # Case type filtering\n    max_pages=10,               # Limit pages processed\n    output_prefix=\"custom\"      # Output file prefix\n)\n</code></pre> <p>See Configurable Court Extractor for detailed configuration options.</p>"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"<p>By default, OPAL outputs data in JSON format to <code>opal_output.json</code>. You can specify a different output file:</p> <pre><code>python -m opal --url https://example.com --parser Parser1819 --output my_data.json\n</code></pre>"},{"location":"getting-started/configuration/#logging","title":"Logging","text":"<p>Control logging verbosity with <code>--log-level</code>:</p> <pre><code>python -m opal --url https://example.com --parser Parser1819 --log-level DEBUG\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.6 or higher</li> <li>Chrome browser (for court parser)</li> </ul>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/alabama-forward/opal_beautifulsoup\ncd opal\n\n#Install dependencies\npip install -r requirements.txt\n\n#Install OPAL\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>#Check OPAL is installed\npython -m opal --help\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with OPAL quickly.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#news-scraping","title":"News Scraping","text":"<p>To scrape news articles from 1819news.com:</p> <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5\n</code></pre>"},{"location":"getting-started/quickstart/#court-records-scraping","title":"Court Records Scraping","text":"<p>To scrape court cases from Alabama Appeals Court:</p> <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre>"},{"location":"getting-started/quickstart/#common-options","title":"Common Options","text":"<ul> <li><code>--url</code>: The base URL to scrape</li> <li><code>--parser</code>: The parser to use (Parser1819, ParserDailyNews, court)</li> <li><code>--suffix</code>: URL suffix for news articles</li> <li><code>--max_pages</code>: Maximum number of pages to scrape</li> <li><code>--output</code>: Output file path (default: opal_output.json)</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the CLI Usage Guide for detailed command options</li> <li>Check Available Parsers for all supported websites</li> <li>Learn about Output Formats for data analysis</li> </ul>"},{"location":"user-guide/cli-usage/","title":"CLI Usage","text":"<p>The OPAL command-line interface provides a simple way to scrape content from supported websites.</p>"},{"location":"user-guide/cli-usage/#basic-command-structure","title":"Basic Command Structure","text":"<pre><code>python -m opal --url &lt;URL&gt; --parser &lt;PARSER&gt; [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli-usage/#arguments","title":"Arguments","text":""},{"location":"user-guide/cli-usage/#required-arguments","title":"Required Arguments","text":"Argument Description Example <code>--url</code> The base URL to scrape <code>https://1819news.com/</code> <code>--parser</code> Parser to use <code>Parser1819</code>, <code>ParserDailyNews</code>, <code>ParserAppealsAL</code>"},{"location":"user-guide/cli-usage/#optional-arguments","title":"Optional Arguments","text":"Argument Description Default <code>--suffix</code> URL suffix for articles <code>''</code> <code>--max_pages</code> Maximum pages to scrape <code>5</code> <code>--output</code> Output file path <code>opal_output.json</code> <code>--log-level</code> Logging level <code>INFO</code>"},{"location":"user-guide/cli-usage/#examples","title":"Examples","text":""},{"location":"user-guide/cli-usage/#scraping-1819-news","title":"Scraping 1819 News","text":"<pre><code>python -m opal \\\n    --url https://1819news.com/ \\\n    --parser Parser1819 \\\n    --suffix /news/item \\\n    --max_pages 10 \\\n    --output 1819_articles.json\n</code></pre>"},{"location":"user-guide/cli-usage/#scraping-alabama-daily-news","title":"Scraping Alabama Daily News","text":"<pre><code>python -m opal \\\n    --url https://www.aldailynews.com/ \\\n    --parser ParserDailyNews \\\n    --suffix /news/item \\\n    --max_pages 5\n</code></pre>"},{"location":"user-guide/cli-usage/#scraping-court-records-parserappealsal","title":"Scraping Court Records (ParserAppealsAL)","text":"<p>Basic court case extraction:</p> <pre><code>python -m opal \\\n    --url https://publicportal.alappeals.gov/portal/search/case/results \\\n    --parser ParserAppealsAL \\\n    --output court_cases.json\n</code></pre> <p>Note: For advanced court extraction with search filters, use the Configurable Court Extractor instead.</p>"},{"location":"user-guide/cli-usage/#parserappealsal-cli-guide","title":"ParserAppealsAL CLI Guide","text":"<p>The ParserAppealsAL parser (<code>--parser ParserAppealsAL</code>) is specifically designed for extracting case data from the Alabama Appeals Court Public Portal.</p>"},{"location":"user-guide/cli-usage/#basic-usage","title":"Basic Usage","text":"<pre><code>python -m opal --url &lt;COURT_URL&gt; --parser ParserAppealsAL [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli-usage/#supported-urls","title":"Supported URLs","text":"<p>The parser works with Alabama Appeals Court URLs:</p> <ul> <li>Search Results: <code>https://publicportal.alappeals.gov/portal/search/case/results?criteria=...</code></li> <li>Direct Case Pages: Individual case detail pages from the portal</li> </ul>"},{"location":"user-guide/cli-usage/#command-options","title":"Command Options","text":"Option Description Default Example <code>--url</code> Court portal URL Required See examples below <code>--parser</code> Must be <code>ParserAppealsAL</code> Required <code>ParserAppealsAL</code> <code>--output</code> Output JSON file <code>opal_output.json</code> <code>appeals_cases.json</code> <code>--max_pages</code> Max pages to process <code>5</code> <code>10</code> <code>--log-level</code> Logging verbosity <code>INFO</code> <code>DEBUG</code>"},{"location":"user-guide/cli-usage/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/cli-usage/#extract-from-search-results","title":"Extract from Search Results","text":"<pre><code># Extract court cases from a search results URL\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\" \\\n    --parser ParserAppealsAL \\\n    --output alabama_appeals.json \\\n    --log-level INFO\n</code></pre>"},{"location":"user-guide/cli-usage/#extract-with-custom-output","title":"Extract with Custom Output","text":"<pre><code># Save to specific file with detailed logging\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser ParserAppealsAL \\\n    --output civil_appeals_2024.json \\\n    --log-level DEBUG\n</code></pre>"},{"location":"user-guide/cli-usage/#limit-page-processing","title":"Limit Page Processing","text":"<pre><code># Process only first 3 pages of results\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser ParserAppealsAL \\\n    --max_pages 3 \\\n    --output limited_results.json\n</code></pre>"},{"location":"user-guide/cli-usage/#what-gets-extracted","title":"What Gets Extracted","text":"<p>The ParserAppealsAL extracts the following information for each case:</p> <ul> <li>Case Number: Full case identifier and detail link</li> <li>Case Title: Complete case title/name</li> <li>Court: Which Appeals Court (Civil, Criminal, Supreme)</li> <li>Classification: Case type (Appeal, Certiorari, etc.)</li> <li>Filing Date: When the case was filed</li> <li>Status: Current case status (Open, Closed, etc.)</li> </ul>"},{"location":"user-guide/cli-usage/#output-format","title":"Output Format","text":"<pre><code>{\n  \"cases\": [\n    {\n      \"court\": \"Alabama Civil Court of Appeals\",\n      \"case_number\": {\n        \"text\": \"CL-2024-0123\",\n        \"link\": \"/portal/case/detail/12345\"\n      },\n      \"case_title\": \"Smith v. Jones Corp.\",\n      \"classification\": \"Appeal\",\n      \"filed_date\": \"01/15/2024\",\n      \"status\": \"Open\"\n    }\n  ],\n  \"metadata\": {\n    \"extraction_date\": \"2024-01-20\",\n    \"extraction_time\": \"14:30:22\",\n    \"total_cases\": 25,\n    \"pages_processed\": 3,\n    \"parser\": \"ParserAppealsAL\"\n  }\n}\n</code></pre>"},{"location":"user-guide/cli-usage/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Specific URLs: Direct search result URLs work better than general portal URLs</li> <li>Limit Pages: Use <code>--max_pages</code> for faster processing of large result sets</li> <li>Enable Debug Logging: Use <code>--log-level DEBUG</code> to monitor progress</li> <li>Check Output: Verify results in the generated JSON file</li> </ol>"},{"location":"user-guide/cli-usage/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/cli-usage/#issue-no-cases-found","title":"Issue: \"No cases found\"","text":"<p>Solution: Ensure the URL contains actual search results with cases displayed</p>"},{"location":"user-guide/cli-usage/#issue-driver-errors","title":"Issue: \"Driver errors\"","text":"<p>Solution: Make sure Chrome browser is installed and updated</p>"},{"location":"user-guide/cli-usage/#issue-slow-processing","title":"Issue: \"Slow processing\"","text":"<p>Solution: Use <code>--max_pages</code> to limit the number of pages processed</p>"},{"location":"user-guide/cli-usage/#advanced-usage","title":"Advanced Usage","text":"<p>For more advanced court case extraction with search parameters, date filtering, and case categorization, use the dedicated Configurable Court Extractor:</p> <pre><code># Advanced extraction with search filters\npython -m opal.configurable_court_extractor \\\n    --court civil \\\n    --date-period 1m \\\n    --case-category Appeal \\\n    --max-pages 10\n</code></pre>"},{"location":"user-guide/cli-usage/#integration-with-other-tools","title":"Integration with Other Tools","text":""},{"location":"user-guide/cli-usage/#processing-results-with-jq","title":"Processing Results with jq","text":"<pre><code># Extract just case numbers\npython -m opal --url \"...\" --parser ParserAppealsAL --output cases.json\ncat cases.json | jq '.cases[].case_number.text'\n\n# Count cases by status\ncat cases.json | jq '.cases | group_by(.status) | map({status: .[0].status, count: length})'\n</code></pre>"},{"location":"user-guide/cli-usage/#converting-to-csv","title":"Converting to CSV","text":"<pre><code># Use the configurable extractor for direct CSV output\npython -m opal.configurable_court_extractor --court civil --format csv\n</code></pre>"},{"location":"user-guide/cli-usage/#output","title":"Output","text":"<p>All scraped data is saved in JSON format with the following structure:</p> <pre><code>{\n  \"results\": [\n    {\n      \"title\": \"Article Title\",\n      \"content\": \"Article content...\",\n      \"date\": \"2024-01-01\",\n      \"url\": \"https://example.com/article\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"Parser1819\",\n    \"scraped_at\": \"2024-01-01T12:00:00\",\n    \"total_items\": 25\n  }\n}\n</code></pre>"},{"location":"user-guide/command_line_tools/","title":"Command Line Tools","text":"<p>OPAL provides several command-line tools for extracting court and news data. This document covers all available CLI interfaces.</p>"},{"location":"user-guide/command_line_tools/#main-opal-cli","title":"Main OPAL CLI","text":"<p>The primary command-line interface for OPAL:</p> <pre><code>python -m opal [options]\n</code></pre>"},{"location":"user-guide/command_line_tools/#basic-usage","title":"Basic Usage","text":"<pre><code># Extract news articles\npython -m opal --url https://alabamanewscenter.com --parser Parser1819 --suffix \"/news/\"\n\n# Extract court cases (uses ParserAppealsAL automatically)\npython -m opal --url https://publicportal.alappeals.gov --parser court\n</code></pre>"},{"location":"user-guide/command_line_tools/#available-options","title":"Available Options","text":"Option Required Description Default <code>--url</code> Yes Base URL to scrape - <code>--parser</code> Yes Parser type (Parser1819, ParserDailyNews, court) - <code>--suffix</code> No URL suffix for news articles '' <code>--max_pages</code> No Maximum pages to scrape 5 <code>--output</code> No Output file path opal_output.json <code>--log-level</code> No Logging level (DEBUG, INFO, WARNING, ERROR) INFO"},{"location":"user-guide/command_line_tools/#configurable-court-extractor-cli","title":"Configurable Court Extractor CLI","text":"<p>Advanced court data extraction with flexible search parameters:</p> <pre><code>python -m opal.configurable_court_extractor [options]\n</code></pre>"},{"location":"user-guide/command_line_tools/#quick-examples","title":"Quick Examples","text":"<pre><code># Extract civil court cases from last 7 days\npython -m opal.configurable_court_extractor --court civil --date-period 7d\n\n# Extract criminal appeals with custom date range\npython -m opal.configurable_court_extractor \\\n    --court criminal \\\n    --date-period custom \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-31 \\\n    --case-category Appeal\n\n# Extract supreme court cases excluding closed ones\npython -m opal.configurable_court_extractor \\\n    --court supreme \\\n    --date-period 3m \\\n    --exclude-closed \\\n    --max-pages 20\n</code></pre>"},{"location":"user-guide/command_line_tools/#all-available-options","title":"All Available Options","text":""},{"location":"user-guide/command_line_tools/#court-selection","title":"Court Selection","text":"<ul> <li><code>--court {civil,criminal,supreme}</code> - Court type to search</li> </ul>"},{"location":"user-guide/command_line_tools/#date-filtering","title":"Date Filtering","text":"<ul> <li><code>--date-period {7d,1m,3m,6m,1y,custom}</code> - Predefined date periods</li> <li><code>--start-date YYYY-MM-DD</code> - Start date (required with custom period)</li> <li><code>--end-date YYYY-MM-DD</code> - End date (required with custom period)</li> </ul>"},{"location":"user-guide/command_line_tools/#case-filtering","title":"Case Filtering","text":"<ul> <li><code>--case-number TEXT</code> - Filter by case number (supports wildcards)</li> <li><code>--case-title TEXT</code> - Filter by case title</li> <li><code>--case-category {Appeal,Certiorari,Original Proceeding,Petition,Certified Question}</code> - Filter by case type</li> <li><code>--exclude-closed</code> - Exclude closed cases</li> </ul>"},{"location":"user-guide/command_line_tools/#processing-options","title":"Processing Options","text":"<ul> <li><code>--max-pages INT</code> - Maximum pages to process (default: unlimited)</li> <li><code>--rate-limit FLOAT</code> - Seconds between requests (default: 1.0)</li> <li><code>--headless</code> / <code>--no-headless</code> - Browser visibility (default: headless)</li> </ul>"},{"location":"user-guide/command_line_tools/#output-options","title":"Output Options","text":"<ul> <li><code>--output-prefix TEXT</code> - Prefix for output files (default: court_cases)</li> <li><code>--format {json,csv,both}</code> - Output format (default: both)</li> </ul>"},{"location":"user-guide/command_line_tools/#advanced-options","title":"Advanced Options","text":"<ul> <li><code>--url TEXT</code> - Use custom URL instead of building search</li> <li><code>--court-id INT</code> - Override automatic court ID discovery</li> <li><code>--verbose</code> - Enable verbose logging</li> </ul>"},{"location":"user-guide/command_line_tools/#complete-example","title":"Complete Example","text":"<pre><code>python -m opal.configurable_court_extractor \\\n    --court civil \\\n    --date-period custom \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-03-31 \\\n    --case-category Appeal \\\n    --case-number \"2024-*\" \\\n    --exclude-closed \\\n    --max-pages 50 \\\n    --rate-limit 2.0 \\\n    --output-prefix \"civil_appeals_q1_2024\" \\\n    --format both \\\n    --verbose\n</code></pre> <p>This command: - Searches civil court - For cases filed January-March 2024 - Only appeals - With case numbers starting with \"2024-\" - Excluding closed cases - Processing up to 50 pages - With 2-second delays between requests - Outputting both JSON and CSV - With verbose logging</p>"},{"location":"user-guide/command_line_tools/#extract-all-court-cases-script","title":"Extract All Court Cases Script","text":"<p>Standalone script for extracting all available court cases:</p> <pre><code>python -m opal.extract_all_court_cases\n</code></pre>"},{"location":"user-guide/command_line_tools/#features","title":"Features","text":"<ul> <li>Extracts all court cases across all pages</li> <li>Hardcoded to handle known pagination (318 cases across 13 pages)</li> <li>Outputs both JSON and CSV formats</li> <li>Includes progress tracking</li> <li>Automatically verifies extraction completeness</li> <li>Uses ParserAppealsAL internally</li> </ul>"},{"location":"user-guide/command_line_tools/#output-files","title":"Output Files","text":"<ul> <li><code>all_court_cases_TIMESTAMP.json</code> - JSON format with metadata</li> <li><code>all_court_cases_TIMESTAMP.csv</code> - CSV format for analysis</li> </ul>"},{"location":"user-guide/command_line_tools/#usage","title":"Usage","text":"<pre><code># Extract all cases\npython -m opal.extract_all_court_cases\n\n# The script is self-contained and requires no parameters\n# Rate limiting and pagination are handled automatically\n</code></pre>"},{"location":"user-guide/command_line_tools/#integration-with-other-tools","title":"Integration with Other Tools","text":""},{"location":"user-guide/command_line_tools/#using-with-jq-json-processing","title":"Using with jq (JSON processing)","text":"<pre><code># Extract specific fields from court results\npython -m opal.configurable_court_extractor --court civil --date-period 1m | \\\n    jq '.cases[] | {case_number: .case_number.text, title: .case_title}'\n\n# Count cases by status\npython -m opal.configurable_court_extractor --court civil --date-period 1m | \\\n    jq '.cases | group_by(.status) | map({status: .[0].status, count: length})'\n</code></pre>"},{"location":"user-guide/command_line_tools/#using-with-csvkit-csv-processing","title":"Using with csvkit (CSV processing)","text":"<pre><code># Generate CSV and analyze\npython -m opal.configurable_court_extractor --court civil --format csv\ncsvstat court_cases.csv --count\ncsvcut -c case_title,status court_cases.csv | csvlook\n</code></pre>"},{"location":"user-guide/command_line_tools/#piping-to-other-commands","title":"Piping to Other Commands","text":"<pre><code># Count total cases\npython -m opal.configurable_court_extractor --court civil --format json | \\\n    jq '.total_cases'\n\n# Extract case numbers only\npython -m opal.configurable_court_extractor --court civil --format json | \\\n    jq -r '.cases[].case_number.text'\n</code></pre>"},{"location":"user-guide/command_line_tools/#automation-and-scripting","title":"Automation and Scripting","text":""},{"location":"user-guide/command_line_tools/#bash-script-example","title":"Bash Script Example","text":"<pre><code>#!/bin/bash\n# extract_monthly_reports.sh\n\nCOURTS=(\"civil\" \"criminal\" \"supreme\")\nDATE=$(date +%Y-%m)\n\nfor court in \"${COURTS[@]}\"; do\n    echo \"Extracting $court court cases for $DATE\"\n\n    python -m opal.configurable_court_extractor \\\n        --court \"$court\" \\\n        --date-period 1m \\\n        --output-prefix \"${court}_${DATE}\" \\\n        --format both \\\n        --exclude-closed\n\n    echo \"Completed $court extraction\"\ndone\n\necho \"All extractions complete\"\n</code></pre>"},{"location":"user-guide/command_line_tools/#python-script-integration","title":"Python Script Integration","text":"<pre><code>#!/usr/bin/env python3\nimport subprocess\nimport json\nfrom datetime import datetime\n\ndef extract_court_data(court_type, date_period=\"1m\"):\n    \"\"\"Extract court data using CLI tool\"\"\"\n    cmd = [\n        \"python\", \"-m\", \"opal.configurable_court_extractor\",\n        \"--court\", court_type,\n        \"--date-period\", date_period\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode == 0:\n        # CLI saves to files, check for output files\n        timestamp = datetime.now().strftime('%Y%m%d')\n        json_file = f\"court_cases_{timestamp}*.json\"\n        # Read the most recent output file\n        import glob\n        files = glob.glob(json_file)\n        if files:\n            with open(files[-1]) as f:\n                return json.load(f)\n    else:\n        print(f\"Error extracting {court_type}: {result.stderr}\")\n        return None\n\n# Direct import approach (preferred)\nfrom opal.configurable_court_extractor import extract_court_cases_with_params\n\ndef extract_court_data_direct(court_type, date_period=\"1m\"):\n    \"\"\"Extract court data using direct function call\"\"\"\n    return extract_court_cases_with_params(\n        court=court_type,\n        date_period=date_period,\n        max_pages=5\n    )\n\n# Usage\nfor court in [\"civil\", \"criminal\", \"supreme\"]:\n    data = extract_court_data_direct(court)\n    if data and data['status'] == 'success':\n        print(f\"{court}: {data['total_cases']} cases found\")\n</code></pre>"},{"location":"user-guide/command_line_tools/#environment-variables","title":"Environment Variables","text":"<p>Control CLI behavior with environment variables:</p> <pre><code># Set default output directory\nexport OPAL_OUTPUT_DIR=\"/path/to/outputs\"\n\n# Set default rate limiting\nexport OPAL_RATE_LIMIT=2.0\n\n# Enable debug mode\nexport OPAL_DEBUG=1\n\n# Set Chrome options\nexport OPAL_CHROME_OPTIONS=\"--proxy-server=proxy.example.com:8080\"\n</code></pre>"},{"location":"user-guide/command_line_tools/#error-handling-and-debugging","title":"Error Handling and Debugging","text":""},{"location":"user-guide/command_line_tools/#verbose-output","title":"Verbose Output","text":"<p>Enable detailed logging:</p> <pre><code>python -m opal.configurable_court_extractor \\\n    --court civil \\\n    --date-period 7d \\\n    --verbose\n</code></pre>"},{"location":"user-guide/command_line_tools/#debug-mode","title":"Debug Mode","text":"<p>Run with debug logging:</p> <pre><code>python -m opal.configurable_court_extractor \\\n    --court civil \\\n    --date-period 7d \\\n    --log-level DEBUG\n</code></pre>"},{"location":"user-guide/command_line_tools/#non-headless-mode","title":"Non-Headless Mode","text":"<p>See browser activity:</p> <pre><code>python -m opal.configurable_court_extractor \\\n    --court civil \\\n    --date-period 7d \\\n    --no-headless\n</code></pre>"},{"location":"user-guide/command_line_tools/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/command_line_tools/#optimize-for-speed","title":"Optimize for Speed","text":"<pre><code># Use higher rate limiting for faster extraction\npython -m opal.configurable_court_extractor \\\n    --court civil \\\n    --rate-limit 0.5 \\\n    --max-pages 10\n</code></pre>"},{"location":"user-guide/command_line_tools/#optimize-for-server-resources","title":"Optimize for Server Resources","text":"<pre><code># Be gentle with the server\npython -m opal.configurable_court_extractor \\\n    --court civil \\\n    --rate-limit 3.0 \\\n    --max-pages 5\n</code></pre>"},{"location":"user-guide/command_line_tools/#exit-codes","title":"Exit Codes","text":"<p>The CLI tools use standard exit codes:</p> <ul> <li><code>0</code> - Success</li> <li><code>1</code> - General error</li> <li><code>2</code> - Invalid arguments</li> <li><code>3</code> - Network error</li> <li><code>4</code> - Parser error</li> <li><code>5</code> - Output error</li> </ul>"},{"location":"user-guide/command_line_tools/#checking-exit-codes","title":"Checking Exit Codes","text":"<pre><code>python -m opal.configurable_court_extractor --court civil --date-period 7d\nif [ $? -eq 0 ]; then\n    echo \"Extraction successful\"\nelse\n    echo \"Extraction failed with code $?\"\nfi\n</code></pre>"},{"location":"user-guide/command_line_tools/#help-and-documentation","title":"Help and Documentation","text":"<p>Get help for any command:</p> <pre><code># Main OPAL help\npython -m opal --help\n\n# Configurable extractor help\npython -m opal.configurable_court_extractor --help\n\n# Get version information\npython -m opal --version\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/","title":"Configurable Court Extractor","text":"<p>The Configurable Court Extractor (<code>opal.configurable_court_extractor</code>) provides advanced searching and filtering capabilities for the Alabama Appeals Court portal. It includes both programmatic and command-line interfaces for extracting court data with precise control over search parameters.</p>"},{"location":"user-guide/configurable_court_extractor/#overview","title":"Overview","text":"<p>The configurable court extractor consists of:</p> <ul> <li><code>CourtSearchBuilder</code> class for building complex search parameters</li> <li><code>extract_court_cases_with_params()</code> function for programmatic extraction</li> <li>Command-line interface for terminal-based extraction</li> </ul> <p>It supports: - Dynamic court selection (Civil, Criminal, Supreme) - Advanced search filtering - Custom URL support for pre-built searches - Multiple output formats (JSON, CSV)</p>"},{"location":"user-guide/configurable_court_extractor/#classes-and-functions","title":"Classes and Functions","text":""},{"location":"user-guide/configurable_court_extractor/#courtsearchbuilder","title":"CourtSearchBuilder","text":"<p>A builder class for constructing Alabama Court search URLs with court-specific parameters.</p> <pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\n\n# Create search builder\nbuilder = CourtSearchBuilder()\n\n# Configure search parameters\nbuilder.set_court(\"civil\")\nbuilder.set_date_range(period=\"1m\")  # Last month\nbuilder.set_case_category(\"Appeal\")\nbuilder.set_exclude_closed(True)\n\n# Build the search URL\nsearch_url = builder.build_url(page_number=0)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#available-methods","title":"Available Methods","text":"<p>Court Configuration: - <code>set_court(court_key)</code> - Set court type ('civil', 'criminal', 'supreme') - <code>get_court_info()</code> - Get information about current court - <code>discover_court_ids(parser_instance)</code> - Auto-discover court IDs from website - <code>set_court_id_manually(court_key, court_id)</code> - Manually set court ID</p> <p>Date Filtering: - <code>set_date_range(start_date=None, end_date=None, period='1y')</code> - Set date filter   - Periods: '7d', '1m', '3m', '6m', '1y', 'custom'   - Custom requires start_date and end_date</p> <p>Case Filtering: - <code>set_case_category(category_name=None)</code> - Filter by case type   - Categories: 'Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question' - <code>set_case_number_filter(case_number=None)</code> - Filter by case number - <code>set_case_title_filter(title=None)</code> - Filter by case title - <code>set_exclude_closed(exclude=False)</code> - Exclude closed cases</p> <p>URL Building: - <code>build_url(page_number=0)</code> - Build complete search URL - <code>build_criteria_string()</code> - Build URL criteria parameters</p>"},{"location":"user-guide/configurable_court_extractor/#extract_court_cases_with_params","title":"extract_court_cases_with_params()","text":"<p>Main extraction function that supports both parameter-based and URL-based searches.</p> <pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\n# Parameter-based search\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='1m',\n    case_category='Appeal',\n    exclude_closed=True,\n    max_pages=5,\n    output_prefix=\"civil_appeals\"\n)\n\n# Custom URL search\nresults = extract_court_cases_with_params(\n    custom_url=\"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\",\n    max_pages=5,\n    output_prefix=\"custom_search\"\n)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#parameters","title":"Parameters","text":"<p>Search Parameters (ignored if custom_url provided): - <code>court</code> (str): Court type ('civil', 'criminal', 'supreme') - <code>date_period</code> (str): Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - <code>start_date</code> (str): Start date for custom range (YYYY-MM-DD) - <code>end_date</code> (str): End date for custom range (YYYY-MM-DD) - <code>case_number</code> (str): Case number filter (partial match) - <code>case_title</code> (str): Case title filter (partial match) - <code>case_category</code> (str): Case category filter - <code>exclude_closed</code> (bool): Whether to exclude closed cases</p> <p>Processing Options: - <code>max_pages</code> (int): Maximum pages to process (None for all) - <code>output_prefix</code> (str): Prefix for output files - <code>custom_url</code> (str): Pre-built search URL (overrides all search params)</p>"},{"location":"user-guide/configurable_court_extractor/#court-definitions","title":"Court Definitions","text":"<p>The system supports three Alabama Appeals Courts:</p> <pre><code>courts = {\n    'civil': {\n        'name': 'Alabama Civil Court of Appeals',\n        'case_prefix': 'CL',\n        'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n    },\n    'criminal': {\n        'name': 'Alabama Court of Criminal Appeals', \n        'case_prefix': 'CR',\n        'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n    },\n    'supreme': {\n        'name': 'Alabama Supreme Court',\n        'case_prefix': 'SC',\n        'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n    }\n}\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#command-line-interface","title":"Command-Line Interface","text":""},{"location":"user-guide/configurable_court_extractor/#basic-usage","title":"Basic Usage","text":"<pre><code># Extract civil court cases from last month\npython -m opal.configurable_court_extractor --court civil --date-period 1m\n\n# Extract criminal appeals with custom date range\npython -m opal.configurable_court_extractor \\\n    --court criminal \\\n    --date-period custom \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-31 \\\n    --case-category Appeal\n\n# Use custom URL\npython -m opal.configurable_court_extractor \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#cli-options","title":"CLI Options","text":"<p>URL Option (overrides all search parameters): - <code>--url</code> - Pre-built search URL with embedded parameters</p> <p>Search Parameters (ignored if --url provided): - <code>--court {civil,criminal,supreme}</code> - Court to search (default: civil) - <code>--date-period {7d,1m,3m,6m,1y,custom}</code> - Date period (default: 1y) - <code>--start-date YYYY-MM-DD</code> - Start date for custom range - <code>--end-date YYYY-MM-DD</code> - End date for custom range - <code>--case-number TEXT</code> - Case number filter - <code>--case-title TEXT</code> - Case title filter - <code>--case-category {Appeal,Certiorari,Original Proceeding,Petition,Certified Question}</code> - Case category - <code>--exclude-closed</code> - Exclude closed cases</p> <p>Output Options: - <code>--max-pages INT</code> - Maximum pages to process - <code>--output-prefix TEXT</code> - Prefix for output files (default: court_cases)</p>"},{"location":"user-guide/configurable_court_extractor/#programmatic-usage-examples","title":"Programmatic Usage Examples","text":""},{"location":"user-guide/configurable_court_extractor/#basic-search","title":"Basic Search","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder, extract_court_cases_with_params\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Simple extraction\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='7d',\n    exclude_closed=True\n)\n\nif results and results['status'] == 'success':\n    print(f\"Found {results['total_cases']} cases\")\n    for case in results['cases']:\n        print(f\"- {case['case_number']['text']}: {case['case_title']}\")\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#advanced-search-with-builder","title":"Advanced Search with Builder","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Create builder and parser\nbuilder = CourtSearchBuilder()\nparser = ParserAppealsAL(headless=True)\n\n# Discover court IDs\nbuilder.discover_court_ids(parser)\n\n# Configure search\nbuilder.set_court('supreme')\nbuilder.set_date_range(start_date='2024-01-01', end_date='2024-03-31', period='custom')\nbuilder.set_case_category('Certiorari')\nbuilder.set_exclude_closed(True)\n\n# Get search URL and extract\nsearch_url = builder.build_url()\nresults = extract_court_cases_with_params(custom_url=search_url)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#processing-custom-urls","title":"Processing Custom URLs","text":"<pre><code># Handle session-based URLs from website\ncustom_url = \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n\nresults = extract_court_cases_with_params(\n    custom_url=custom_url,\n    max_pages=10,\n    output_prefix=\"session_search\"\n)\n\n# The function will warn about session expiration\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#court-id-discovery","title":"Court ID Discovery","text":"<p>The system automatically discovers court IDs from the website:</p> <pre><code>builder = CourtSearchBuilder()\nparser = ParserAppealsAL()\n\n# Auto-discover court IDs\nbuilder.discover_court_ids(parser)\n\n# Check discovery results\nfor court_key, court_info in builder.courts.items():\n    print(f\"{court_info['name']}: {court_info['id']}\")\n\n# Manual override if needed\nbuilder.set_court_id_manually('civil', '68f021c4-6a44-4735-9a76-5360b2e8af13')\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#output-format","title":"Output Format","text":""},{"location":"user-guide/configurable_court_extractor/#json-output-structure","title":"JSON Output Structure","text":"<pre><code>{\n    \"status\": \"success\",\n    \"search_parameters\": {\n        \"court\": \"civil\",\n        \"date_period\": \"1m\",\n        \"case_category\": \"Appeal\",\n        \"exclude_closed\": true\n    },\n    \"total_cases\": 25,\n    \"extraction_date\": \"2024-01-15\",\n    \"extraction_time\": \"14:30:22\",\n    \"pages_processed\": 2,\n    \"cases\": [\n        {\n            \"court\": \"Alabama Civil Court of Appeals\",\n            \"case_number\": {\n                \"text\": \"CL-2024-0123\",\n                \"link\": \"/portal/case/detail/12345\"\n            },\n            \"case_title\": \"Smith v. Jones\",\n            \"classification\": \"Appeal\",\n            \"filed_date\": \"01/10/2024\",\n            \"status\": \"Open\"\n        }\n    ]\n}\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#csv-output","title":"CSV Output","text":"<p>Automatically generated alongside JSON: - Court, Case Number, Case Title, Classification, Filed Date, Status, Case Link</p>"},{"location":"user-guide/configurable_court_extractor/#error-handling-and-warnings","title":"Error Handling and Warnings","text":""},{"location":"user-guide/configurable_court_extractor/#session-url-warnings","title":"Session URL Warnings","text":"<p>When using custom URLs:</p> <pre><code>\u26a0\ufe0f  WARNING: Custom URLs contain session-specific parameters that expire.\n   This URL will only work temporarily and may become invalid after your browser session ends.\n   For reliable, repeatable searches, use the CLI search parameters instead of --url option.\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#court-id-discovery-failures","title":"Court ID Discovery Failures","text":"<pre><code># Graceful fallback to known IDs\nif court_info['id'] is None:\n    raise ValueError(f\"Could not discover court ID for {court_name}. \"\n                   \"Try using the --url option with a pre-built search URL instead.\")\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#integration-with-other-components","title":"Integration with Other Components","text":""},{"location":"user-guide/configurable_court_extractor/#with-parserappealsal","title":"With ParserAppealsAL","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\n\n# The extractor uses ParserAppealsAL internally\nparser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\nbuilder = CourtSearchBuilder()\n\n# Discovery requires parser instance\nbuilder.discover_court_ids(parser)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#with-court-url-paginator","title":"With Court URL Paginator","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nfrom opal.court_url_paginator import parse_court_url\n\n# Extract cases and check pagination\nresults = extract_court_cases_with_params(court='civil', date_period='1m')\n\n# The function internally handles pagination automatically\nprint(f\"Processed {results['pages_processed']} pages\")\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Court ID Discovery: Requires web scraping, cached after first discovery</li> <li>Rate Limiting: Built-in 2-second delays between requests</li> <li>Memory Usage: Processes pages sequentially to manage memory</li> <li>Session Management: Custom URLs expire, use parameters for reliability</li> </ul>"},{"location":"user-guide/configurable_court_extractor/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configurable_court_extractor/#common-issues","title":"Common Issues","text":"<ol> <li>Court ID Discovery Fails</li> <li>Use <code>--url</code> option with pre-built search URL</li> <li> <p>Manually set court IDs with <code>set_court_id_manually()</code></p> </li> <li> <p>Custom URL Stops Working</p> </li> <li>URLs are session-based and expire</li> <li>Switch to parameter-based search</li> <li> <p>Create new search on website to get fresh URL</p> </li> <li> <p>No Results Found</p> </li> <li>Check date range (default is last year)</li> <li>Verify court has cases in date range</li> <li>Try broader search criteria</li> </ol>"},{"location":"user-guide/configurable_court_extractor/#debug-mode","title":"Debug Mode","text":"<pre><code># Run with headless=False to see browser\n# Modify the script to set headless=False in ParserAppealsAL\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\nfrom opal.configurable_court_extractor import extract_court_cases_with_params\n\ndef main():\n    # Extract recent civil appeals\n    results = extract_court_cases_with_params(\n        court='civil',\n        date_period='1m',\n        case_category='Appeal',\n        exclude_closed=True,\n        max_pages=5,\n        output_prefix='civil_appeals_recent'\n    )\n\n    if results and results['status'] == 'success':\n        print(f\"\u2713 Extracted {results['total_cases']} cases\")\n        print(f\"\u2713 Processed {results['pages_processed']} pages\")\n\n        # Show sample cases\n        for case in results['cases'][:3]:\n            print(f\"- {case['case_number']['text']}: {case['case_title']}\")\n    else:\n        print(\"\u274c Extraction failed\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This extractor provides a powerful interface for accessing Alabama Appeals Court data with flexible search capabilities and robust error handling.</p>"},{"location":"user-guide/output-formats/","title":"Output Formats","text":"<p>OPAL outputs scraped data in structured JSON format for easy analysis and processing.</p>"},{"location":"user-guide/output-formats/#json-structure","title":"JSON Structure","text":""},{"location":"user-guide/output-formats/#news-articles","title":"News Articles","text":"<pre><code>{\n  \"results\": [\n    {\n      \"title\": \"Article headline\",\n      \"content\": \"Full article text...\",\n      \"date\": \"2024-01-15\",\n      \"author\": \"John Doe\",\n      \"url\": \"https://example.com/article-url\",\n      \"tags\": [\"politics\", \"alabama\"],\n      \"image_url\": \"https://example.com/image.jpg\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"Parser1819\",\n    \"base_url\": \"https://1819news.com/\",\n    \"scraped_at\": \"2024-01-15T10:30:00Z\",\n    \"total_items\": 50,\n    \"pages_scraped\": 5\n  }\n}\n</code></pre>"},{"location":"user-guide/output-formats/#court-cases","title":"Court Cases","text":"<pre><code>{\n  \"results\": [\n    {\n      \"case_number\": \"2024-CV-001234\",\n      \"case_title\": \"State v. Defendant\",\n      \"court\": \"Alabama Court of Civil Appeals\",\n      \"date_filed\": \"2024-01-10\",\n      \"status\": \"Active\",\n      \"parties\": {\n        \"plaintiff\": \"State of Alabama\",\n        \"defendant\": \"John Doe\"\n      },\n      \"docket_entries\": [\n        {\n          \"date\": \"2024-01-10\",\n          \"description\": \"Case filed\",\n          \"document_url\": \"https://example.com/doc.pdf\"\n        }\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"ParserAppealsAL\",\n    \"scraped_at\": \"2024-01-15T10:30:00Z\",\n    \"total_cases\": 25\n  }\n}\n</code></pre>"},{"location":"user-guide/output-formats/#working-with-output","title":"Working with Output","text":""},{"location":"user-guide/output-formats/#python-example","title":"Python Example","text":"<pre><code>import json\n\n# Load scraped data\nwith open('opal_output.json', 'r') as f:\n    data = json.load(f)\n\n# Access articles\nfor article in data['results']:\n    print(f\"Title: {article['title']}\")\n    print(f\"Date: {article['date']}\")\n    print(f\"URL: {article['url']}\")\n    print(\"---\")\n\n# Get metadata\nprint(f\"Total items: {data['metadata']['total_items']}\")\n</code></pre>"},{"location":"user-guide/output-formats/#data-analysis","title":"Data Analysis","text":"<p>The JSON output can be easily imported into: - Pandas DataFrames for analysis - Excel/CSV for spreadsheet work - Database systems for storage - Visualization tools for insights</p>"},{"location":"user-guide/output-formats/#error-handling","title":"Error Handling","text":"<p>Failed scrapes include error information:</p> <pre><code>{\n  \"results\": [],\n  \"metadata\": {\n    \"source\": \"Parser1819\",\n    \"error\": \"Connection timeout\",\n    \"scraped_at\": \"2024-01-15T10:30:00Z\"\n  }\n}\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/","title":"ParserAppealsAL CLI Reference","text":"<p>Complete command-line reference for the Alabama Appeals Court parser.</p>"},{"location":"user-guide/parser-appeals-al-cli/#overview","title":"Overview","text":"<p>ParserAppealsAL is the specialized parser for extracting case data from the Alabama Appeals Court Public Portal. It's accessed through the main OPAL CLI using <code>--parser ParserAppealsAL</code>.</p>"},{"location":"user-guide/parser-appeals-al-cli/#quick-start","title":"Quick Start","text":"<pre><code># Basic court case extraction\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser ParserAppealsAL \\\n    --output my_cases.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#command-syntax","title":"Command Syntax","text":"<pre><code>python -m opal --url &lt;COURT_URL&gt; --parser ParserAppealsAL [OPTIONS]\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#required-arguments","title":"Required Arguments","text":""},{"location":"user-guide/parser-appeals-al-cli/#-url-url","title":"<code>--url &lt;URL&gt;</code>","text":"<p>The Alabama Appeals Court portal URL to scrape.</p> <p>Supported URL Types: - Search results pages with case listings - Individual case detail pages - Paginated search results</p> <p>Examples: <pre><code>--url \"https://publicportal.alappeals.gov/portal/search/case/results\"\n--url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#-parser-parserappealsal","title":"<code>--parser ParserAppealsAL</code>","text":"<p>Specifies to use the ParserAppealsAL court parser.</p> <p>Must be exactly: <code>ParserAppealsAL</code></p>"},{"location":"user-guide/parser-appeals-al-cli/#optional-arguments","title":"Optional Arguments","text":""},{"location":"user-guide/parser-appeals-al-cli/#-output-filename","title":"<code>--output &lt;FILENAME&gt;</code>","text":"<p>Specify the output JSON file path.</p> <p>Default: <code>opal_output.json</code></p> <p>Examples: <pre><code>--output court_cases.json\n--output /path/to/appeals_data.json\n--output \"cases_$(date +%Y%m%d).json\"\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#-max_pages-number","title":"<code>--max_pages &lt;NUMBER&gt;</code>","text":"<p>Limit the number of pages to process from paginated results.</p> <p>Default: <code>5</code> Range: 1 or higher</p> <p>Examples: <pre><code>--max_pages 1      # Process only first page\n--max_pages 10     # Process up to 10 pages\n--max_pages 50     # Process up to 50 pages\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#-log-level-level","title":"<code>--log-level &lt;LEVEL&gt;</code>","text":"<p>Set the logging verbosity level.</p> <p>Default: <code>INFO</code> Options: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code></p> <p>Examples: <pre><code>--log-level DEBUG    # Detailed progress information\n--log-level INFO     # Standard progress updates\n--log-level WARNING  # Only warnings and errors\n--log-level ERROR    # Only error messages\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#complete-examples","title":"Complete Examples","text":""},{"location":"user-guide/parser-appeals-al-cli/#basic-extraction","title":"Basic Extraction","text":"<pre><code># Extract court cases with default settings\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser ParserAppealsAL\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#custom-output-file","title":"Custom Output File","text":"<pre><code># Save to specific file\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --output alabama_appeals_2024.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#limited-page-processing","title":"Limited Page Processing","text":"<pre><code># Process only first 3 pages for faster results\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --max_pages 3 \\\n    --output quick_sample.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable detailed logging to monitor progress\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --log-level DEBUG \\\n    --output debug_cases.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#production-extraction","title":"Production Extraction","text":"<pre><code># Extract with timestamp in filename\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --max_pages 20 \\\n    --output \"court_cases_$(date +%Y%m%d_%H%M%S).json\" \\\n    --log-level INFO\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#output-structure","title":"Output Structure","text":"<p>The parser generates JSON output with this structure:</p> <pre><code>{\n  \"cases\": [\n    {\n      \"court\": \"Alabama Civil Court of Appeals\",\n      \"case_number\": {\n        \"text\": \"CL-2024-0123\",\n        \"link\": \"/portal/case/detail/12345\"\n      },\n      \"case_title\": \"Smith v. Jones Corporation\",\n      \"classification\": \"Appeal\",\n      \"filed_date\": \"01/15/2024\",\n      \"status\": \"Open\"\n    }\n  ],\n  \"metadata\": {\n    \"extraction_date\": \"2024-01-20\",\n    \"extraction_time\": \"14:30:22\", \n    \"total_cases\": 156,\n    \"pages_processed\": 7,\n    \"parser\": \"ParserAppealsAL\",\n    \"source_url\": \"https://publicportal.alappeals.gov/...\"\n  }\n}\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#field-descriptions","title":"Field Descriptions","text":"<p>Case Fields: - <code>court</code>: Which Appeals Court handled the case - <code>case_number.text</code>: The case identifier (e.g., \"CL-2024-0123\") - <code>case_number.link</code>: Relative URL to case details page - <code>case_title</code>: Full case title/name - <code>classification</code>: Type of case (Appeal, Certiorari, etc.) - <code>filed_date</code>: Date case was filed (MM/DD/YYYY format) - <code>status</code>: Current case status (Open, Closed, etc.)</p> <p>Metadata Fields: - <code>extraction_date</code>: Date of extraction (YYYY-MM-DD) - <code>extraction_time</code>: Time of extraction (HH:MM:SS) - <code>total_cases</code>: Number of cases extracted - <code>pages_processed</code>: Number of pages processed - <code>parser</code>: Parser used (always \"ParserAppealsAL\") - <code>source_url</code>: Original URL scraped</p>"},{"location":"user-guide/parser-appeals-al-cli/#working-with-results","title":"Working with Results","text":""},{"location":"user-guide/parser-appeals-al-cli/#view-case-count","title":"View Case Count","text":"<pre><code># Extract and show total cases\npython -m opal --url \"...\" --parser ParserAppealsAL --output cases.json\ncat cases.json | jq '.metadata.total_cases'\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#extract-specific-fields","title":"Extract Specific Fields","text":"<pre><code># Get just case numbers\ncat cases.json | jq '.cases[].case_number.text'\n\n# Get case titles\ncat cases.json | jq '.cases[].case_title'\n\n# Get cases by status\ncat cases.json | jq '.cases[] | select(.status == \"Open\")'\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#convert-to-csv","title":"Convert to CSV","text":"<pre><code># Basic CSV conversion\ncat cases.json | jq -r '.cases[] | [.case_number.text, .case_title, .status, .filed_date] | @csv'\n\n# CSV with headers\necho \"Case Number,Title,Status,Filed Date\" &gt; cases.csv\ncat cases.json | jq -r '.cases[] | [.case_number.text, .case_title, .status, .filed_date] | @csv' &gt;&gt; cases.csv\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/parser-appeals-al-cli/#processing-speed","title":"Processing Speed","text":"<ul> <li>Typical rate: 1-2 pages per minute</li> <li>Factors: Network speed, page complexity, rate limiting</li> <li>Recommendation: Use <code>--max_pages</code> for initial testing</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#memory-usage","title":"Memory Usage","text":"<ul> <li>Low memory: Processes pages sequentially</li> <li>Scalable: Can handle large result sets</li> <li>Storage: JSON output size varies by case count</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Built-in delays: 3-second delays between requests</li> <li>Respectful: Designed not to overwhelm the server</li> <li>Adjustable: Modify in source code if needed</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/parser-appeals-al-cli/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/parser-appeals-al-cli/#no-cases-found","title":"\"No cases found\"","text":"<p>Cause: URL doesn't contain case listings Solution: Verify URL shows cases in browser first</p>"},{"location":"user-guide/parser-appeals-al-cli/#webdriver-error","title":"\"WebDriver error\"","text":"<p>Cause: Chrome browser not installed or outdated Solution: Install/update Chrome browser</p>"},{"location":"user-guide/parser-appeals-al-cli/#connection-timeout","title":"\"Connection timeout\"","text":"<p>Cause: Network issues or server problems Solution: Check internet connection, try again later</p>"},{"location":"user-guide/parser-appeals-al-cli/#permission-denied-writing-output","title":"\"Permission denied writing output\"","text":"<p>Cause: No write access to output directory Solution: Use different output path or fix permissions</p>"},{"location":"user-guide/parser-appeals-al-cli/#debug-steps","title":"Debug Steps","text":"<ol> <li>Test URL in browser: Verify cases are visible</li> <li>Enable debug logging: Use <code>--log-level DEBUG</code></li> <li>Limit pages: Use <code>--max_pages 1</code> for testing</li> <li>Check output: Verify JSON file is created and valid</li> </ol>"},{"location":"user-guide/parser-appeals-al-cli/#getting-help","title":"Getting Help","text":"<p>If issues persist: 1. Check the Error Handling Guide 2. Review Troubleshooting Documentation 3. Enable debug logging to see detailed error messages</p>"},{"location":"user-guide/parser-appeals-al-cli/#advanced-usage","title":"Advanced Usage","text":"<p>For more sophisticated court case extraction with search parameters, filtering, and automation, see:</p> <ul> <li>Configurable Court Extractor: Advanced search parameters</li> <li>Command Line Tools: Complete CLI reference</li> <li>API Reference: Programmatic usage</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature Basic CLI (<code>--parser ParserAppealsAL</code>) Configurable Extractor Search filters \u274c None \u2705 Date, category, court, case number Output formats JSON only JSON + CSV Multiple courts Single URL only All courts in one command Automation Manual URLs Parameterized searches Complexity Simple Advanced <p>Recommendation: Use basic CLI for simple extractions, Configurable Extractor for advanced needs.</p>"},{"location":"user-guide/parsers/","title":"Available Parsers","text":"<p>OPAL includes several parsers for different Alabama news and government websites.</p>"},{"location":"user-guide/parsers/#news-parsers","title":"News Parsers","text":""},{"location":"user-guide/parsers/#parser1819","title":"Parser1819","text":"<ul> <li>Website: 1819 News</li> <li>Content: Conservative news outlet covering Alabama politics and culture</li> <li>Usage:    <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#parserdailynews","title":"ParserDailyNews","text":"<ul> <li>Website: Alabama Daily News</li> <li>Content: Daily news covering Alabama politics, business, and current events</li> <li>Usage:   <pre><code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /news/item\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#government-parsers","title":"Government Parsers","text":""},{"location":"user-guide/parsers/#court-parser-parserappealsal","title":"Court Parser (ParserAppealsAL)","text":"<ul> <li>Website: Alabama Appeals Court Public Portal</li> <li>Content: Court cases, opinions, and legal documents</li> <li>Features:</li> <li>Automated Chrome WebDriver handling</li> <li>JavaScript-rendered content support</li> <li>Case details extraction</li> <li>Usage:   <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#parser-capabilities","title":"Parser Capabilities","text":"Parser Pagination JavaScript Support Authentication Parser1819 \u2713 \u2717 \u2717 ParserDailyNews \u2713 \u2717 \u2717 ParserAppealsAL \u2713 \u2713 \u2717"},{"location":"user-guide/parsers/#adding-new-parsers","title":"Adding New Parsers","text":"<p>To add support for a new website, see the Creating New Parsers guide.</p>"}]}