{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OPAL - Oppositional Positions in ALabama","text":"<p>Welcome to OPAL's documentation! OPAL is a web scraping tool that extracts content from websites like Alabama news sites and court records.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udcf0 Multiple News Sources - Parse articles from 1819news.com and Alabama Daily News</li> <li>\u2696\ufe0f Court Records - Extract data from Alabama Appeals Court Public Portal</li> <li>\ud83d\udd27 Extensible Architecture - Easy to add new parsers</li> <li>\ud83d\udcca Structured Output - Clean JSON format for analysis</li> <li>\ud83d\ude80 CLI Tool - Simple command-line interface</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#install-opal","title":"Install OPAL","text":"<p>pip install -e .</p>"},{"location":"#scrape-news-articles","title":"Scrape news articles","text":"<p>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5</p>"},{"location":"#scrape-court-cases","title":"Scrape court cases","text":"<p>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court</p>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li>Prerequisites Checker - Verify your system is ready for OPAL</li> <li>Complete Setup Guide - Step-by-step installation for beginners</li> <li>Environment-Specific Guides - Setup instructions for Windows, Mac, Linux, and cloud</li> <li>Quick Start Tutorial - Hands-on tutorial with real examples</li> <li>Output Examples - See exactly what OPAL produces</li> <li>User Guide - Detailed usage instructions</li> <li>Developer Guide - Extend OPAL with new parsers</li> </ul>"},{"location":"#built-by-alabama-forward","title":"Built by Alabama Forward","text":"<p>This project was created by Gabriel Cab\u00e1n Cubero, Data Director at Alabama Forward.</p>"},{"location":"about/contributing/","title":"Contributing to OPAL","text":"<p>We welcome contributions to OPAL! This guide will help you get started.</p>"},{"location":"about/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/yourusername/opal.git\ncd opal\n</code></pre></li> <li>Create a new branch for your feature:    <pre><code>git checkout -b feature-name\n</code></pre></li> </ol>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\npip install -e .\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install pytest black flake8\n</code></pre></p> </li> </ol>"},{"location":"about/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"about/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use meaningful variable and function names</li> <li>Add docstrings to all functions and classes</li> <li>Keep lines under 88 characters (Black's default)</li> </ul>"},{"location":"about/contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new features</li> <li>Ensure all tests pass before submitting</li> <li>Run tests with: <code>pytest tests/</code></li> </ul>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for new features</li> <li>Include docstrings in your code</li> <li>Update README if needed</li> </ul>"},{"location":"about/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request on GitHub</p> </li> </ol>"},{"location":"about/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code follows the style guidelines</li> <li>Update documentation as needed</li> <li>Add tests for new functionality</li> <li>Ensure all tests pass</li> <li>Update the CHANGELOG.md with your changes</li> </ol>"},{"location":"about/contributing/#adding-new-parsers","title":"Adding New Parsers","text":"<p>When contributing a new parser:</p> <ol> <li>Follow the BaseParser structure</li> <li>Include comprehensive error handling</li> <li>Add documentation to the parser class</li> <li>Create an example in the documentation</li> <li>Test with various edge cases</li> </ol>"},{"location":"about/contributing/#reporting-issues","title":"Reporting Issues","text":"<ul> <li>Use GitHub Issues to report bugs</li> <li>Include detailed reproduction steps</li> <li>Provide error messages and logs</li> <li>Specify your Python version and OS</li> </ul>"},{"location":"about/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers and help them get started</li> <li>Focus on constructive criticism</li> <li>Respect differing viewpoints</li> </ul>"},{"location":"about/contributing/#questions","title":"Questions?","text":"<p>Feel free to open an issue for any questions about contributing!</p>"},{"location":"about/license/","title":"License","text":"<p>OPAL is released under the MIT License.</p>"},{"location":"about/license/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2025 Gabriel Cab\u00e1n Cubero / Alabama Forward</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/license/#what-this-means","title":"What this means","text":"<ul> <li>You can use OPAL for commercial and non-commercial purposes</li> <li>You can modify the code to suit your needs</li> <li>You can distribute the software</li> <li>You must include the copyright notice in copies</li> <li>The software is provided \"as is\" without warranty</li> </ul>"},{"location":"about/license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>OPAL uses the following open-source libraries:</p> <ul> <li>BeautifulSoup4 - MIT License</li> <li>Requests - Apache License 2.0</li> <li>Selenium - Apache License 2.0</li> <li>MkDocs - BSD License</li> <li>Material for MkDocs - MIT License</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to OPAL, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/","title":"Alabama Appeals Court Public Portal Scraper - Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#overview","title":"Overview","text":"<p>Create a court case scraper extension for OPAL that extracts tabular data from the Alabama Appeals Court Public Portal. The scraper must handle JavaScript-rendered content, complex URL-based pagination, and preserve both text and link references.</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-by-step-implementation-instructions","title":"Step-by-Step Implementation Instructions","text":""},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-1-update-dependencies","title":"Step 1: Update Dependencies","text":"<p>Add the following to <code>requirements.txt</code> and <code>pyproject.toml</code>: - <code>selenium&gt;=4.0.0</code> or <code>playwright&gt;=1.40.0</code> (for JavaScript rendering) - <code>webdriver-manager&gt;=4.0.0</code> (if using Selenium for automatic driver management)</p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-2-create-court-case-parser-module","title":"Step 2: Create Court Case Parser Module","text":"<p>Create a new file <code>opal/court_case_parser.py</code> with the following specifications:</p> <ol> <li>Import necessary libraries:</li> <li>Selenium/Playwright for JavaScript rendering</li> <li>BeautifulSoup for HTML parsing</li> <li> <p>Standard libraries for URL manipulation and JSON output</p> </li> <li> <p>Create <code>CourtCaseParser</code> class that extends <code>BaseParser</code>:</p> </li> <li>Override <code>make_request()</code> to use Selenium/Playwright instead of requests</li> <li>Implement JavaScript rendering with appropriate wait conditions</li> <li> <p>Add rate limiting (minimum 2-3 seconds between requests)</p> </li> <li> <p>Implement <code>parse_table_row()</code> method to extract:</p> </li> <li>Court name from <code>&lt;td class=\"text-start\"&gt;</code> (column 1)</li> <li>Case number text and href from <code>&lt;a href=\"/portal/court/...\"&gt;</code> (column 2)</li> <li>Case title from <code>&lt;td class=\"text-start\"&gt;</code> (column 3)</li> <li>Classification from <code>&lt;td class=\"text-start\"&gt;</code> (column 4)</li> <li>Filed date from <code>&lt;td class=\"text-start\"&gt;</code> (column 5)</li> <li>Open/Closed status from <code>&lt;td class=\"text-start\"&gt;</code> (column 6)</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-3-create-custom-url-pagination-handler","title":"Step 3: Create Custom URL Pagination Handler","text":"<p>Create <code>opal/court_url_paginator.py</code> with:</p> <ol> <li>URL parser function to:</li> <li>Extract and decode the complex URL parameters</li> <li>Identify current page number from <code>page~(number~X)</code></li> <li> <p>Extract total pages from <code>totalPages~X</code></p> </li> <li> <p>URL builder function to:</p> </li> <li>Take base URL and page number</li> <li>Update <code>page~(number~X)</code> parameter</li> <li>Maintain all other search parameters</li> <li> <p>Handle special encoding (<code>~</code>, <code>%2a2f</code>, etc.)</p> </li> <li> <p>Pagination iterator that:</p> </li> <li>Starts at page 0</li> <li>Continues until reaching <code>totalPages</code></li> <li>Yields properly formatted URLs for each page</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-4-implement-data-extraction-logic","title":"Step 4: Implement Data Extraction Logic","text":"<p>In <code>CourtCaseParser</code>, create <code>parse_all_cases()</code> method that:</p> <ol> <li>Initialize browser driver (Selenium/Playwright)</li> <li>Load first page and extract total pages from URL</li> <li>For each page:</li> <li>Navigate to page URL</li> <li>Wait for table to load (use explicit waits)</li> <li>Extract all table rows</li> <li>Parse each row using <code>parse_table_row()</code></li> <li>Store results with preserved link references</li> <li>Close browser driver when complete</li> <li>Return combined results from all pages as single dataset</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-5-define-output-format","title":"Step 5: Define Output Format","text":"<p>Structure the output JSON as: <pre><code>{\n  \"status\": \"success\",\n  \"total_cases\": 317,\n  \"extraction_date\": \"2025-06-11\",\n  \"cases\": [\n    {\n      \"court\": \"Alabama Supreme Court\",\n      \"case_number\": {\n        \"text\": \"SC-2025-0424\",\n        \"link\": \"/portal/court/68f021c4-6a44-4735-9a76-5360b2e8af13/case/d024d958-58a1-41c9-9fae-39c645c7977e\"\n      },\n      \"case_title\": \"Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.\",\n      \"classification\": \"Appeal - Civil - Injunction Other\",\n      \"filed_date\": \"06/10/2025\",\n      \"status\": \"Open\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-6-integrate-with-opal-cli","title":"Step 6: Integrate with OPAL CLI","text":"<p>Modify existing OPAL files:</p> <ol> <li>Update <code>opal/__init__.py</code>:</li> <li>Add <code>from .court_case_parser import CourtCaseParser</code></li> <li> <p>Add <code>from .court_url_paginator import paginate_court_urls</code></p> </li> <li> <p>Update <code>opal/integrated_parser.py</code>:</p> </li> <li>Add conditional logic to handle court case URLs differently</li> <li> <p>Use <code>paginate_court_urls</code> instead of <code>get_all_news_urls</code> for court sites</p> </li> <li> <p>Update <code>opal/main.py</code>:</p> </li> <li>Add <code>--parser court</code> option to argparse choices</li> <li>Add court parser to the parser selection logic</li> <li>Adjust output filename format for court data</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-7-handle-technical-requirements","title":"Step 7: Handle Technical Requirements","text":"<p>Implement the following in <code>CourtCaseParser</code>:</p> <ol> <li>JavaScript rendering:</li> <li>Wait for table element to be present</li> <li>Wait for data rows to load</li> <li> <p>Handle any loading spinners or dynamic content</p> </li> <li> <p>Error handling:</p> </li> <li>Timeout exceptions for slow page loads</li> <li>Missing table elements</li> <li>Network errors</li> <li> <p>Browser crashes</p> </li> <li> <p>Rate limiting:</p> </li> <li>Add configurable delay between page requests (default 3 seconds)</li> <li>Respect server response times</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-8-testing-urls","title":"Step 8: Testing URLs","text":"<p>Use these URLs for testing: - First page: <code>https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</code> - Second page: Same URL but with <code>page~(number~1)</code> and updated <code>totalElements~317~totalPages~13</code></p>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#step-9-final-integration","title":"Step 9: Final Integration","text":"<ol> <li>Test the complete flow with: <code>python -m opal --url [court_url] --parser court</code></li> <li>Ensure output file is created with court case data in tabular format</li> <li>Verify all pages are scraped and combined into single result set</li> <li>Confirm case number links are preserved in the output</li> </ol>"},{"location":"developer/alabama_appeals_court_scraper_instructions/#expected-deliverables","title":"Expected Deliverables","text":"<ol> <li><code>opal/court_case_parser.py</code> - Main parser for court data</li> <li><code>opal/court_url_paginator.py</code> - URL pagination handler</li> <li>Updated <code>opal/__init__.py</code>, <code>opal/integrated_parser.py</code>, and <code>opal/main.py</code></li> <li>Updated <code>requirements.txt</code> and <code>pyproject.toml</code> with new dependencies</li> <li>JSON output file with all court cases in structured format</li> </ol>"},{"location":"developer/architecture/","title":"Architecture","text":"<p>OPAL follows a modular architecture that makes it easy to add new parsers and extend functionality.</p>"},{"location":"developer/architecture/#core-components","title":"Core Components","text":""},{"location":"developer/architecture/#baseparser","title":"BaseParser","text":"<p>The foundation of all parsers, providing: - Common web scraping functionality - Error handling and retry logic - Logging infrastructure - Output formatting</p>"},{"location":"developer/architecture/#parser-classes","title":"Parser Classes","text":"<p>Each website has its own parser class that inherits from BaseParser: - <code>Parser1819</code>: For 1819 News - <code>ParserDailyNews</code>: For Alabama Daily News - <code>ParserAppealsAL</code>: For Alabama Appeals Court</p>"},{"location":"developer/architecture/#main-module","title":"Main Module","text":"<p>The <code>__main__.py</code> module handles: - Command-line argument parsing - Parser instantiation - Execution flow - Output management</p>"},{"location":"developer/architecture/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser\n\u251c\u2500\u2500 Parser1819\n\u251c\u2500\u2500 ParserDailyNews\n\u2514\u2500\u2500 ParserAppealsAL\n</code></pre>"},{"location":"developer/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Input: User provides URL and parser type via CLI</li> <li>Initialization: Main module creates parser instance</li> <li>Scraping: Parser fetches and processes web pages</li> <li>Extraction: Parser extracts structured data</li> <li>Output: Data saved to JSON file</li> </ol>"},{"location":"developer/architecture/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"developer/architecture/#template-method-pattern","title":"Template Method Pattern","text":"<p>BaseParser defines the scraping workflow: <pre><code>class BaseParser:\n    def scrape(self):\n        self.setup()\n        data = self.extract_data()\n        self.save_output(data)\n</code></pre></p>"},{"location":"developer/architecture/#factory-pattern","title":"Factory Pattern","text":"<p>Parser selection based on command-line argument: <pre><code>parsers = {\n    'Parser1819': Parser1819,\n    'ParserDailyNews': ParserDailyNews,\n    'court': ParserAppealsAL\n}\nparser_class = parsers[args.parser]\n</code></pre></p>"},{"location":"developer/architecture/#extension-points","title":"Extension Points","text":""},{"location":"developer/architecture/#adding-new-parsers","title":"Adding New Parsers","text":"<ol> <li>Create new class inheriting from BaseParser</li> <li>Implement required methods:</li> <li><code>extract_article_data()</code></li> <li><code>get_article_links()</code></li> <li><code>parse_article()</code></li> <li>Register in main module</li> </ol>"},{"location":"developer/architecture/#customizing-output","title":"Customizing Output","text":"<p>Override <code>format_output()</code> method to customize data structure.</p>"},{"location":"developer/architecture/#adding-features","title":"Adding Features","text":"<ul> <li>Authentication: Add login methods</li> <li>Caching: Implement request caching</li> <li>Rate limiting: Add delay mechanisms</li> </ul>"},{"location":"developer/configurable_court_extractor_design/","title":"Configurable Court Extractor Design","text":""},{"location":"developer/configurable_court_extractor_design/#problem-statement","title":"Problem Statement","text":"<p>A former version of <code>extract_all_court_cases.py</code> had hardcoded search parameters in the URL, making it inflexible for different search criteria. Users couldn't dynamically change: - Date ranges - Case number filters - Case title filters - Whether to include/exclude closed cases</p>"},{"location":"developer/configurable_court_extractor_design/#solution-overview","title":"Solution Overview","text":"<p>I designed a configurable court extractor that separates URL construction from data extraction, allowing users to specify search parameters via command line arguments or function parameters.</p>"},{"location":"developer/configurable_court_extractor_design/#architecture","title":"Architecture","text":""},{"location":"developer/configurable_court_extractor_design/#1-courtsearchbuilder-class","title":"1. CourtSearchBuilder Class","text":"<p>Purpose: Encapsulates the complex URL building logic for Alabama Appeals Court searches.</p>"},{"location":"developer/configurable_court_extractor_design/#why-i-designed-it-this-way","title":"Why I designed it this way:","text":"<ul> <li> <p>Separation of Concerns: URL building is separate from data extraction</p> </li> <li> <p>Maintainability: Changes to URL structure only affect one class</p> </li> <li> <p>Reusability: Can be used by different scripts or tools</p> </li> <li> <p>Readability: Clear methods for each search parameter</p> </li> </ul> <pre><code>class CourtSearchBuilder:\n    def __init__(self):\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n        self.court_id = \"68f021c4-6a44-4735-9a76-5360b2e8af13\"\n        self.reset_params()\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#2-key-methods-explained","title":"2. Key Methods Explained","text":""},{"location":"developer/configurable_court_extractor_design/#set_date_range","title":"<code>set_date_range()</code>","text":"<p>Purpose: Handle different date range options Design rationale: - Supports both predefined periods (<code>-1y</code>, <code>-6m</code>) and custom date ranges - Automatically converts dates to the portal's expected format (<code>*2f</code> encoding) - Provides sensible defaults</p>"},{"location":"developer/configurable_court_extractor_design/#build_criteria_string","title":"<code>build_criteria_string()</code>","text":"<p>Purpose: Construct the complex URL-encoded criteria parameter Design rationale: - Handles the intricate URL encoding required by the portal - Builds the nested parameter structure programmatically - Reduces human error in URL construction</p>"},{"location":"developer/configurable_court_extractor_design/#build_url","title":"<code>build_url()</code>","text":"<p>Purpose: Create complete search URLs with pagination Design rationale: - Updates page numbers dynamically - Maintains other search parameters across pages - Returns ready-to-use URLs</p>"},{"location":"developer/configurable_court_extractor_design/#configuration-options","title":"Configuration Options","text":""},{"location":"developer/configurable_court_extractor_design/#court-selection","title":"Court Selection","text":"<pre><code># Available courts\ncourts = {\n    'civil': 'Alabama Civil Court of Appeals',\n    'criminal': 'Alabama Court of Criminal Appeals', \n    'supreme': 'Alabama Supreme Court'\n}\n\n# Select court\nsearch_builder.set_court('civil')  # or 'criminal', 'supreme'\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-number-formats","title":"Case Number Formats","text":"<pre><code># Open-ended search\nsearch_builder.set_case_number_filter('2024-001')\n\n# Court-specific formats\nsearch_builder.set_case_number_filter('CL-2024-0001')  # Civil Appeals\nsearch_builder.set_case_number_filter('CR-2024-0001')  # Criminal Appeals  \nsearch_builder.set_case_number_filter('SC-2024-0001')  # Supreme Court\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-categories","title":"Case Categories","text":"<pre><code># For Civil Appeals and Criminal Appeals\ncategories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n\n# For Supreme Court (includes additional option)\nsupreme_categories = ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n\n# Set category\nsearch_builder.set_case_category('Appeal')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#date-filters","title":"Date Filters","text":"<pre><code># Predefined periods (matching portal options)\nsearch_builder.set_date_range(period='7d')   # Last 7 days\nsearch_builder.set_date_range(period='1m')   # Last month\nsearch_builder.set_date_range(period='3m')   # Last 3 months\nsearch_builder.set_date_range(period='6m')   # Last 6 months\nsearch_builder.set_date_range(period='1y')   # Last year\n\n# Custom date range\nsearch_builder.set_date_range('2024-01-01', '2024-12-31', 'custom')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#case-title-and-status-filters","title":"Case Title and Status Filters","text":"<pre><code># Filter by case title (partial match)\nsearch_builder.set_case_title_filter('Smith v Jones')\n\n# Exclude closed cases\nsearch_builder.set_exclude_closed(True)\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#command-line-interface","title":"Command Line Interface","text":"<p>Why I included CLI arguments: - User-friendly: No need to modify code for different searches - Scriptable: Can be integrated into automated workflows - Documented: Built-in help shows all options</p>"},{"location":"developer/configurable_court_extractor_design/#usage-examples","title":"Usage Examples","text":""},{"location":"developer/configurable_court_extractor_design/#option-1-use-built-in-search-parameters-recommended","title":"Option 1: Use Built-in Search Parameters (Recommended)","text":"<pre><code># Extract all cases from last year (default from all courts)\npython configurable_court_extractor.py\n\n# Extract cases from Alabama Supreme Court only\npython configurable_court_extractor.py --court supreme\n\n# Extract cases from last 7 days from Criminal Appeals\npython configurable_court_extractor.py --court criminal --date-period 7d\n\n# Extract Appeal cases from Civil Court\npython configurable_court_extractor.py --court civil --case-category Appeal\n\n# Extract cases with custom date range from Supreme Court\npython configurable_court_extractor.py --court supreme --date-period custom --start-date 2024-01-01 --end-date 2024-06-30\n\n# Filter by specific case number format\npython configurable_court_extractor.py --court civil --case-number \"CL-2024-\"\n\n# Filter by case title in Criminal Appeals\npython configurable_court_extractor.py --court criminal --case-title \"State v\"\n\n# Exclude closed cases from Supreme Court\npython configurable_court_extractor.py --court supreme --exclude-closed\n\n# Extract Certified Questions from Supreme Court (unique to Supreme Court)\npython configurable_court_extractor.py --court supreme --case-category \"Certified Question\"\n\n# Comprehensive search with multiple filters\npython configurable_court_extractor.py --court civil --case-category Appeal --date-period 3m --exclude-closed --output-prefix \"civil_appeals_q1\"\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#option-2-use-pre-built-url-with-embedded-search-terms","title":"Option 2: Use Pre-built URL with Embedded Search Terms","text":"<p>\u26a0\ufe0f  WARNING: Custom URLs are temporary and session-based. They may stop working when the website session expires.</p> <pre><code># Use your existing URL with search terms already embedded\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29\"\n\n# Use custom URL with limited pages and custom output prefix\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\" --max-pages 5 --output-prefix \"my_custom_search\"\n\n# Any URL from the portal search interface works\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=YOUR_CUSTOM_SEARCH_CRITERIA\"\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#hybrid-approach","title":"Hybrid Approach","text":"<pre><code># You can also programmatically call the function with a custom URL\nfrom configurable_court_extractor import extract_court_cases_with_params\n\n# Use your existing URL\nyour_url = \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\nresult = extract_court_cases_with_params(custom_url=your_url, max_pages=10)\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#dynamic-court-id-discovery","title":"Dynamic Court ID Discovery","text":""},{"location":"developer/configurable_court_extractor_design/#the-problem-with-dynamic-ids","title":"The Problem with Dynamic IDs","text":"<p>Modern web applications often generate session-specific or dynamic identifiers that change between visits. The Alabama Appeals Court portal appears to use dynamic court IDs that are assigned during the user's session rather than being static, predictable values.</p>"},{"location":"developer/configurable_court_extractor_design/#solution","title":"Solution","text":"<p>Chosen Solution: Automatic Discovery The <code>discover_court_ids()</code> method navigates to the court's search interface and programmatically extracts the current court IDs by:</p> <ol> <li>Loading the search page - Navigates to the main case search interface</li> <li>Inspecting form elements - Locates the court selection dropdown or form elements</li> <li>Extracting ID mappings - Parses the HTML to find court names and their corresponding dynamic IDs</li> <li>Caching for session - Stores the discovered IDs for the duration of the session</li> </ol> <p>Option 2: Manual Discovery If automatic discovery fails, users can:</p> <ol> <li>Inspect browser network traffic - Use browser developer tools to monitor the search requests</li> <li>Extract court ID from URL - Copy a working search URL and extract the court ID parameter</li> <li>Set manually - Use <code>set_court_id_manually()</code> to override the discovered ID</li> </ol> <p>Option 3: URL Bypass (Fallback) When court ID discovery completely fails, users can:</p> <ol> <li>Use browser to build URL - Manually configure search on the website</li> <li>Copy complete URL - Get the full URL with embedded parameters</li> <li>Use --url option - Pass the pre-built URL directly, bypassing all parameter building</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#implementation-benefits","title":"Implementation Benefits","text":"<ol> <li>Resilient to changes - Automatically adapts to new court ID schemes</li> <li>Fallback options - Multiple strategies when automatic discovery fails</li> <li>User-friendly - Handles complexity behind the scenes</li> <li>Transparent - Shows discovered IDs to user for verification</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#usage-examples-with-dynamic-ids","title":"Usage Examples with Dynamic IDs","text":"<pre><code># Let the system discover court IDs automatically\npython configurable_court_extractor.py --court civil --date-period 1m\n\n# If discovery fails, fall back to custom URL\npython configurable_court_extractor.py --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n\n# For debugging: manually set a court ID\nsearch_builder = CourtSearchBuilder()\nsearch_builder.set_court_id_manually('civil', 'discovered-session-id-12345')\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"developer/configurable_court_extractor_design/#url-encoding-strategy","title":"URL Encoding Strategy","text":"<p>The Alabama Appeals Court portal uses a complex nested URL structure: <pre><code>?criteria=~%28advanced~false~courtID~%27{court_id}~page~%28...%29~sort~%28...%29~case~%28...%29%29\n</code></pre></p> <p>My approach:</p> <ol> <li>Build parameters as nested dictionaries</li> <li>Convert to the portal's specific encoding format</li> <li>Handle special characters and escaping automatically</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#error-handling","title":"Error Handling","text":"<p>Graceful degradation: - If total page count can't be determined, process incrementally - Continue processing if individual pages fail - Provide detailed error messages with stack traces</p>"},{"location":"developer/configurable_court_extractor_design/#performance-considerations","title":"Performance Considerations","text":"<p>Rate limiting:  - Configurable delays between requests - Respectful of server resources</p> <p>Memory efficiency: - Process pages incrementally - Don't load all data into memory at once</p> <p>Progress reporting: - Real-time feedback on processing status - Clear indication of completion</p>"},{"location":"developer/configurable_court_extractor_design/#advantages-over-former-implementation","title":"Advantages Over Former Implementation","text":""},{"location":"developer/configurable_court_extractor_design/#1-flexibility","title":"1. Flexibility","text":"<ul> <li>Before: Fixed search parameters in hardcoded URL</li> <li>After: Configurable search criteria via parameters OR custom URLs</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#2-maintainability","title":"2. Maintainability","text":"<ul> <li>Before: URL changes require code modification</li> <li>After: URL structure centralized in builder class with dynamic discovery</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#3-usability","title":"3. Usability","text":"<ul> <li>Before: Developers need to understand complex URL structure</li> <li>After: Simple method calls and CLI arguments</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#4-reusability","title":"4. Reusability","text":"<ul> <li>Before: Single-purpose script</li> <li>After: Reusable components for different use cases</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#5-documentation","title":"5. Documentation","text":"<ul> <li>Before: Search parameters hidden in URL</li> <li>After: Clear parameter documentation and examples</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#6-resilience-to-changes","title":"6. Resilience to Changes","text":"<ul> <li>Before: Hardcoded court IDs break when website changes</li> <li>After: Automatic discovery adapts to dynamic court ID schemes</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#7-multiple-fallback-options","title":"7. Multiple Fallback Options","text":"<ul> <li>Before: Script fails completely if URL structure changes</li> <li>After: Automatic discovery \u2192 manual discovery \u2192 custom URL bypass</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#integration-with-existing-code","title":"Integration with Existing Code","text":"<p>The new extractor coexists with the current implementation: - Uses the same <code>ParserAppealsAL</code> class - Produces the same JSON/CSV output format - Follows the same error handling patterns</p>"},{"location":"developer/configurable_court_extractor_design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"developer/configurable_court_extractor_design/#advanced-features","title":"Advanced Features","text":"<ul> <li>Save/load search configurations</li> <li>Scheduled extractions</li> <li>Differential updates (only new cases)</li> <li>Export to additional formats (Excel, XML)</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Parallel page processing</li> <li>Caching of search results</li> <li>Resume interrupted extractions</li> </ul>"},{"location":"developer/configurable_court_extractor_design/#code-structure","title":"Code Structure","text":"<pre><code>configurable_court_extractor.py\n\u251c\u2500\u2500 CourtSearchBuilder class\n\u2502   \u251c\u2500\u2500 Parameter management methods\n\u2502   \u251c\u2500\u2500 URL building methods\n\u2502   \u2514\u2500\u2500 Validation methods\n\u251c\u2500\u2500 extract_court_cases_with_params() function\n\u2502   \u251c\u2500\u2500 Search execution logic\n\u2502   \u251c\u2500\u2500 Progress reporting\n\u2502   \u2514\u2500\u2500 Output generation\n\u2514\u2500\u2500 main() function\n    \u251c\u2500\u2500 CLI argument parsing\n    \u251c\u2500\u2500 Parameter validation\n    \u2514\u2500\u2500 Function orchestration\n</code></pre>"},{"location":"developer/configurable_court_extractor_design/#why-this-design-is-better","title":"Why This Design is Better","text":"<ol> <li>Single Responsibility: Each class/function has one clear purpose</li> <li>Open/Closed Principle: Easy to extend without modifying existing code</li> <li>DRY (Don't Repeat Yourself): URL logic is centralized</li> <li>User-Centered: Designed around user needs, not technical constraints</li> <li>Testable: Components can be unit tested independently</li> <li>Documented: Self-documenting code with clear method names</li> </ol> <p>This design transforms a rigid, single-purpose script into a flexible, user-friendly tool that can adapt to various research needs while maintaining the reliability and performance of the original implementation.</p>"},{"location":"developer/configurable_court_extractor_design/#implementation-reference","title":"Implementation Reference","text":"<p>Note: The complete implementation code has been moved to the reference documentation for better organization. Please refer to the Configurable Court Extractor Reference for the full source code, API documentation, and detailed implementation examples.</p>"},{"location":"developer/configurable_court_extractor_design/#key-design-decisions-explained","title":"Key Design Decisions Explained","text":"<ol> <li>Builder Pattern: Separates URL construction complexity from business logic</li> <li>Dual-Mode Operation: Supports both parameter-based search and pre-built URL input</li> <li>Parameter Validation: Ensures required combinations are provided (custom dates)</li> <li>Progressive Enhancement: Starts with defaults, allows selective customization</li> <li>Error Recovery: Graceful handling when page counts can't be determined</li> <li>Output Consistency: Maintains same format as original extractor</li> <li>User Feedback: Real-time progress and parameter confirmation</li> <li>URL Flexibility: Custom URLs override all search parameters for maximum flexibility</li> </ol>"},{"location":"developer/configurable_court_extractor_design/#summary","title":"Summary","text":"<p>This design transforms a rigid, single-purpose script into a flexible, user-friendly tool that can adapt to various research needs while maintaining the reliability and performance of the original implementation. The modular architecture ensures that each component has a single responsibility, making the system easy to extend, test, and maintain.</p>"},{"location":"developer/court_scraper_analysis/","title":"Court Scraper Analysis","text":"<p>The ParserAppealsAL scraper is different from the news scrapers. This is because of the courts' use of JavaScript, which renders content dynamically. </p>"},{"location":"developer/court_scraper_analysis/#1-dynamic-javascript-rendered-content","title":"1. Dynamic JavaScript-Rendered Content","text":"<ul> <li>The site loads content dynamically, which means BeautifulSoup alone won't work</li> <li>You'll need Selenium or Playwright to render JavaScript before parsing</li> </ul>"},{"location":"developer/court_scraper_analysis/#2-complex-url-based-pagination","title":"2. Complex URL-Based Pagination","text":"<ul> <li>Pagination uses URL parameters (<code>page~(number~0)</code> becomes <code>page~(number~1)</code>)</li> <li>Total pages/elements are embedded in the URL (<code>totalElements~317~totalPages~13</code>)</li> <li>This is very different from the simple <code>/page/2</code> pattern in the news scrapers</li> </ul>"},{"location":"developer/court_scraper_analysis/#3-structured-table-data","title":"3. Structured Table Data","text":"<ul> <li>We needed 6 specific columns: Court, Case Number, Case Title, Classification, Filed Date, Open/Closed</li> <li>Case Numbers contained links we wanted to preserve (both text and href)</li> <li>We wanted all paginated results combined into one table</li> </ul>"},{"location":"developer/court_scraper_analysis/#4-custom-url-encoding","title":"4. Custom URL Encoding","text":"<ul> <li>The URLs use a unique encoding scheme with <code>~</code> and <code>%2a2f</code> (for slashes)</li> <li>Search parameters are complex with date ranges and multiple filters</li> </ul>"},{"location":"developer/court_scraper_analysis/#key-challenges","title":"Key Challenges:","text":"<ul> <li>Need to handle JavaScript rendering</li> <li>Must parse and manipulate the encoded URL format for pagination</li> <li>Need to extract both text and href attributes from case number links</li> <li>Different data structure (tabular vs. article paragraphs)</li> </ul>"},{"location":"developer/court_scraper_requirements/","title":"Court Case Scraper Extension Requirements","text":"<p>To help you build this court case scraper extension while maintaining OPAL's modularity, I'll need the following information:</p>"},{"location":"developer/court_scraper_requirements/#1-website-details","title":"1. Website Details","text":"<ul> <li>The URL of the court case website Example URL: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~0~totalElements~0~totalPages~0%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Example URLs of pages containing the tables you want to scrape Example URL after pagination of next batch of table elements: https://publicportal.alappeals.gov/portal/search/case/results?criteria=~%28advanced~false~courtID~%2768f021c4-6a44-4735-9a76-5360b2e8af13~page~%28size~25~number~1~totalElements~317~totalPages~13%29~sort~%28sortBy~%27caseHeader.filedDate~sortDesc~true%29~case~%28caseCategoryID~1000000~caseNumberQueryTypeID~10463~caseTitleQueryTypeID~300054~filedDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025~excludeClosed~false%29%29</li> <li>Screenshots or HTML snippets of the table structure</li> </ul>"},{"location":"developer/court_scraper_requirements/#2-data-requirements","title":"2. Data Requirements","text":"<ul> <li>What specific data fields do you need from the tables? (case number, parties, dates, status, etc.) I will need to access the following html fields for data</li> </ul> <p>Column 1 Title: Court Court</p> <p>Column 1 Content Follows this pattern: Alabama Supreme Court</p> <p>Column 2 Title: Case Number Case Number</p> <p>Column 2 Content Follows this pattern:  SC-2025-0424 </p> <p>Column 3 Title: Case Title Case Title</p> <p>Column 3 Content Follows this pattern: Frank Thomas Shumate, Jr. v. Berry Contracting L.P. d/b/a Bay Ltd.</p> <p>Column 4 Title: Classification Classification</p> <p>Column 4 Content Follows this pattern: Appeal - Civil - Injunction Other</p> <p>Column 5 Title: Filed Date Filed Date</p> <p>Column 5 Content Follows this pattern:  06/10/2025 </p> <p>Column 6 Title: Open / Closed Open / Closed</p> <p>Column 6 Content Follows this pattern:  Open </p> <ul> <li>Do you need data from multiple tables per page or one main table?</li> </ul> <p>I only want one table with all of the results of all of the pages at that url. Even if pagination is used to reduce the number of table elements that appear at a time, I want all the results in a single table.</p> <ul> <li>Any specific formatting requirements for the extracted data?</li> </ul>"},{"location":"developer/court_scraper_requirements/#3-navigation-pattern","title":"3. Navigation Pattern","text":"<ul> <li> <p>Does the site use pagination like the news sites? The pagination is not the same. The content is grouped into small chunks, but accessible at the same base url.</p> </li> <li> <p>Are there search/filter parameters in the URL? There are multiple search parameters in the URL. For example, here are the search terms for this url [case~%28caseCategoryID~1000000, caseNumberQueryTypeID~10463, aseTitleQueryTypeID~300054, iledDateChoice~%27-1y~filedDateStart~%2706%2a2f11%2a2f2024~filedDateEnd~%2706%2a2f11%2a2f2025, excludeClosed~false%29%29]</p> </li> <li>Do you need to follow links within tables to get additional details?</li> </ul> <p>I do not want to follow the links within the table, but I do want to store the text and the reference embedded in the link.</p>"},{"location":"developer/court_scraper_requirements/#4-technical-considerations","title":"4. Technical Considerations","text":"<ul> <li>Does the site require authentication? No</li> <li>Is the content loaded dynamically (JavaScript) or static HTML? Dynamically</li> <li>Any rate limiting concerns we should be aware of? Please keep the rate limits low</li> </ul>"},{"location":"developer/court_scraper_requirements/#proposed-extension-architecture","title":"Proposed Extension Architecture","text":"<p>Based on OPAL's current architecture, here's how we'd extend it:</p> <ol> <li>Create a new parser class (e.g., <code>CourtCaseParser</code>) extending <code>NewsParser</code> in <code>parser_module.py</code></li> <li>Adapt or create a new URL discovery function if the pagination pattern differs from the news sites</li> <li>Modify the CLI in <code>main.py</code> to add the court parser option</li> <li>Ensure the output format makes sense for tabular data (might need to adjust from the line-by-line article format)</li> </ol>"},{"location":"developer/court_scraper_requirements/#next-steps","title":"Next Steps","text":"<p>Please provide: 1. The court website URL 2. Description of the table structure you need to parse 3. Any specific requirements or constraints</p> <p>This will help me design the extension to fit seamlessly with your existing OPAL architecture.</p>"},{"location":"developer/creating-court-parsers/","title":"Creating Court Parsers with OPAL","text":""},{"location":"developer/creating-court-parsers/#overview","title":"Overview","text":"<p>This guide explains how to create parsers for court websites using OPAL's framework. We'll use <code>ParserAppealsAL</code> as our reference implementation, which extracts court case data from the Alabama Appeals Court Public Portal. Court parsers typically differ from news parsers because they often need to handle JavaScript-rendered content, complex table structures, and paginated results.</p>"},{"location":"developer/creating-court-parsers/#key-features","title":"Key Features","text":"<ul> <li>JavaScript Support: Uses Selenium WebDriver to render JavaScript-heavy pages</li> <li>Automatic Browser Management: Handles Chrome driver setup and teardown</li> <li>Rate Limiting: Built-in configurable delays between requests to avoid overwhelming the server</li> <li>Table Parsing: Specialized logic for extracting structured data from HTML tables</li> <li>Error Handling: Robust error handling with graceful fallbacks</li> <li>Headless Operation: Can run with or without a visible browser window</li> </ul>"},{"location":"developer/creating-court-parsers/#architecture","title":"Architecture","text":""},{"location":"developer/creating-court-parsers/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>BaseParser (Abstract Base Class)\n    \u2514\u2500\u2500 ParserAppealsAL\n</code></pre> <p>ParserAppealsAL inherits from the <code>BaseParser</code> base class, which defines the common interface for all parsers in the OPAL system. It overrides key methods to provide court-specific functionality.</p>"},{"location":"developer/creating-court-parsers/#dependencies","title":"Dependencies","text":"<pre><code># Core Dependencies\nselenium &gt;= 4.0.0          # Browser automation\nwebdriver-manager &gt;= 4.0.0 # Automatic ChromeDriver management\nbeautifulsoup4            # HTML parsing\nrequests                  # HTTP requests (inherited from base)\n\n# Standard Library\njson                      # JSON data handling\ntime                      # Rate limiting\ndatetime                  # Timestamp generation\ntyping                    # Type hints\n</code></pre>"},{"location":"developer/creating-court-parsers/#implementation-guide","title":"Implementation Guide","text":""},{"location":"developer/creating-court-parsers/#1-basic-structure","title":"1. Basic Structure","text":"<p>To implement your own court parser based on ParserAppealsAL, start with this structure:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nfrom your_project.parser_module import BaseParser\n\nclass YourCourtParser(BaseParser):\n    def __init__(self, headless=True, rate_limit_seconds=3):\n        super().__init__()\n        self.headless = headless\n        self.rate_limit_seconds = rate_limit_seconds\n        self.driver = None\n</code></pre>"},{"location":"developer/creating-court-parsers/#2-core-methods","title":"2. Core Methods","text":""},{"location":"developer/creating-court-parsers/#__init__self-headless-bool-true-rate_limit_seconds-int-3","title":"<code>__init__(self, headless: bool = True, rate_limit_seconds: int = 3)</code>","text":"<p>Initializes the parser with configuration options.</p> <p>Parameters: - <code>headless</code>: Run Chrome in headless mode (no visible window) - <code>rate_limit_seconds</code>: Delay between requests to avoid rate limiting</p>"},{"location":"developer/creating-court-parsers/#_setup_driverself","title":"<code>_setup_driver(self)</code>","text":"<p>Sets up the Chrome WebDriver with appropriate options:</p> <pre><code>def _setup_driver(self):\n    chrome_options = Options()\n    if self.headless:\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920,1080\")\n\n    service = Service(ChromeDriverManager().install())\n    self.driver = webdriver.Chrome(service=service, options=chrome_options)\n</code></pre>"},{"location":"developer/creating-court-parsers/#make_requestself-url-str-timeout-int-30-optionalstr","title":"<code>make_request(self, url: str, timeout: int = 30) -&gt; Optional[str]</code>","text":"<p>Overrides the base class method to use Selenium instead of requests library.</p> <p>Key Features: - Lazy driver initialization - Waits for specific elements to load - Implements rate limiting - Returns page source HTML</p> <pre><code>def make_request(self, url, timeout=30):\n    if not self.driver:\n        self._setup_driver()\n\n    self.driver.get(url)\n\n    # Wait for your specific element\n    WebDriverWait(self.driver, timeout).until(\n        EC.presence_of_element_located((By.CSS_SELECTOR, \"table\"))\n    )\n\n    time.sleep(self.rate_limit_seconds)\n    return self.driver.page_source\n</code></pre>"},{"location":"developer/creating-court-parsers/#parse_table_rowself-row-optionaldict","title":"<code>parse_table_row(self, row) -&gt; Optional[Dict]</code>","text":"<p>Extracts data from a single table row. This method is specific to the table structure of your court portal.</p> <p>Expected Table Structure: 1. Court Name 2. Case Number (with optional link) 3. Case Title 4. Classification 5. Filed Date 6. Status</p> <p>Returns: <pre><code>{\n    \"court\": \"Court of Civil Appeals\",\n    \"case_number\": {\n        \"text\": \"2230123\",\n        \"link\": \"/case/details/...\"\n    },\n    \"case_title\": \"Smith v. Jones\",\n    \"classification\": \"Civil\",\n    \"filed_date\": \"06/11/2024\",\n    \"status\": \"Active\"\n}\n</code></pre></p>"},{"location":"developer/creating-court-parsers/#parse_articleself-url-str-dict","title":"<code>parse_article(self, url: str) -&gt; Dict</code>","text":"<p>Main parsing method that processes a single page of court results.</p> <p>Process: 1. Loads the page using <code>make_request</code> 2. Parses HTML with BeautifulSoup 3. Finds the main data table 4. Extracts data from each row 5. Returns structured results</p>"},{"location":"developer/creating-court-parsers/#parse_all_casesself-base_url-str-page_urls-liststr-dict","title":"<code>parse_all_cases(self, base_url: str, page_urls: List[str]) -&gt; Dict</code>","text":"<p>Processes multiple pages of results and combines them.</p> <p>Returns: <pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        # List of case dictionaries\n    ]\n}\n</code></pre></p>"},{"location":"developer/creating-court-parsers/#3-integration-with-opal-system","title":"3. Integration with OPAL System","text":"<p>The parser integrates with OPAL through the <code>IntegratedParser</code> class:</p> <pre><code>from opal.integrated_parser import IntegratedParser\nfrom your_parser import YourCourtParser\n\n# Create parser instance\nparser = IntegratedParser(YourCourtParser)\n\n# Process court data\nresult = parser.process_site(\n    base_url=\"https://your-court-portal.gov/search\",\n    suffix=\"\",  # Not used for court parsers\n    max_pages=None  # Will process all available pages\n)\n</code></pre>"},{"location":"developer/creating-court-parsers/#4-url-pagination","title":"4. URL Pagination","text":"<p>Court portals often use complex URL parameters for pagination. The system includes helper functions in <code>court_url_paginator.py</code>:</p> <ul> <li><code>parse_court_url()</code>: Extracts page number and total pages from URL</li> <li><code>build_court_url()</code>: Constructs URLs for specific pages</li> <li><code>paginate_court_urls()</code>: Generates list of all page URLs</li> </ul>"},{"location":"developer/creating-court-parsers/#5-best-practices","title":"5. Best Practices","text":"<ol> <li>Error Handling: Always wrap operations in try-except blocks</li> <li>Resource Management: Ensure driver is closed in finally blocks</li> <li>Rate Limiting: Respect server limits to avoid IP bans</li> <li>Dynamic Waits: Use WebDriverWait instead of fixed sleep times when possible</li> <li>Memory Management: Close driver after processing to free resources</li> </ol>"},{"location":"developer/creating-court-parsers/#6-testing","title":"6. Testing","text":"<p>Create test scripts to validate your parser:</p> <pre><code>from your_parser import YourCourtParser\n\ndef test_single_page():\n    parser = YourCourtParser(headless=True)\n    result = parser.parse_article(\"https://court-url.gov/page1\")\n\n    assert result[\"cases\"]\n    assert len(result[\"cases\"]) &gt; 0\n\n    # Validate case structure\n    case = result[\"cases\"][0]\n    assert \"court\" in case\n    assert \"case_number\" in case\n    assert \"case_title\" in case\n</code></pre>"},{"location":"developer/creating-court-parsers/#customization-guide","title":"Customization Guide","text":""},{"location":"developer/creating-court-parsers/#adapting-for-different-court-systems","title":"Adapting for Different Court Systems","text":"<ol> <li>Table Structure: Modify <code>parse_table_row()</code> to match your court's table columns</li> <li>Wait Conditions: Update the element selector in <code>make_request()</code> </li> <li>URL Patterns: Adjust pagination logic in helper functions</li> <li>Data Fields: Add or remove fields based on available data</li> </ol>"},{"location":"developer/creating-court-parsers/#common-modifications","title":"Common Modifications","text":"<ol> <li> <p>Different Table Selectors: <pre><code># Instead of generic \"table\"\nWebDriverWait(self.driver, timeout).until(\n    EC.presence_of_element_located((By.ID, \"case-results-table\"))\n)\n</code></pre></p> </li> <li> <p>Additional Data Extraction: <pre><code># Add judge information if available\njudge = cells[6].get_text(strip=True) if len(cells) &gt; 6 else \"\"\n</code></pre></p> </li> <li> <p>Custom Headers: <pre><code># Some courts require authentication headers\nself.driver.add_cookie({\"name\": \"session\", \"value\": \"your-session-id\"})\n</code></pre></p> </li> </ol>"},{"location":"developer/creating-court-parsers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/creating-court-parsers/#common-issues","title":"Common Issues","text":"<ol> <li>ChromeDriver Not Found: </li> <li>Solution: webdriver-manager should handle this automatically</li> <li> <p>Manual fix: Download ChromeDriver matching your Chrome version</p> </li> <li> <p>Elements Not Loading:</p> </li> <li>Increase timeout in WebDriverWait</li> <li>Check if element selectors have changed</li> <li> <p>Verify JavaScript is executing properly</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Increase <code>rate_limit_seconds</code></li> <li>Implement exponential backoff</li> <li> <p>Consider using proxy rotation</p> </li> <li> <p>Memory Leaks:</p> </li> <li>Ensure driver is closed after use</li> <li>Implement periodic driver restarts for long runs</li> </ol>"},{"location":"developer/creating-court-parsers/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Headless Mode: Significantly faster than visible browser</li> <li>Parallel Processing: Not recommended due to rate limits</li> <li>Caching: Consider caching parsed results to avoid re-parsing</li> <li>Resource Usage: Each driver instance uses ~100-200MB RAM</li> </ul>"},{"location":"developer/creating-court-parsers/#example-output","title":"Example Output","text":"<pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 318,\n    \"extraction_date\": \"2025-01-13\",\n    \"cases\": [\n        {\n            \"court\": \"Court of Civil Appeals\",\n            \"case_number\": {\n                \"text\": \"CL-2024-000123\",\n                \"link\": \"/portal/case/details/123\"\n            },\n            \"case_title\": \"Smith v. Jones Corporation\",\n            \"classification\": \"Civil Appeal\",\n            \"filed_date\": \"01/10/2025\",\n            \"status\": \"Pending\"\n        },\n        {\n            \"court\": \"Court of Criminal Appeals\",\n            \"case_number\": {\n                \"text\": \"CR-2024-000456\",\n                \"link\": \"/portal/case/details/456\"\n            },\n            \"case_title\": \"State of Alabama v. Doe\",\n            \"classification\": \"Criminal Appeal\",\n            \"filed_date\": \"01/09/2025\",\n            \"status\": \"Active\"\n        }\n    ]\n}\n</code></pre>"},{"location":"developer/creating-court-parsers/#security-considerations","title":"Security Considerations","text":"<ol> <li>Input Validation: Always validate URLs before processing</li> <li>Sandbox Mode: Chrome runs with --no-sandbox for compatibility</li> <li>Credential Storage: Never hardcode credentials in parser</li> <li>SSL Verification: Selenium handles SSL by default</li> </ol>"},{"location":"developer/creating-court-parsers/#future-enhancements","title":"Future Enhancements","text":"<p>Consider these improvements for production use:</p> <ol> <li>Retry Logic: Implement automatic retries for failed requests</li> <li>Progress Tracking: Add callbacks for progress updates</li> <li>Data Validation: Implement schema validation for parsed data</li> <li>Export Formats: Support multiple output formats (CSV, Excel)</li> <li>Incremental Updates: Track previously parsed cases to avoid duplicates</li> </ol>"},{"location":"developer/creating-custom-parsers/","title":"Creating Custom Parsers","text":"<p>This comprehensive guide covers everything you need to know about creating custom parsers for OPAL, from web scraping fundamentals to practical implementation.</p>"},{"location":"developer/creating-custom-parsers/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Web Scraping</li> <li>Core Concepts</li> <li>Beautiful Soup Fundamentals</li> <li>BaseParser Architecture</li> <li>Step-by-Step Parser Creation</li> <li>Real-World Examples</li> <li>Special Cases &amp; Advanced Topics</li> <li>Registration &amp; Testing</li> <li>Best Practices &amp; Common Challenges</li> </ol>"},{"location":"developer/creating-custom-parsers/#introduction-to-web-scraping","title":"Introduction to Web Scraping","text":"<p>Web scraping is the process of programmatically extracting data from websites. OPAL uses web scraping to gather news articles and court records from Alabama-based websites.</p>"},{"location":"developer/creating-custom-parsers/#key-components","title":"Key Components","text":"<ol> <li>HTTP Requests: Fetching web pages from servers</li> <li>HTML Parsing: Extracting structured data from HTML</li> <li>Data Extraction: Finding specific information within the parsed content</li> <li>Error Handling: Gracefully managing network issues and unexpected content</li> </ol>"},{"location":"developer/creating-custom-parsers/#core-concepts","title":"Core Concepts","text":""},{"location":"developer/creating-custom-parsers/#http-requests-and-responses","title":"HTTP Requests and Responses","text":"<p>When scraping websites, you're making HTTP requests to web servers:</p> <pre><code>import requests\n\nresponse = requests.get('https://example.com')\nprint(f\"Status Code: {response.status_code}\")\nprint(f\"Content Type: {response.headers.get('Content-Type')}\")\n</code></pre> <p>Common status codes: - 200: Success - 404: Not Found - 403: Forbidden - 500: Server Error</p>"},{"location":"developer/creating-custom-parsers/#html-structure-and-dom","title":"HTML Structure and DOM","text":"<p>HTML documents have a tree-like structure called the DOM (Document Object Model):</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Page Title&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"article\"&gt;\n      &lt;h1&gt;Article Title&lt;/h1&gt;\n      &lt;p&gt;Article content...&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"developer/creating-custom-parsers/#beautiful-soup-fundamentals","title":"Beautiful Soup Fundamentals","text":"<p>Beautiful Soup is Python's primary HTML parsing library. OPAL uses it extensively for extracting data.</p>"},{"location":"developer/creating-custom-parsers/#basic-usage","title":"Basic Usage","text":"<pre><code>from bs4 import BeautifulSoup\n\n# Parse HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n\n# Find elements\ntitle = soup.find('title')\nall_links = soup.find_all('a')\narticle_div = soup.find('div', class_='article')\n</code></pre>"},{"location":"developer/creating-custom-parsers/#key-methods","title":"Key Methods","text":"Method Description Example <code>find()</code> Find first matching element <code>soup.find('h1')</code> <code>find_all()</code> Find all matching elements <code>soup.find_all('p')</code> <code>select()</code> CSS selector <code>soup.select('.article h1')</code> <code>get_text()</code> Extract text content <code>element.get_text(strip=True)</code> <code>get()</code> Get attribute value <code>link.get('href')</code>"},{"location":"developer/creating-custom-parsers/#navigation","title":"Navigation","text":"<pre><code># Parent/child navigation\nparent = element.parent\nchildren = element.children\nnext_sibling = element.next_sibling\n\n# Finding by attributes\narticles = soup.find_all('div', {'class': 'article', 'data-type': 'news'})\n</code></pre>"},{"location":"developer/creating-custom-parsers/#baseparser-architecture","title":"BaseParser Architecture","text":"<p>OPAL uses an Abstract Base Class (ABC) design pattern for parsers. This ensures consistency while allowing flexibility.</p>"},{"location":"developer/creating-custom-parsers/#the-baseparser-class","title":"The BaseParser Class","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass BaseParser(ABC):\n    \"\"\"Abstract base class for all OPAL parsers\"\"\"\n\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'OPAL Web Scraper'\n        })\n\n    @abstractmethod\n    def parse_article(self, url):\n        \"\"\"Must be implemented by each parser\"\"\"\n        pass\n\n    @abstractmethod\n    def extract_article_data(self, soup):\n        \"\"\"Must be implemented by each parser\"\"\"\n        pass\n</code></pre>"},{"location":"developer/creating-custom-parsers/#why-abstract-base-classes","title":"Why Abstract Base Classes?","text":"<ol> <li>Enforces Interface: All parsers must implement required methods</li> <li>Code Reuse: Common functionality in base class</li> <li>Type Safety: Better IDE support and error detection</li> <li>Extensibility: Easy to add new parsers</li> </ol>"},{"location":"developer/creating-custom-parsers/#step-by-step-parser-creation","title":"Step-by-Step Parser Creation","text":""},{"location":"developer/creating-custom-parsers/#step-1-create-your-parser-class","title":"Step 1: Create Your Parser Class","text":"<p>Create a new file in the <code>opal/</code> directory:</p> <pre><code># opal/parser_mynews.py\nfrom opal.parser_module import BaseParser\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass ParserMyNews(BaseParser):\n    def __init__(self):\n        super().__init__()\n        self.source_name = \"My News Site\"\n</code></pre>"},{"location":"developer/creating-custom-parsers/#step-2-implement-required-methods","title":"Step 2: Implement Required Methods","text":"<pre><code>def parse_article(self, url):\n    \"\"\"Parse a single article from the given URL\"\"\"\n    try:\n        response = self.make_request(url)\n        if response and response.status_code == 200:\n            soup = BeautifulSoup(response.text, 'html.parser')\n            return self.extract_article_data(soup, url)\n    except Exception as e:\n        print(f\"Error parsing article {url}: {e}\")\n    return None\n\ndef make_request(self, url):\n    \"\"\"Make HTTP request with error handling\"\"\"\n    try:\n        response = self.session.get(url, timeout=10)\n        response.raise_for_status()\n        return response\n    except requests.RequestException as e:\n        print(f\"Request failed for {url}: {e}\")\n        return None\n</code></pre>"},{"location":"developer/creating-custom-parsers/#step-3-implement-data-extraction","title":"Step 3: Implement Data Extraction","text":"<pre><code>def extract_article_data(self, soup, url):\n    \"\"\"Extract structured data from the parsed HTML\"\"\"\n    try:\n        # Extract title\n        title_element = soup.find('h1', class_='article-title')\n        title = title_element.get_text(strip=True) if title_element else \"No title\"\n\n        # Extract author\n        author_element = soup.find('span', class_='author-name')\n        author = author_element.get_text(strip=True) if author_element else \"Unknown\"\n\n        # Extract date\n        date_element = soup.find('time', class_='publish-date')\n        date = date_element.get('datetime', 'Unknown date') if date_element else \"Unknown date\"\n\n        # Extract content\n        content_div = soup.find('div', class_='article-content')\n        paragraphs = content_div.find_all('p') if content_div else []\n\n        # Format content\n        line_content = {}\n        for i, p in enumerate(paragraphs, 1):\n            text = p.get_text(strip=True)\n            if text:  # Skip empty paragraphs\n                line_content[f\"line {i}\"] = text\n\n        return {\n            'url': url,\n            'title': title,\n            'author': author,\n            'date': date,\n            'line_count': len(line_content),\n            'line_content': line_content\n        }\n    except Exception as e:\n        print(f\"Error extracting data: {e}\")\n        return None\n</code></pre>"},{"location":"developer/creating-custom-parsers/#step-4-handle-article-discovery","title":"Step 4: Handle Article Discovery","text":"<pre><code>def get_article_links(self, base_url, suffix=None, max_pages=None):\n    \"\"\"Discover article links from the news site\"\"\"\n    all_links = set()\n    page = 1\n\n    while True:\n        if max_pages and page &gt; max_pages:\n            break\n\n        page_url = f\"{base_url}?page={page}\"\n        response = self.make_request(page_url)\n\n        if not response:\n            break\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Find article links\n        articles = soup.find_all('a', class_='article-link')\n\n        if not articles:\n            break  # No more articles\n\n        for article in articles:\n            href = article.get('href')\n            if href:\n                # Make absolute URL\n                full_url = urljoin(base_url, href)\n\n                # Apply suffix filter if provided\n                if not suffix or suffix in full_url:\n                    all_links.add(full_url)\n\n        page += 1\n\n    return list(all_links)\n</code></pre>"},{"location":"developer/creating-custom-parsers/#real-world-examples","title":"Real-World Examples","text":""},{"location":"developer/creating-custom-parsers/#example-1-parser1819-simple-news-site","title":"Example 1: Parser1819 (Simple News Site)","text":"<pre><code>class Parser1819(BaseParser):\n    def extract_article_data(self, soup, url):\n        # Real implementation from OPAL\n        title = soup.find('title').text.strip() if soup.find('title') else \"No title\"\n\n        # Author extraction with multiple fallbacks\n        author = \"Unknown author\"\n        author_tags = soup.find_all('a', {'rel': 'author'})\n        if author_tags:\n            author = author_tags[0].text.strip()\n\n        # Date extraction from meta tag\n        date = \"Unknown date\"\n        date_tag = soup.find('meta', {'property': 'article:published_time'})\n        if date_tag:\n            date = date_tag.get('content', 'Unknown date')\n</code></pre>"},{"location":"developer/creating-custom-parsers/#example-2-parserappealsal-javascript-heavy-site","title":"Example 2: ParserAppealsAL (JavaScript-Heavy Site)","text":"<pre><code>class ParserAppealsAL(BaseParser):\n    def __init__(self):\n        super().__init__()\n        # Uses Selenium for JavaScript rendering\n        self.driver = self._setup_driver()\n\n    def _setup_driver(self):\n        options = webdriver.ChromeOptions()\n        options.add_argument('--headless')\n        return webdriver.Chrome(options=options)\n\n    def parse_article(self, url):\n        # Load page with JavaScript\n        self.driver.get(url)\n        # Wait for content to load\n        WebDriverWait(self.driver, 10).until(\n            EC.presence_of_element_located((By.CLASS_NAME, \"case-table\"))\n        )\n        # Parse rendered HTML\n        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n</code></pre>"},{"location":"developer/creating-custom-parsers/#special-cases-advanced-topics","title":"Special Cases &amp; Advanced Topics","text":""},{"location":"developer/creating-custom-parsers/#javascript-rendered-content","title":"JavaScript-Rendered Content","text":"<p>Some sites load content dynamically with JavaScript. Use Selenium:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\ndef setup_selenium_parser(self):\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless')  # Run without GUI\n    options.add_argument('--no-sandbox')\n    options.add_argument('--disable-dev-shm-usage')\n\n    self.driver = webdriver.Chrome(options=options)\n</code></pre>"},{"location":"developer/creating-custom-parsers/#handling-pagination","title":"Handling Pagination","text":"<pre><code>def handle_pagination(self, base_url, max_pages=None):\n    \"\"\"Navigate through paginated content\"\"\"\n    page = 1\n    all_data = []\n\n    while True:\n        if max_pages and page &gt; max_pages:\n            break\n\n        # Different pagination patterns\n        page_url = f\"{base_url}?page={page}\"  # Query parameter\n        # OR: page_url = f\"{base_url}/page/{page}\"  # Path-based\n\n        response = self.make_request(page_url)\n        if not response or response.status_code == 404:\n            break  # No more pages\n\n        # Extract data from page\n        data = self.extract_page_data(response)\n        if not data:\n            break  # No data found\n\n        all_data.extend(data)\n        page += 1\n\n    return all_data\n</code></pre>"},{"location":"developer/creating-custom-parsers/#rate-limiting","title":"Rate Limiting","text":"<p>Respect server resources:</p> <pre><code>import time\n\ndef parse_with_rate_limit(self, urls, delay=1):\n    \"\"\"Parse URLs with delay between requests\"\"\"\n    results = []\n\n    for i, url in enumerate(urls):\n        if i &gt; 0:\n            time.sleep(delay)  # Delay between requests\n\n        result = self.parse_article(url)\n        if result:\n            results.append(result)\n\n    return results\n</code></pre>"},{"location":"developer/creating-custom-parsers/#registration-testing","title":"Registration &amp; Testing","text":""},{"location":"developer/creating-custom-parsers/#register-your-parser","title":"Register Your Parser","text":"<p>Add your parser to <code>opal/__main__.py</code>:</p> <pre><code>from opal.parser_mynews import ParserMyNews\n\n# In the parser dictionary\nparsers = {\n    'Parser1819': Parser1819,\n    'ParserDailyNews': ParserDailyNews,\n    'ParserAppealsAL': ParserAppealsAL,\n    'ParserMyNews': ParserMyNews  # Add your parser here\n}\n</code></pre>"},{"location":"developer/creating-custom-parsers/#test-your-parser","title":"Test Your Parser","text":"<ol> <li> <p>Unit Test: <pre><code># tests/test_parser_mynews.py\nimport pytest\nfrom opal.parser_mynews import ParserMyNews\n\ndef test_parser_initialization():\n    parser = ParserMyNews()\n    assert parser.source_name == \"My News Site\"\n\ndef test_extract_article_data():\n    parser = ParserMyNews()\n    # Test with sample HTML\n    html = '&lt;h1 class=\"article-title\"&gt;Test Article&lt;/h1&gt;'\n    soup = BeautifulSoup(html, 'html.parser')\n    data = parser.extract_article_data(soup, 'http://test.com')\n    assert data['title'] == 'Test Article'\n</code></pre></p> </li> <li> <p>Integration Test: <pre><code># Test with one page\npython -m opal --url https://mynews.com --parser ParserMyNews --max_pages 1\n\n# Verify output\ncat YYYY-MM-DD_ParserMyNews.json\n</code></pre></p> </li> </ol>"},{"location":"developer/creating-custom-parsers/#best-practices-common-challenges","title":"Best Practices &amp; Common Challenges","text":""},{"location":"developer/creating-custom-parsers/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling</li> <li>Always use try-except blocks</li> <li>Log errors with context</li> <li>Return None or empty data on failure</li> <li> <p>Never let parser crash the entire application</p> </li> <li> <p>Respect robots.txt <pre><code>from urllib.robotparser import RobotFileParser\n\ndef check_robots_txt(self, url):\n    rp = RobotFileParser()\n    rp.set_url(urljoin(url, '/robots.txt'))\n    rp.read()\n    return rp.can_fetch('*', url)\n</code></pre></p> </li> <li> <p>User-Agent Headers</p> </li> <li>Always set a descriptive User-Agent</li> <li> <p>Include contact information if scraping extensively</p> </li> <li> <p>Data Validation</p> </li> <li>Verify extracted data makes sense</li> <li>Handle missing fields gracefully</li> <li> <p>Validate URLs before making requests</p> </li> <li> <p>Performance</p> </li> <li>Use session objects for connection pooling</li> <li>Implement caching for repeated requests</li> <li>Consider concurrent requests for large datasets</li> </ol>"},{"location":"developer/creating-custom-parsers/#common-challenges","title":"Common Challenges","text":"<ol> <li>Dynamic Content</li> <li>Problem: Content loaded by JavaScript</li> <li> <p>Solution: Use Selenium or analyze API calls</p> </li> <li> <p>Anti-Scraping Measures</p> </li> <li>Problem: IP blocking, CAPTCHAs</li> <li> <p>Solution: Rotate user agents, add delays, respect rate limits</p> </li> <li> <p>Changing HTML Structure</p> </li> <li>Problem: Site redesigns break selectors</li> <li> <p>Solution: Use multiple selectors, flexible matching</p> </li> <li> <p>Encoding Issues</p> </li> <li>Problem: Special characters display incorrectly</li> <li> <p>Solution: Specify encoding: <code>response.encoding = 'utf-8'</code></p> </li> <li> <p>Large Datasets</p> </li> <li>Problem: Memory issues with thousands of articles</li> <li>Solution: Process in batches, write incrementally</li> </ol>"},{"location":"developer/creating-custom-parsers/#debugging-tips","title":"Debugging Tips","text":"<pre><code># Debug HTML structure\nprint(soup.prettify())  # See formatted HTML\n\n# Check element existence\nif not soup.find('div', class_='article'):\n    print(\"Article container not found!\")\n\n# Log selector results\nelements = soup.select('.article-title')\nprint(f\"Found {len(elements)} title elements\")\n\n# Save problematic HTML\nwith open('debug.html', 'w') as f:\n    f.write(str(soup))\n</code></pre>"},{"location":"developer/creating-custom-parsers/#conclusion","title":"Conclusion","text":"<p>Creating custom parsers for OPAL involves understanding web scraping fundamentals, implementing the BaseParser interface, and handling real-world challenges. Start simple, test thoroughly, and always respect the websites you're scraping.</p> <p>For more examples, examine the existing parsers in the OPAL codebase. Each demonstrates different techniques for handling various website structures and challenges.</p>"},{"location":"developer/error_handling/","title":"Error Handling","text":"<p>OPAL includes comprehensive error handling mechanisms to ensure robust data extraction. This document covers error types, handling strategies, and troubleshooting guidance for developers.</p> <p>For users experiencing errors: See Understanding Errors for user-friendly explanations and solutions.</p>"},{"location":"developer/error_handling/#error-categories","title":"Error Categories","text":""},{"location":"developer/error_handling/#1-selenium-webdriver-errors","title":"1. Selenium WebDriver Errors","text":""},{"location":"developer/error_handling/#timeoutexception","title":"TimeoutException","text":"<p>Occurs when elements don't load within expected timeframes.</p> <p>Common Causes: - Slow network connection - Page taking longer than usual to load - Element selectors changed on the website</p> <p>Handling Strategy: <pre><code>from selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support.ui import WebDriverWait\n\ntry:\n    element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, \"target-element\"))\n    )\nexcept TimeoutException:\n    logger.warning(\"Element not found within timeout, retrying...\")\n    # Implement retry logic or fallback\n</code></pre></p>"},{"location":"developer/error_handling/#staleelementreferenceexception","title":"StaleElementReferenceException","text":"<p>Occurs when trying to interact with elements that are no longer attached to the DOM.</p> <p>Handling Strategy: <pre><code>from selenium.common.exceptions import StaleElementReferenceException\n\ndef safe_element_interaction(driver, locator, action):\n    max_retries = 3\n    for attempt in range(max_retries):\n        try:\n            element = driver.find_element(*locator)\n            return action(element)\n        except StaleElementReferenceException:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)  # Wait before retry\n</code></pre></p>"},{"location":"developer/error_handling/#webdriverexception","title":"WebDriverException","text":"<p>General driver-related errors including crashes.</p> <p>Handling Strategy: <pre><code>from selenium.common.exceptions import WebDriverException\n\ndef restart_driver_on_failure(parser_instance):\n    try:\n        # Perform driver operation\n        return parser_instance.extract_data()\n    except WebDriverException as e:\n        logger.error(f\"Driver failed: {e}\")\n        parser_instance._restart_driver()\n        return parser_instance.extract_data()  # Retry once\n</code></pre></p>"},{"location":"developer/error_handling/#2-network-errors","title":"2. Network Errors","text":""},{"location":"developer/error_handling/#connection-timeouts","title":"Connection Timeouts","text":"<p>Network connectivity issues or server unresponsiveness.</p> <p>Handling Strategy: - Implement exponential backoff - Retry with longer timeouts - Check network connectivity</p> <pre><code>import time\nimport random\n\ndef retry_with_backoff(func, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except (requests.ConnectionError, requests.Timeout) as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = (2 ** attempt) + random.uniform(0, 1)\n            logger.warning(f\"Network error, retrying in {wait_time:.2f}s\")\n            time.sleep(wait_time)\n</code></pre>"},{"location":"developer/error_handling/#http-status-errors","title":"HTTP Status Errors","text":"<p>Server returning error status codes (404, 500, etc.)</p> <pre><code>def handle_http_errors(response):\n    if response.status_code == 404:\n        logger.error(\"Page not found - URL may have changed\")\n        return None\n    elif response.status_code &gt;= 500:\n        logger.error(\"Server error - may be temporary\")\n        raise ServerError(\"Server returned error status\")\n    elif response.status_code != 200:\n        logger.warning(f\"Unexpected status code: {response.status_code}\")\n</code></pre>"},{"location":"developer/error_handling/#3-parsing-errors","title":"3. Parsing Errors","text":""},{"location":"developer/error_handling/#elementnotfound","title":"ElementNotFound","text":"<p>Target HTML elements are missing or have changed.</p> <p>Handling Strategy: <pre><code>def safe_find_element(driver, primary_selector, fallback_selectors=None):\n    \"\"\"Try multiple selectors for robustness\"\"\"\n    selectors = [primary_selector] + (fallback_selectors or [])\n\n    for selector in selectors:\n        try:\n            return driver.find_element(*selector)\n        except NoSuchElementException:\n            continue\n\n    logger.error(\"No matching elements found with any selector\")\n    return None\n</code></pre></p>"},{"location":"developer/error_handling/#datavalidation","title":"DataValidation","text":"<p>Extracted data doesn't match expected format.</p> <pre><code>def validate_case_data(case_data):\n    \"\"\"Validate extracted court case data\"\"\"\n    required_fields = ['case_number', 'case_title', 'court', 'status']\n\n    for field in required_fields:\n        if field not in case_data or not case_data[field]:\n            logger.warning(f\"Missing required field: {field}\")\n            return False\n\n    # Validate date format\n    if case_data.get('filed_date'):\n        try:\n            datetime.strptime(case_data['filed_date'], '%m/%d/%Y')\n        except ValueError:\n            logger.warning(f\"Invalid date format: {case_data['filed_date']}\")\n            case_data['filed_date'] = None\n\n    return True\n</code></pre>"},{"location":"developer/error_handling/#4-session-management-errors","title":"4. Session Management Errors","text":""},{"location":"developer/error_handling/#session-expiration","title":"Session Expiration","text":"<p>Court system URLs contain session tokens that expire.</p> <p>Detection: <pre><code>def is_session_expired(driver):\n    \"\"\"Check if current session has expired\"\"\"\n    try:\n        # Look for session expired indicators\n        expired_indicators = [\n            \"Session has expired\",\n            \"Please log in again\",\n            \"Invalid session\"\n        ]\n\n        page_text = driver.page_source.lower()\n        return any(indicator.lower() in page_text for indicator in expired_indicators)\n    except:\n        return False\n</code></pre></p> <p>Handling: <pre><code>def handle_session_expiration(custom_url):\n    \"\"\"Provide guidance for expired sessions\"\"\"\n    logger.error(\"Session appears to have expired\")\n    logger.info(\"Custom URLs are session-based and expire after ~30 minutes\")\n    logger.info(\"Please create a new search and provide the fresh URL\")\n\n    # Suggest alternative\n    logger.info(\"Or use the configurable extractor to build a new search:\")\n    logger.info(\"python -m opal.configurable_court_extractor --court civil --date-period 7d\")\n</code></pre></p>"},{"location":"developer/error_handling/#5-configuration-errors","title":"5. Configuration Errors","text":""},{"location":"developer/error_handling/#invalid-parameters","title":"Invalid Parameters","text":"<p>User-provided parameters don't match expected values.</p> <pre><code>def validate_court_type(court_type):\n    \"\"\"Validate court type parameter\"\"\"\n    valid_courts = ['civil', 'criminal', 'supreme']\n    if court_type not in valid_courts:\n        raise ValueError(f\"Invalid court type: {court_type}. Must be one of {valid_courts}\")\n\ndef validate_date_period(date_period):\n    \"\"\"Validate date period parameter\"\"\"\n    valid_periods = ['7d', '1m', '3m', '6m', '1y', 'custom']\n    if date_period not in valid_periods:\n        raise ValueError(f\"Invalid date period: {date_period}. Must be one of {valid_periods}\")\n</code></pre>"},{"location":"developer/error_handling/#error-recovery-strategies","title":"Error Recovery Strategies","text":""},{"location":"developer/error_handling/#1-graceful-degradation","title":"1. Graceful Degradation","text":"<p>When non-critical operations fail, continue with reduced functionality:</p> <pre><code>def extract_with_fallbacks(driver):\n    \"\"\"Extract data with fallback strategies\"\"\"\n    results = []\n\n    try:\n        # Primary extraction method\n        results = extract_full_data(driver)\n    except Exception as e:\n        logger.warning(f\"Primary extraction failed: {e}\")\n\n        try:\n            # Fallback to basic extraction\n            results = extract_basic_data(driver)\n            logger.info(\"Using fallback extraction method\")\n        except Exception as e2:\n            logger.error(f\"Fallback extraction also failed: {e2}\")\n            # Return partial results if any\n            results = extract_minimal_data(driver)\n\n    return results\n</code></pre>"},{"location":"developer/error_handling/#2-partial-success-handling","title":"2. Partial Success Handling","text":"<p>Continue processing even when some operations fail:</p> <pre><code>def process_all_pages(page_urls):\n    \"\"\"Process all pages, continuing on individual failures\"\"\"\n    successful_pages = 0\n    failed_pages = 0\n    all_results = []\n\n    for i, url in enumerate(page_urls):\n        try:\n            logger.info(f\"Processing page {i+1}/{len(page_urls)}\")\n            page_results = extract_page_data(url)\n            all_results.extend(page_results)\n            successful_pages += 1\n        except Exception as e:\n            logger.error(f\"Failed to process page {i+1}: {e}\")\n            failed_pages += 1\n            continue  # Continue with next page\n\n    logger.info(f\"Completed: {successful_pages} successful, {failed_pages} failed\")\n    return all_results\n</code></pre>"},{"location":"developer/error_handling/#3-state-recovery","title":"3. State Recovery","text":"<p>Save progress to recover from failures:</p> <pre><code>import json\nimport os\n\nclass StatefulExtractor:\n    def __init__(self, state_file=\"extraction_state.json\"):\n        self.state_file = state_file\n        self.state = self.load_state()\n\n    def load_state(self):\n        \"\"\"Load previous state if exists\"\"\"\n        if os.path.exists(self.state_file):\n            with open(self.state_file, 'r') as f:\n                return json.load(f)\n        return {\"completed_pages\": [], \"results\": []}\n\n    def save_state(self):\n        \"\"\"Save current state\"\"\"\n        with open(self.state_file, 'w') as f:\n            json.dump(self.state, f)\n\n    def extract_with_recovery(self, page_urls):\n        \"\"\"Extract data with state recovery\"\"\"\n        for url in page_urls:\n            if url in self.state[\"completed_pages\"]:\n                logger.info(f\"Skipping already processed page: {url}\")\n                continue\n\n            try:\n                results = extract_page_data(url)\n                self.state[\"results\"].extend(results)\n                self.state[\"completed_pages\"].append(url)\n                self.save_state()  # Save after each page\n            except Exception as e:\n                logger.error(f\"Failed to process {url}: {e}\")\n                continue\n\n        # Clean up state file on completion\n        if os.path.exists(self.state_file):\n            os.remove(self.state_file)\n\n        return self.state[\"results\"]\n</code></pre>"},{"location":"developer/error_handling/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"developer/error_handling/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass StructuredLogger:\n    def __init__(self, name):\n        self.logger = logging.getLogger(name)\n\n    def log_extraction_start(self, court_type, parameters):\n        \"\"\"Log extraction start with context\"\"\"\n        self.logger.info(\"Extraction started\", extra={\n            \"event\": \"extraction_start\",\n            \"court_type\": court_type,\n            \"parameters\": parameters,\n            \"timestamp\": datetime.utcnow().isoformat()\n        })\n\n    def log_error(self, error_type, error_message, context=None):\n        \"\"\"Log errors with structured data\"\"\"\n        self.logger.error(\"Error occurred\", extra={\n            \"event\": \"error\",\n            \"error_type\": error_type,\n            \"error_message\": str(error_message),\n            \"context\": context or {},\n            \"timestamp\": datetime.utcnow().isoformat()\n        })\n</code></pre>"},{"location":"developer/error_handling/#error-metrics","title":"Error Metrics","text":"<p>Track error rates and types:</p> <pre><code>from collections import defaultdict\nimport time\n\nclass ErrorMetrics:\n    def __init__(self):\n        self.error_counts = defaultdict(int)\n        self.start_time = time.time()\n\n    def record_error(self, error_type):\n        \"\"\"Record an error occurrence\"\"\"\n        self.error_counts[error_type] += 1\n\n    def get_error_summary(self):\n        \"\"\"Get summary of all errors\"\"\"\n        total_errors = sum(self.error_counts.values())\n        runtime = time.time() - self.start_time\n\n        return {\n            \"total_errors\": total_errors,\n            \"error_rate\": total_errors / (runtime / 60),  # errors per minute\n            \"error_breakdown\": dict(self.error_counts),\n            \"runtime_minutes\": runtime / 60\n        }\n</code></pre>"},{"location":"developer/error_handling/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"developer/error_handling/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"developer/error_handling/#1-element-not-found-errors","title":"1. \"Element not found\" errors","text":"<p>Symptoms: NoSuchElementException, TimeoutException Causes: Website changes, slow loading, wrong selectors Solutions: - Check if website structure changed - Increase timeout values - Use more robust selectors - Add wait conditions</p>"},{"location":"developer/error_handling/#2-empty-results-returned","title":"2. Empty results returned","text":"<p>Symptoms: No data extracted, empty result sets Causes: Page not loading, changed selectors, session issues Solutions: - Run in non-headless mode to see browser - Check page source for expected elements - Verify URL is correct and accessible - Check for session expiration</p>"},{"location":"developer/error_handling/#3-driver-crashes-or-hangs","title":"3. Driver crashes or hangs","text":"<p>Symptoms: WebDriverException, processes not terminating Causes: Memory issues, driver version problems, resource limits Solutions: - Update ChromeDriver - Increase system resources - Add proper cleanup in finally blocks - Use context managers for driver lifecycle</p>"},{"location":"developer/error_handling/#4-slow-extraction-performance","title":"4. Slow extraction performance","text":"<p>Symptoms: Very slow processing, timeouts Causes: Network issues, server rate limiting, inefficient code Solutions: - Adjust rate limiting parameters - Optimize element selection - Use headless mode - Check network connectivity</p>"},{"location":"developer/error_handling/#debug-mode-usage","title":"Debug Mode Usage","text":"<p>Enable comprehensive debugging:</p> <pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Run extraction with debugging\nfrom opal.configurable_court_extractor import extract_court_cases_with_params\n\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='7d',\n    max_pages=3\n    # The function internally uses ParserAppealsAL with headless=True by default\n    # To debug, modify the function to set headless=False\n)\n\n# Add breakpoints for inspection\nimport pdb; pdb.set_trace()\n</code></pre>"},{"location":"developer/error_handling/#health-checks","title":"Health Checks","text":"<p>Implement health checks for long-running extractions:</p> <pre><code>def health_check(driver):\n    \"\"\"Check if extraction environment is healthy\"\"\"\n    checks = {\n        \"driver_responsive\": False,\n        \"page_loaded\": False,\n        \"elements_present\": False\n    }\n\n    try:\n        # Check driver responsiveness\n        driver.current_url\n        checks[\"driver_responsive\"] = True\n\n        # Check page loaded\n        driver.execute_script(\"return document.readyState\") == \"complete\"\n        checks[\"page_loaded\"] = True\n\n        # Check for expected elements\n        driver.find_element(By.TAG_NAME, \"body\")\n        checks[\"elements_present\"] = True\n\n    except Exception as e:\n        logger.warning(f\"Health check failed: {e}\")\n\n    return all(checks.values()), checks\n</code></pre>"},{"location":"developer/error_handling/#best-practices","title":"Best Practices","text":"<ol> <li>Always use timeouts - Never wait indefinitely</li> <li>Implement retry logic - Network issues are common</li> <li>Log comprehensively - Errors and successful operations</li> <li>Validate data - Check extracted data makes sense</li> <li>Handle partial failures - Don't let one failure stop everything</li> <li>Clean up resources - Always close drivers and files</li> <li>Provide user feedback - Show progress and error context</li> <li>Plan for recovery - Save state for long operations</li> </ol>"},{"location":"developer/error_handling/#error-response-formats","title":"Error Response Formats","text":"<p>Standardized error responses for consistency:</p> <pre><code>def create_error_response(error_type, message, context=None):\n    \"\"\"Create standardized error response\"\"\"\n    return {\n        \"status\": \"error\",\n        \"error_type\": error_type,\n        \"error_message\": message,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"context\": context or {}\n    }\n\n# Usage examples\ntimeout_error = create_error_response(\n    \"TimeoutError\",\n    \"Page failed to load within 30 seconds\",\n    {\"url\": \"https://example.com\", \"timeout\": 30}\n)\n\nsession_error = create_error_response(\n    \"SessionExpired\",\n    \"Court session has expired\",\n    {\"suggestion\": \"Create a new search\"}\n)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/","title":"User-Agent Headers Guide","text":""},{"location":"developer/user_agent_headers_guide/#what-is-a-user-agent","title":"What is a User-Agent?","text":"<p>User-Agent headers are strings that identify the client (browser, bot, or application) making an HTTP request to a web server.</p> <p>It's an HTTP header that tells the server: - What software is making the request - What version it is - What operating system it's running on</p>"},{"location":"developer/user_agent_headers_guide/#examples-of-user-agent-strings","title":"Examples of User-Agent Strings","text":""},{"location":"developer/user_agent_headers_guide/#chrome-browser","title":"Chrome Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#python-requests-default","title":"Python Requests (default)","text":"<pre><code>python-requests/2.28.0\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#googlebot","title":"Googlebot","text":"<pre><code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#firefox-browser","title":"Firefox Browser","text":"<pre><code>Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#safari-browser","title":"Safari Browser","text":"<pre><code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#why-user-agents-matter","title":"Why User-Agents Matter","text":"<ol> <li>Server Behavior: Websites may serve different content based on User-Agent</li> <li>Access Control: Some sites block requests with suspicious or missing User-Agents</li> <li>Analytics: Helps websites understand their traffic</li> <li>Bot Detection: Sites use it to identify and potentially block scrapers</li> <li>Content Optimization: Sites may serve mobile vs desktop versions</li> </ol>"},{"location":"developer/user_agent_headers_guide/#setting-user-agent-in-python","title":"Setting User-Agent in Python","text":""},{"location":"developer/user_agent_headers_guide/#basic-example","title":"Basic Example","text":"<pre><code>import requests\n\n# Without User-Agent (might be blocked)\nresponse = requests.get('https://example.com')\n\n# With User-Agent (appears as a browser)\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#advanced-example-with-multiple-headers","title":"Advanced Example with Multiple Headers","text":"<pre><code>headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Accept-Encoding': 'gzip, deflate',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n}\n\nresponse = requests.get('https://example.com', headers=headers)\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#common-user-agent-patterns","title":"Common User-Agent Patterns","text":""},{"location":"developer/user_agent_headers_guide/#desktop-browsers","title":"Desktop Browsers","text":"<pre><code># Windows Chrome\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n\n# macOS Safari\n'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'\n\n# Linux Firefox\n'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#mobile-browsers","title":"Mobile Browsers","text":"<pre><code># iPhone Safari\n'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n\n# Android Chrome\n'Mozilla/5.0 (Linux; Android 11; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#custom-bot-honest-approach","title":"Custom Bot (Honest Approach)","text":"<pre><code>'OPAL-Bot/1.0 (+https://github.com/yourusername/opal)'\n'MyCompany-Scraper/2.1 (contact@mycompany.com)'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#implementing-user-agents-in-opal","title":"Implementing User-Agents in OPAL","text":""},{"location":"developer/user_agent_headers_guide/#enhanced-baseparser","title":"Enhanced BaseParser","text":"<pre><code>def make_request(self, urls: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Shared request functionality for all parsers with proper headers\"\"\"\n\n    # Realistic browser headers\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.9',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'DNT': '1',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n    }\n\n    responses = []\n    successful_urls = []\n\n    for url in urls:\n        try:\n            print(f\"Requesting: {url}\")\n            response = requests.get(url, headers=headers, timeout=5)\n            response.raise_for_status()\n            responses.append(response.text)\n            successful_urls.append(url)\n        except requests.exceptions.RequestException:\n            print(f\"Skipping URL due to error: {url}\")\n            continue\n\n    return responses, successful_urls\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#user-agent-rotation","title":"User-Agent Rotation","text":"<pre><code>import random\n\nclass RotatingUserAgentParser(BaseParser):\n    def __init__(self):\n        super().__init__()\n        self.user_agents = [\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0',\n        ]\n\n    def get_random_user_agent(self):\n        return random.choice(self.user_agents)\n\n    def make_request(self, urls):\n        # Use different User-Agent for each request\n        headers = {'User-Agent': self.get_random_user_agent()}\n        # ... rest of request logic\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#best-practices","title":"Best Practices","text":""},{"location":"developer/user_agent_headers_guide/#1-be-strategic","title":"1. Be Strategic","text":"<ul> <li>Use real User-Agents: Copy from actual browsers</li> <li>Stay current: Browser versions change frequently</li> <li>Match behavior: If you claim to be Chrome, act like Chrome</li> </ul>"},{"location":"developer/user_agent_headers_guide/#2-be-respectful","title":"2. Be Respectful","text":"<ul> <li>Respect robots.txt: Even with a browser User-Agent</li> <li>Rate limit: Don't overwhelm servers</li> <li>Be honest when possible: Some sites appreciate transparent bots</li> </ul>"},{"location":"developer/user_agent_headers_guide/#3-be-consistent","title":"3. Be Consistent","text":"<ul> <li>Use complete headers: Include Accept, Accept-Language, etc.</li> <li>Maintain session: Use the same User-Agent throughout a session</li> <li>Handle responses: Check if the site is behaving differently</li> </ul>"},{"location":"developer/user_agent_headers_guide/#4-be-prepared","title":"4. Be Prepared","text":"<ul> <li>Rotate User-Agents: Avoid detection patterns</li> <li>Handle blocks: Have fallback strategies</li> <li>Monitor changes: Sites may update their detection methods</li> </ul>"},{"location":"developer/user_agent_headers_guide/#user-agent-detection-techniques","title":"User-Agent Detection Techniques","text":"<p>Websites can detect fake User-Agents by:</p> <ol> <li>Header Analysis: Checking if browser behavior matches the User-Agent</li> <li>Missing Headers: Looking for headers real browsers always send</li> <li>JavaScript Testing: Testing browser capabilities that match the claimed version</li> <li>Request Patterns: Analyzing timing and request sequences</li> <li>Feature Detection: Checking for browser-specific features</li> </ol>"},{"location":"developer/user_agent_headers_guide/#common-mistakes","title":"Common Mistakes","text":""},{"location":"developer/user_agent_headers_guide/#1-outdated-user-agents","title":"1. Outdated User-Agents","text":"<pre><code># Bad - very old browser version\n'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 Chrome/45.0.2454.85'\n\n# Good - recent browser version\n'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/91.0.4472.124'\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#2-inconsistent-headers","title":"2. Inconsistent Headers","text":"<pre><code># Bad - claims to be Chrome but uses Firefox Accept header\nheaders = {\n    'User-Agent': 'Chrome/91.0.4472.124',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'  # Firefox style\n}\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#3-missing-common-headers","title":"3. Missing Common Headers","text":"<pre><code># Bad - only User-Agent\nheaders = {'User-Agent': 'Mozilla/5.0...'}\n\n# Good - realistic browser headers\nheaders = {\n    'User-Agent': 'Mozilla/5.0...',\n    'Accept': 'text/html,application/xhtml+xml...',\n    'Accept-Language': 'en-US,en;q=0.9',\n    'Accept-Encoding': 'gzip, deflate, br',\n}\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#testing-user-agents","title":"Testing User-Agents","text":""},{"location":"developer/user_agent_headers_guide/#check-what-youre-sending","title":"Check What You're Sending","text":"<pre><code>import requests\n\n# Test your headers\nresponse = requests.get('https://httpbin.org/headers', headers=your_headers)\nprint(response.json())\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#verify-server-response","title":"Verify Server Response","text":"<pre><code># Check if the site is treating you differently\nresponse_bot = requests.get(url)  # Default requests User-Agent\nresponse_browser = requests.get(url, headers=browser_headers)\n\nif response_bot.content != response_browser.content:\n    print(\"Site serves different content based on User-Agent\")\n</code></pre>"},{"location":"developer/user_agent_headers_guide/#tools-and-resources","title":"Tools and Resources","text":"<ul> <li>Browser DevTools: Copy real User-Agent strings from Network tab</li> <li>User-Agent Databases: Sites like whatismybrowser.com</li> <li>Header Checkers: Use httpbin.org to test your headers</li> <li>Browser Testing: Use Selenium to see what real browsers send</li> </ul>"},{"location":"developer/user_agent_headers_guide/#conclusion","title":"Conclusion","text":"<p>User-Agent headers are a crucial part of web scraping that can mean the difference between successful data extraction and being blocked. Use them thoughtfully and responsibly to build robust scrapers that respect both the technical and ethical aspects of web crawling.</p>"},{"location":"developer/workflows/","title":"Workflows","text":"<p>This document describes the internal workflows and processing patterns used by OPAL parsers. For comprehensive visual diagrams showing the complete data flow, architecture, and decision trees, see Visual Flow Diagrams.</p>"},{"location":"developer/workflows/#overview","title":"Overview","text":"<p>OPAL uses several interconnected workflows:</p> <ul> <li>Parser Selection: Automatic detection based on URL patterns</li> <li>URL Collection: Site-specific pagination and link discovery  </li> <li>Data Extraction: HTML parsing and content extraction</li> <li>Court Searching: Advanced portal navigation and filtering</li> <li>Error Handling: Graceful fallbacks and retry logic</li> </ul>"},{"location":"developer/workflows/#key-workflow-components","title":"Key Workflow Components","text":""},{"location":"developer/workflows/#1-court-id-discovery","title":"1. Court ID Discovery","text":"<p>The CourtSearchBuilder automatically discovers available courts:</p> <ol> <li>Navigation: Load the court portal search page</li> <li>Element Detection: Locate court selection dropdowns  </li> <li>Data Extraction: Parse option elements for court names and IDs</li> <li>Fallback Strategy: Use known court IDs if discovery fails</li> </ol>"},{"location":"developer/workflows/#2-search-url-building","title":"2. Search URL Building","text":"<p>For court extractor operations:</p> <ol> <li>Parameter Validation: Verify court type, dates, filters</li> <li>URL Construction: Build search URLs with proper parameters</li> <li>Session Management: Handle portal session requirements</li> <li>Result Pagination: Generate URLs for all result pages</li> </ol>"},{"location":"developer/workflows/#3-data-processing-pipeline","title":"3. Data Processing Pipeline","text":"<p>For all parser types:</p> <ol> <li>URL Collection: Gather all target URLs for processing</li> <li>Content Extraction: Parse HTML and extract structured data  </li> <li>Data Validation: Verify extracted fields meet requirements</li> <li>Output Generation: Format data as JSON/CSV with timestamps</li> </ol>"},{"location":"developer/workflows/#implementation-details","title":"Implementation Details","text":""},{"location":"developer/workflows/#parser-factory-pattern","title":"Parser Factory Pattern","text":"<pre><code>def get_parser(url, parser_name):\n    \"\"\"Select appropriate parser based on URL and name\"\"\"\n    if 'appealscourts.gov' in url:\n        return ParserAppealsAL()\n    elif '1819news.com' in url:\n        return Parser1819()\n    # ... other parsers\n</code></pre>"},{"location":"developer/workflows/#error-recovery","title":"Error Recovery","text":"<ul> <li>Network Issues: Exponential backoff with retry limits</li> <li>Element Detection: Alternative selector strategies  </li> <li>Browser Crashes: Automatic driver restart</li> <li>Rate Limiting: Adaptive delay mechanisms</li> </ul>"},{"location":"developer/workflows/#visual-references","title":"Visual References","text":"<p>For complete visual representations of these workflows:</p> <ul> <li>\ud83d\udcca Visual Flow Diagrams - Comprehensive system diagrams</li> <li>\ud83d\udd27 Architecture Overview - System components and relationships</li> <li>\u26a0\ufe0f Error Handling - Error recovery strategies</li> </ul>"},{"location":"developer/workflows/#development-guidelines","title":"Development Guidelines","text":"<p>When extending workflows:</p> <ol> <li>Follow the established parser inheritance pattern</li> <li>Implement proper error handling with fallbacks</li> <li>Add appropriate logging for debugging</li> <li>Consider rate limiting for respectful scraping</li> <li>Update visual diagrams when adding new flows</li> </ol>"},{"location":"developer/workflows/#testing-workflows","title":"Testing Workflows","text":"<pre><code># Test court discovery\npython -c \"from opal.configurable_court_extractor import CourtSearchBuilder; CourtSearchBuilder().discover_court_ids()\"\n\n# Test parser selection  \npython -c \"from opal.main import get_parser; print(get_parser('https://1819news.com/', 'Parser1819'))\"\n\n# Validate search URL building\npython -m opal.configurable_court_extractor --court civil --date-period 7d --dry-run\n</code></pre>"},{"location":"getting-started/complete-setup-guide/","title":"Complete Setup Guide","text":"<p>This guide will walk you through setting up OPAL from scratch, even if you're new to Python development. We'll cover everything you need to get started.</p>"},{"location":"getting-started/complete-setup-guide/#quick-install-for-experienced-users","title":"Quick Install (For Experienced Users)","text":"<p>If you're already familiar with Python development:</p> <pre><code># Clone and install\ngit clone https://github.com/alabama-forward/opal_beautifulsoup\ncd opal_beautifulsoup\npip install -e .\n</code></pre> <p>Requirements: Python 3.8+, Google Chrome (for court scraping), Internet connection</p>"},{"location":"getting-started/complete-setup-guide/#before-you-start","title":"Before You Start","text":"<p>\ud83d\udd0d Check Your System First: Run our Prerequisites Checker to verify your system is ready and identify any issues early.</p> <pre><code>python check_prerequisites.py\n</code></pre>"},{"location":"getting-started/complete-setup-guide/#prerequisites-overview","title":"Prerequisites Overview","text":"<p>Before installing OPAL, you'll need: - Python 3.8 or higher - A terminal/command prompt - Internet connection - About 15 minutes</p>"},{"location":"getting-started/complete-setup-guide/#step-1-install-python","title":"Step 1: Install Python","text":"<p>For detailed platform-specific installation instructions, see Environment-Specific Guides.</p>"},{"location":"getting-started/complete-setup-guide/#quick-installation-summary","title":"Quick Installation Summary","text":"<p>Windows: Download from python.org (\u2705 check \"Add Python to PATH\") macOS: Use Homebrew (<code>brew install python3</code>) or download from python.org Linux: Use package manager (<code>sudo apt install python3 python3-pip python3-venv</code>)</p>"},{"location":"getting-started/complete-setup-guide/#verify-installation","title":"Verify Installation","text":"<pre><code># Check Python version (should be 3.8+)\npython --version      # Windows\npython3 --version     # macOS/Linux\n\n# Check pip is available\npip --version         # Windows  \npip3 --version        # macOS/Linux\n</code></pre>"},{"location":"getting-started/complete-setup-guide/#step-2-set-up-your-project-directory","title":"Step 2: Set Up Your Project Directory","text":"<ol> <li> <p>Create a folder for OPAL:    <pre><code># Create project directory\nmkdir opal-project\ncd opal-project\n</code></pre></p> </li> <li> <p>Download OPAL:    <pre><code># Clone repository (recommended)\ngit clone https://github.com/alabama-forward/opal_beautifulsoup .\n\n# OR download ZIP from GitHub and extract here\n</code></pre></p> </li> </ol> <p>Platform-specific commands: See Environment-Specific Guides for detailed OS-specific instructions.</p>"},{"location":"getting-started/complete-setup-guide/#step-3-create-a-virtual-environment","title":"Step 3: Create a Virtual Environment","text":"<p>A virtual environment keeps OPAL's dependencies separate from other Python projects.</p> <pre><code># Create virtual environment\npython -m venv venv          # Windows\npython3 -m venv venv         # macOS/Linux\n\n# Activate virtual environment  \nsource venv/bin/activate     # macOS/Linux\nvenv\\Scripts\\activate        # Windows\n</code></pre> <p>You'll see <code>(venv)</code> appear in your prompt when activated.</p> <p>Troubleshooting: See Environment-Specific Guides for platform-specific activation issues.</p>"},{"location":"getting-started/complete-setup-guide/#step-4-install-opal-and-dependencies","title":"Step 4: Install OPAL and Dependencies","text":"<p>With your virtual environment activated:</p> <ol> <li>Install OPAL:    <pre><code>pip install -e .\n</code></pre></li> </ol> <p>Or if that doesn't work:    <pre><code>pip install -r requirements.txt\n</code></pre></p> <ol> <li>Wait for Installation:</li> <li>This will download and install all necessary packages</li> <li>It may take 2-5 minutes depending on your internet speed</li> </ol>"},{"location":"getting-started/complete-setup-guide/#step-5-set-up-chrome-driver-for-court-scraping","title":"Step 5: Set Up Chrome Driver (for Court Scraping)","text":"<p>OPAL uses Selenium for scraping JavaScript-heavy sites like the Alabama Appeals Court.</p> <ol> <li>Good News: OPAL automatically manages ChromeDriver!</li> <li>The <code>webdriver-manager</code> package handles this for you</li> <li> <p>It will download the correct version when first needed</p> </li> <li> <p>Requirements:</p> </li> <li>You need Google Chrome installed on your computer</li> <li>Download from: google.com/chrome</li> </ol>"},{"location":"getting-started/complete-setup-guide/#step-6-verify-your-installation","title":"Step 6: Verify Your Installation","text":"<p>Let's make sure everything is working:</p> <ol> <li>Test OPAL is installed:    <pre><code>python -m opal --help\n</code></pre></li> </ol> <p>You should see the help menu with available options.</p> <ol> <li>Test with a simple scrape:    <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 1\n</code></pre></li> </ol> <p>This will scrape just one page to verify everything works.</p>"},{"location":"getting-started/complete-setup-guide/#step-7-understanding-the-output","title":"Step 7: Understanding the Output","text":"<p>After running OPAL, you'll find: - A JSON file in your project directory - Named with timestamp: <code>YYYY-MM-DD_ParserName.json</code> - Contains structured data from the scraped content</p>"},{"location":"getting-started/complete-setup-guide/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"getting-started/complete-setup-guide/#issue-python-is-not-recognized-windows","title":"Issue: \"python is not recognized\" (Windows)","text":"<p>Solution: Python wasn't added to PATH during installation 1. Reinstall Python and check \"Add Python to PATH\" 2. Or manually add Python to your system PATH</p>"},{"location":"getting-started/complete-setup-guide/#issue-pip-is-not-recognized","title":"Issue: \"pip is not recognized\"","text":"<p>Solution:  <pre><code># Windows\npython -m ensurepip --upgrade\n\n# macOS/Linux\npython3 -m ensurepip --upgrade\n</code></pre></p>"},{"location":"getting-started/complete-setup-guide/#issue-no-module-named-opal","title":"Issue: \"No module named 'opal'\"","text":"<p>Solution: Make sure you're in the correct directory and virtual environment is activated</p>"},{"location":"getting-started/complete-setup-guide/#issue-chrome-driver-errors","title":"Issue: Chrome driver errors","text":"<p>Solution:  1. Ensure Google Chrome is installed 2. Try updating Chrome to the latest version 3. The webdriver-manager should handle version matching automatically</p>"},{"location":"getting-started/complete-setup-guide/#next-steps","title":"Next Steps","text":"<p>Now that OPAL is installed: 1. Read the Quick Start Tutorial for your first real scraping task 2. Check out Output Examples to understand the data format 3. Explore Common Use Cases</p>"},{"location":"getting-started/complete-setup-guide/#getting-help","title":"Getting Help","text":"<p>If you encounter issues: 1. Check the error message carefully 2. Review the Troubleshooting Guide 3. Ensure your virtual environment is activated (you see <code>(venv)</code>) 4. Try reinstalling in a fresh virtual environment</p> <p>Remember: Every developer started as a beginner. Take it step by step, and you'll be scraping Alabama news and court data in no time!</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>OPAL can be configured through command-line arguments, environment variables, and parser-specific options.</p>"},{"location":"getting-started/configuration/#command-line-arguments","title":"Command-Line Arguments","text":""},{"location":"getting-started/configuration/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>--url</code>: The base URL to scrape</li> <li><code>--parser</code>: The parser to use (Parser1819, ParserDailyNews, court)</li> </ul>"},{"location":"getting-started/configuration/#optional-arguments","title":"Optional Arguments","text":"<ul> <li><code>--suffix</code>: URL suffix for news articles (default: '')</li> <li><code>--max_pages</code>: Maximum number of pages to scrape (default: 5)</li> <li><code>--output</code>: Output file path (default: opal_output.json)</li> <li><code>--log-level</code>: Logging level (DEBUG, INFO, WARNING, ERROR)</li> </ul>"},{"location":"getting-started/configuration/#parser-specific-configuration","title":"Parser-Specific Configuration","text":""},{"location":"getting-started/configuration/#news-parsers-parser1819-parserdailynews","title":"News Parsers (Parser1819, ParserDailyNews)","text":"<ul> <li>Require <code>--suffix</code> parameter for article URLs</li> <li>Support pagination with <code>--max_pages</code></li> </ul>"},{"location":"getting-started/configuration/#court-parser-parserappealsal","title":"Court Parser (ParserAppealsAL)","text":"<p>Basic configuration: - Automatically handles Chrome WebDriver setup - Processes all available court cases - No pagination parameters needed</p> <p>Advanced configuration options: - <code>headless</code> (bool): Run browser in headless mode (default: True) - <code>rate_limit_seconds</code> (int): Delay between requests (default: 3)</p> <pre><code>from opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL(\n    headless=True,              # Run without visible browser\n    rate_limit_seconds=2        # 2 second delay between pages\n)\n</code></pre>"},{"location":"getting-started/configuration/#chrome-webdriver-options","title":"Chrome WebDriver Options","text":"<p>The court parser configures Chrome with these options: - <code>--disable-gpu</code>: Disable GPU hardware acceleration - <code>--no-sandbox</code>: Required for some environments - <code>--disable-dev-shm-usage</code>: Overcome limited resource problems - <code>--window-size=1920,1080</code>: Set browser window size</p> <p>Custom WebDriver options can be set:</p> <pre><code>from selenium import webdriver\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Custom Chrome options would need to be set in the parser's _setup_driver method\n# The parser uses webdriver_manager for automatic ChromeDriver management\n</code></pre>"},{"location":"getting-started/configuration/#configurable-court-extractor","title":"Configurable Court Extractor","text":"<p>The configurable court extractor supports additional parameters through the main function:</p> <pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\nresults = extract_court_cases_with_params(\n    court=\"civil\",              # Court selection\n    date_period=\"1m\",           # Date filtering\n    case_category=\"Appeal\",     # Case type filtering\n    max_pages=10,               # Limit pages processed\n    output_prefix=\"custom\"      # Output file prefix\n)\n</code></pre> <p>See Configurable Court Extractor for detailed configuration options.</p>"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"<p>By default, OPAL outputs data in JSON format to <code>opal_output.json</code>. You can specify a different output file:</p> <pre><code>python -m opal --url https://example.com --parser Parser1819 --output my_data.json\n</code></pre>"},{"location":"getting-started/configuration/#logging","title":"Logging","text":"<p>Control logging verbosity with <code>--log-level</code>:</p> <pre><code>python -m opal --url https://example.com --parser Parser1819 --log-level DEBUG\n</code></pre>"},{"location":"getting-started/environment-guides/","title":"Environment-Specific Setup Guides","text":"<p>This guide provides detailed setup instructions for different operating systems and environments. Choose your platform below for specific instructions.</p>"},{"location":"getting-started/environment-guides/#quick-platform-selection","title":"Quick Platform Selection","text":"<ul> <li>Windows Setup - PowerShell, Command Prompt, PATH configuration</li> <li>macOS Setup - Terminal, Homebrew, security settings</li> <li>Linux Setup - Ubuntu, CentOS, Debian, and others</li> <li>Cloud Environments - Google Colab, GitHub Codespaces</li> </ul>"},{"location":"getting-started/environment-guides/#windows-setup","title":"Windows Setup","text":""},{"location":"getting-started/environment-guides/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have: - Windows 10 or newer - Administrator access (for Python installation) - Internet connection</p>"},{"location":"getting-started/environment-guides/#step-1-install-python","title":"Step 1: Install Python","text":""},{"location":"getting-started/environment-guides/#option-a-from-microsoft-store-easiest","title":"Option A: From Microsoft Store (Easiest)","text":"<ol> <li>Open Microsoft Store</li> <li>Search for \"Python 3.11\" or \"Python 3.10\"</li> <li>Click \"Install\"</li> <li>Python will be automatically added to PATH</li> </ol>"},{"location":"getting-started/environment-guides/#option-b-from-pythonorg-recommended","title":"Option B: From Python.org (Recommended)","text":"<ol> <li>Go to python.org/downloads</li> <li>Download Python 3.8+ for Windows</li> <li>IMPORTANT: Check \"Add Python to PATH\" during installation</li> <li>Choose \"Install Now\"</li> </ol>"},{"location":"getting-started/environment-guides/#verify-installation","title":"Verify Installation","text":"<p>Open Command Prompt or PowerShell and run: <pre><code>python --version\npip --version\n</code></pre></p> <p>Troubleshooting: - If \"python is not recognized\": Reinstall Python with \"Add to PATH\" checked - If PowerShell shows execution policy errors: Run <code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser</code></p>"},{"location":"getting-started/environment-guides/#step-2-create-project-directory","title":"Step 2: Create Project Directory","text":"<pre><code># Create and navigate to project folder\nmkdir C:\\opal-project\ncd C:\\opal-project\n\n# Download OPAL\ngit clone https://github.com/your-repo/opal_beautifulsoup .\n# OR download and extract ZIP file to this folder\n</code></pre>"},{"location":"getting-started/environment-guides/#step-3-set-up-virtual-environment","title":"Step 3: Set Up Virtual Environment","text":"<p>Command Prompt: <pre><code># Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\nvenv\\Scripts\\activate\n\n# You should see (venv) in your prompt\n</code></pre></p> <p>PowerShell: <pre><code># Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\nvenv\\Scripts\\Activate.ps1\n\n# If you get execution policy error:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nvenv\\Scripts\\Activate.ps1\n</code></pre></p>"},{"location":"getting-started/environment-guides/#step-4-install-opal","title":"Step 4: Install OPAL","text":"<pre><code># With virtual environment activated\npip install -e .\n\n# Or install requirements first:\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/environment-guides/#step-5-install-google-chrome","title":"Step 5: Install Google Chrome","text":"<ol> <li>Download from google.com/chrome</li> <li>Install normally</li> <li>Chrome will be automatically detected by OPAL</li> </ol>"},{"location":"getting-started/environment-guides/#step-6-test-installation","title":"Step 6: Test Installation","text":"<pre><code># Check prerequisites\npython check_prerequisites.py\n\n# Test OPAL\npython -m opal --help\n</code></pre>"},{"location":"getting-started/environment-guides/#windows-specific-issues","title":"Windows-Specific Issues","text":""},{"location":"getting-started/environment-guides/#path-problems","title":"PATH Problems","text":"<pre><code># Check if Python is in PATH\nwhere python\n\n# If not found, add manually:\n# 1. Open \"Environment Variables\" in Windows settings\n# 2. Add Python installation path (e.g., C:\\Python39\\)\n# 3. Add Scripts folder (e.g., C:\\Python39\\Scripts\\)\n</code></pre>"},{"location":"getting-started/environment-guides/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<pre><code># If activation fails:\npython -m venv --clear venv\nvenv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/environment-guides/#permission-errors","title":"Permission Errors","text":"<pre><code># Run Command Prompt as Administrator if needed\n# Or use user-level installation:\npip install --user -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#chrome-driver-issues","title":"Chrome Driver Issues","text":"<ul> <li>OPAL automatically manages ChromeDriver</li> <li>Ensure Chrome is updated to latest version</li> <li>Windows Defender may sometimes block automated browsers - add exceptions if needed</li> </ul>"},{"location":"getting-started/environment-guides/#macos-setup","title":"macOS Setup","text":""},{"location":"getting-started/environment-guides/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>macOS 10.15 (Catalina) or newer</li> <li>Terminal access</li> <li>Internet connection</li> </ul>"},{"location":"getting-started/environment-guides/#step-1-install-python_1","title":"Step 1: Install Python","text":""},{"location":"getting-started/environment-guides/#option-a-using-homebrew-recommended","title":"Option A: Using Homebrew (Recommended)","text":"<p>First, install Homebrew if you don't have it: <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> <p>Then install Python: <pre><code># Install Python\nbrew install python3\n\n# Verify installation\npython3 --version\npip3 --version\n</code></pre></p>"},{"location":"getting-started/environment-guides/#option-b-from-pythonorg","title":"Option B: From Python.org","text":"<ol> <li>Download Python 3.8+ from python.org/downloads</li> <li>Install the .pkg file</li> <li>Python will be available as <code>python3</code></li> </ol>"},{"location":"getting-started/environment-guides/#option-c-using-pyenv-for-multiple-versions","title":"Option C: Using pyenv (For Multiple Versions)","text":"<pre><code># Install pyenv\nbrew install pyenv\n\n# Add to your shell profile (.zshrc or .bash_profile):\necho 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' &gt;&gt; ~/.zshrc\necho 'eval \"$(pyenv init --path)\"' &gt;&gt; ~/.zshrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.zshrc\n\n# Restart terminal, then:\npyenv install 3.10.0\npyenv global 3.10.0\n</code></pre>"},{"location":"getting-started/environment-guides/#step-2-create-project-directory_1","title":"Step 2: Create Project Directory","text":"<pre><code># Create and navigate to project folder\nmkdir ~/opal-project\ncd ~/opal-project\n\n# Download OPAL\ngit clone https://github.com/your-repo/opal_beautifulsoup .\n</code></pre>"},{"location":"getting-started/environment-guides/#step-3-set-up-virtual-environment_1","title":"Step 3: Set Up Virtual Environment","text":"<pre><code># Create virtual environment\npython3 -m venv venv\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# You should see (venv) in your prompt\n</code></pre>"},{"location":"getting-started/environment-guides/#step-4-install-opal_1","title":"Step 4: Install OPAL","text":"<pre><code># With virtual environment activated\npip install -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#step-5-install-google-chrome_1","title":"Step 5: Install Google Chrome","text":"<ol> <li>Download from google.com/chrome</li> <li>Drag to Applications folder</li> <li>Open Chrome once to complete setup</li> </ol>"},{"location":"getting-started/environment-guides/#step-6-handle-macos-security","title":"Step 6: Handle macOS Security","text":"<p>When OPAL first runs ChromeDriver:</p> <ol> <li>macOS may show: \"chromedriver cannot be opened because it is from an unidentified developer\"</li> <li>Go to System Preferences \u2192 Security &amp; Privacy</li> <li>Click \"Allow Anyway\" next to the ChromeDriver message</li> <li>Or run: <code>xattr -d com.apple.quarantine /path/to/chromedriver</code></li> </ol>"},{"location":"getting-started/environment-guides/#step-7-test-installation","title":"Step 7: Test Installation","text":"<pre><code># Check prerequisites\npython check_prerequisites.py\n\n# Test OPAL\npython -m opal --help\n</code></pre>"},{"location":"getting-started/environment-guides/#macos-specific-issues","title":"macOS-Specific Issues","text":""},{"location":"getting-started/environment-guides/#command-line-tools","title":"Command Line Tools","text":"<pre><code># If you get compiler errors:\nxcode-select --install\n</code></pre>"},{"location":"getting-started/environment-guides/#homebrew-issues","title":"Homebrew Issues","text":"<pre><code># If brew command not found:\necho 'export PATH=\"/opt/homebrew/bin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"getting-started/environment-guides/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<pre><code># If you get SSL errors:\n/Applications/Python\\ 3.x/Install\\ Certificates.command\n</code></pre>"},{"location":"getting-started/environment-guides/#permission-issues","title":"Permission Issues","text":"<pre><code># If pip install fails with permissions:\npip install --user -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#linux-setup","title":"Linux Setup","text":""},{"location":"getting-started/environment-guides/#ubuntudebian","title":"Ubuntu/Debian","text":""},{"location":"getting-started/environment-guides/#step-1-update-system","title":"Step 1: Update System","text":"<pre><code>sudo apt update\nsudo apt upgrade -y\n</code></pre>"},{"location":"getting-started/environment-guides/#step-2-install-python-and-dependencies","title":"Step 2: Install Python and Dependencies","text":"<pre><code># Install Python 3.8+\nsudo apt install python3 python3-pip python3-venv python3-dev -y\n\n# Install additional dependencies\nsudo apt install build-essential curl git -y\n\n# Verify installation\npython3 --version\npip3 --version\n</code></pre>"},{"location":"getting-started/environment-guides/#step-3-install-google-chrome","title":"Step 3: Install Google Chrome","text":"<pre><code># Download and install Chrome\nwget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\necho \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" | sudo tee /etc/apt/sources.list.d/google-chrome.list\nsudo apt update\nsudo apt install google-chrome-stable -y\n</code></pre>"},{"location":"getting-started/environment-guides/#step-4-set-up-opal","title":"Step 4: Set Up OPAL","text":"<pre><code># Create project directory\nmkdir ~/opal-project\ncd ~/opal-project\n\n# Download OPAL\ngit clone https://github.com/your-repo/opal_beautifulsoup .\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install OPAL\npip install -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#centosrhelfedora","title":"CentOS/RHEL/Fedora","text":""},{"location":"getting-started/environment-guides/#step-1-install-python_2","title":"Step 1: Install Python","text":"<pre><code># CentOS/RHEL 8+\nsudo dnf install python3 python3-pip python3-devel git -y\n\n# CentOS/RHEL 7\nsudo yum install python3 python3-pip python3-devel git -y\n\n# Fedora\nsudo dnf install python3 python3-pip python3-devel git -y\n</code></pre>"},{"location":"getting-started/environment-guides/#step-2-install-google-chrome","title":"Step 2: Install Google Chrome","text":"<pre><code># Add Google Chrome repository\nsudo tee /etc/yum.repos.d/google-chrome.repo &lt;&lt;EOF\n[google-chrome]\nname=google-chrome\nbaseurl=http://dl.google.com/linux/chrome/rpm/stable/x86_64\nenabled=1\ngpgcheck=1\ngpgkey=https://dl.google.com/linux/linux_signing_key.pub\nEOF\n\n# Install Chrome\nsudo dnf install google-chrome-stable -y\n# or: sudo yum install google-chrome-stable -y\n</code></pre>"},{"location":"getting-started/environment-guides/#step-3-set-up-opal","title":"Step 3: Set Up OPAL","text":"<pre><code># Same as Ubuntu steps above\nmkdir ~/opal-project\ncd ~/opal-project\ngit clone https://github.com/your-repo/opal_beautifulsoup .\npython3 -m venv venv\nsource venv/bin/activate\npip install -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#arch-linux","title":"Arch Linux","text":"<pre><code># Install dependencies\nsudo pacman -S python python-pip git google-chrome\n\n# Set up OPAL\nmkdir ~/opal-project\ncd ~/opal-project\ngit clone https://github.com/your-repo/opal_beautifulsoup .\npython -m venv venv\nsource venv/bin/activate\npip install -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#linux-server-headless","title":"Linux Server (Headless)","text":"<p>For servers without GUI:</p> <pre><code># Install Chrome headless\nwget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -\necho \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" | sudo tee /etc/apt/sources.list.d/google-chrome.list\nsudo apt update\nsudo apt install google-chrome-stable -y\n\n# Install virtual display (if needed)\nsudo apt install xvfb -y\n\n# OPAL runs in headless mode by default, so this should work fine\n</code></pre>"},{"location":"getting-started/environment-guides/#linux-specific-issues","title":"Linux-Specific Issues","text":""},{"location":"getting-started/environment-guides/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># If you get build errors:\nsudo apt install build-essential python3-dev libffi-dev libssl-dev\n\n# For CentOS/RHEL:\nsudo dnf groupinstall \"Development Tools\"\nsudo dnf install python3-devel libffi-devel openssl-devel\n</code></pre>"},{"location":"getting-started/environment-guides/#chrome-sandbox-issues","title":"Chrome Sandbox Issues","text":"<pre><code># If Chrome fails to start:\ngoogle-chrome --no-sandbox --disable-dev-shm-usage --headless --version\n</code></pre>"},{"location":"getting-started/environment-guides/#virtual-environment-issues_1","title":"Virtual Environment Issues","text":"<pre><code># If venv creation fails:\nsudo apt install python3-venv\n# or\npip3 install virtualenv\nvirtualenv venv\n</code></pre>"},{"location":"getting-started/environment-guides/#cloud-environments","title":"Cloud Environments","text":""},{"location":"getting-started/environment-guides/#google-colab","title":"Google Colab","text":"<p>Google Colab provides a pre-configured Python environment. Here's how to set up OPAL:</p>"},{"location":"getting-started/environment-guides/#step-1-install-dependencies","title":"Step 1: Install Dependencies","text":"<pre><code># In a Colab cell:\n!pip install requests beautifulsoup4 selenium webdriver-manager\n\n# Install Chrome (already available in Colab)\n!apt-get update\n!apt install chromium-chromedriver\n</code></pre>"},{"location":"getting-started/environment-guides/#step-2-download-opal","title":"Step 2: Download OPAL","text":"<pre><code># Clone OPAL repository\n!git clone https://github.com/your-repo/opal_beautifulsoup\n%cd opal_beautifulsoup\n\n# Install OPAL\n!pip install -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#step-3-configure-for-colab","title":"Step 3: Configure for Colab","text":"<pre><code># Set up Chrome options for Colab\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\n# OPAL automatically handles headless mode, but you can verify:\nimport os\nos.environ['DISPLAY'] = ':99'  # Virtual display\n</code></pre>"},{"location":"getting-started/environment-guides/#step-4-test-opal","title":"Step 4: Test OPAL","text":"<pre><code># Check prerequisites\n!python check_prerequisites.py\n\n# Test basic functionality\n!python -m opal --help\n</code></pre>"},{"location":"getting-started/environment-guides/#colab-specific-notes","title":"Colab-Specific Notes","text":"<ul> <li>Chrome runs in headless mode (no GUI)</li> <li>Files are saved to Colab's temporary storage</li> <li>Download results before session ends</li> <li>Runtime may reset after inactivity</li> </ul>"},{"location":"getting-started/environment-guides/#github-codespaces","title":"GitHub Codespaces","text":""},{"location":"getting-started/environment-guides/#step-1-create-codespace","title":"Step 1: Create Codespace","text":"<ol> <li>Go to your OPAL repository on GitHub</li> <li>Click \"Code\" \u2192 \"Codespaces\" \u2192 \"Create codespace\"</li> <li>Wait for environment to load</li> </ol>"},{"location":"getting-started/environment-guides/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<pre><code># Codespaces comes with Python pre-installed\npython --version\n\n# Install Chrome\nsudo apt update\nsudo apt install google-chrome-stable -y\n\n# Set up OPAL\npip install -e .\n</code></pre>"},{"location":"getting-started/environment-guides/#step-3-configure-environment","title":"Step 3: Configure Environment","text":"<pre><code># Check prerequisites\npython check_prerequisites.py\n\n# Test OPAL\npython -m opal --help\n</code></pre>"},{"location":"getting-started/environment-guides/#codespaces-specific-notes","title":"Codespaces-Specific Notes","text":"<ul> <li>Pre-configured development environment</li> <li>Persistent storage within the codespace</li> <li>Chrome runs in headless mode</li> <li>Good for development and testing</li> </ul>"},{"location":"getting-started/environment-guides/#docker","title":"Docker","text":"<p>For containerized deployment:</p>"},{"location":"getting-started/environment-guides/#dockerfile-example","title":"Dockerfile Example","text":"<pre><code>FROM python:3.9-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    wget \\\n    gnupg \\\n    unzip \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Chrome\nRUN wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - \\\n    &amp;&amp; echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" &gt;&gt; /etc/apt/sources.list.d/google-chrome.list \\\n    &amp;&amp; apt-get update \\\n    &amp;&amp; apt-get install -y google-chrome-stable \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Set up Python environment\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nRUN pip install -e .\n\n# Run OPAL\nCMD [\"python\", \"-m\", \"opal\", \"--help\"]\n</code></pre>"},{"location":"getting-started/environment-guides/#building-and-running","title":"Building and Running","text":"<pre><code># Build container\ndocker build -t opal .\n\n# Run container\ndocker run -v $(pwd)/output:/app/output opal python -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 2\n</code></pre>"},{"location":"getting-started/environment-guides/#environment-verification","title":"Environment Verification","text":"<p>After setup on any platform, run these verification steps:</p>"},{"location":"getting-started/environment-guides/#1-prerequisites-check","title":"1. Prerequisites Check","text":"<pre><code>python check_prerequisites.py\n</code></pre>"},{"location":"getting-started/environment-guides/#2-basic-functionality-test","title":"2. Basic Functionality Test","text":"<pre><code># Test help command\npython -m opal --help\n\n# Test small scrape\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 1\n</code></pre>"},{"location":"getting-started/environment-guides/#3-court-scraping-test","title":"3. Court Scraping Test","text":"<pre><code># Test court functionality (uses Chrome)\npython -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court --max_pages 1\n</code></pre>"},{"location":"getting-started/environment-guides/#4-check-output","title":"4. Check Output","text":"<pre><code># List generated files\nls -la *.json\n\n# View file contents\ncat *Parser1819*.json | head -20\n</code></pre>"},{"location":"getting-started/environment-guides/#getting-help","title":"Getting Help","text":"<p>If you encounter issues specific to your environment:</p> <ol> <li>Run the prerequisites checker - It will identify most platform-specific issues</li> <li>Check the main setup guide - Complete Setup Guide for general instructions</li> <li>Review error solutions - Understanding Errors for troubleshooting</li> <li>Platform-specific forums - Each OS has community forums for technical support</li> </ol> <p>Most setup issues are related to: - Python not being in PATH - Virtual environment not activated - Missing permissions - Chrome/ChromeDriver configuration</p> <p>The environment-specific instructions above address the most common issues for each platform!</p>"},{"location":"getting-started/prerequisites-checker/","title":"Prerequisites Checker","text":"<p>Before installing OPAL, run our prerequisites checker to verify your system is ready. This tool will identify potential issues and provide specific solutions.</p>"},{"location":"getting-started/prerequisites-checker/#quick-check","title":"Quick Check","text":"<p>Run this single command to check your system:</p> <pre><code>python check_prerequisites.py\n</code></pre> <p>The checker will automatically verify all requirements and give you a detailed report.</p>"},{"location":"getting-started/prerequisites-checker/#what-it-checks","title":"What It Checks","text":""},{"location":"getting-started/prerequisites-checker/#python-environment","title":"\u2705 Python Environment","text":"<ul> <li>Python Version: Ensures you have Python 3.8+ (required for OPAL)</li> <li>Package Installer (pip): Verifies pip is available for installing packages</li> <li>Virtual Environment: Checks if you're using a virtual environment (recommended)</li> </ul>"},{"location":"getting-started/prerequisites-checker/#required-packages","title":"\u2705 Required Packages","text":"<ul> <li>requests: HTTP library for web requests</li> <li>beautifulsoup4: HTML parsing library</li> <li>selenium: Web browser automation for court scraping</li> <li>webdriver-manager: Automatic ChromeDriver management</li> </ul>"},{"location":"getting-started/prerequisites-checker/#browser-requirements","title":"\u2705 Browser Requirements","text":"<ul> <li>Google Chrome: Required for court scraping functionality</li> <li>ChromeDriver Management: Verifies automatic driver setup will work</li> </ul>"},{"location":"getting-started/prerequisites-checker/#network-connectivity","title":"\u2705 Network Connectivity","text":"<p>Tests connections to: - 1819news.com - Alabama Daily News - Alabama Appeals Court portal - Python Package Index (PyPI)</p>"},{"location":"getting-started/prerequisites-checker/#system-resources","title":"\u2705 System Resources","text":"<ul> <li>File Permissions: Can write output files</li> <li>Memory (RAM): Sufficient for scraping operations</li> <li>Disk Space: Available for storing scraped data</li> </ul>"},{"location":"getting-started/prerequisites-checker/#understanding-the-results","title":"Understanding the Results","text":""},{"location":"getting-started/prerequisites-checker/#passed-checks","title":"\ud83d\udfe2 Passed Checks","text":"<pre><code>\u2705 Python Version: Python 3.9.7 (compatible)\n\u2705 Package Installer (pip): pip 21.3.1 available\n\u2705 Google Chrome: Found at /Applications/Google Chrome.app\n</code></pre> <p>These items are working correctly and ready for OPAL.</p>"},{"location":"getting-started/prerequisites-checker/#warnings","title":"\ud83d\udfe1 Warnings","text":"<pre><code>\u26a0\ufe0f  Virtual Environment: Not running in virtual environment\n   \ud83d\udca1 Fix: Create virtual environment: python -m venv venv &amp;&amp; source venv/bin/activate\n\n\u26a0\ufe0f  requests: Not installed (HTTP library for web requests)\n   \ud83d\udca1 Fix: Install with: pip install requests\n</code></pre> <p>Warnings indicate items that should be addressed but won't prevent OPAL from working.</p>"},{"location":"getting-started/prerequisites-checker/#failed-checks","title":"\ud83d\udd34 Failed Checks","text":"<pre><code>\u274c Python Version: Python 2.7.18 (incompatible)\n   \ud83d\udca1 Fix: Install Python 3.8 or higher from https://python.org\n\n\u274c Google Chrome: Google Chrome not found\n   \ud83d\udca1 Fix: Install Chrome from https://google.com/chrome for court scraping functionality\n</code></pre> <p>Failed checks must be fixed before OPAL will work properly.</p>"},{"location":"getting-started/prerequisites-checker/#sample-output","title":"Sample Output","text":"<p>Here's what a typical check looks like:</p> <pre><code>============================================================\nOPAL Prerequisites Checker\n============================================================\n\n\ud83d\udccb Python Environment\n------------------\n\u2705 Python Version: Python 3.9.7 (compatible)\n\n\u2705 Package Installer (pip): pip 21.3.1 available\n\n\u26a0\ufe0f  Virtual Environment: Not running in virtual environment\n   \ud83d\udca1 Fix: Create virtual environment: python -m venv venv &amp;&amp; source venv/bin/activate\n\n\ud83d\udccb Required Packages\n-----------------\n\u26a0\ufe0f  requests: Not installed (HTTP library for web requests)\n   \ud83d\udca1 Fix: Install with: pip install requests\n\n\u26a0\ufe0f  beautifulsoup4: Not installed (HTML parsing library)\n   \ud83d\udca1 Fix: Install with: pip install beautifulsoup4\n\n\u26a0\ufe0f  selenium: Not installed (Web browser automation)\n   \ud83d\udca1 Fix: Install with: pip install selenium\n\n\u26a0\ufe0f  webdriver-manager: Not installed (Automatic ChromeDriver management)\n   \ud83d\udca1 Fix: Install with: pip install webdriver-manager\n\n\ud83d\udccb Browser Requirements\n--------------------\n\u2705 Google Chrome: Found at /Applications/Google Chrome.app/Contents/MacOS/Google Chrome\n\n\u2705 ChromeDriver Management: webdriver-manager will handle ChromeDriver automatically\n\n\ud83d\udccb Network Connectivity\n---------------------\n\u2705 Connection to 1819news.com: Successfully connected (200)\n\n\u2705 Connection to Alabama Daily News: Successfully connected (200)\n\n\u2705 Connection to Alabama Appeals Court: Successfully connected (200)\n\n\u2705 Connection to Python Package Index: Successfully connected (200)\n\n\ud83d\udccb File System Permissions\n------------------------\n\u2705 Write Permissions: Can write files in current directory\n\n\u2705 Home Directory Access: Can write to home directory\n\n\ud83d\udccb System Resources\n-----------------\n\u2705 Memory (RAM): 16.0 GB available\n\n\u2705 Disk Space: 45.2 GB free space available\n\n\ud83d\udcca Summary\n============================================================\nTotal checks: 14\n\u2705 Passed: 8\n\u26a0\ufe0f  Warnings: 6\n\u274c Failed: 0\n\n\u2705 Good to go with minor warnings!\n\nYour system should work with OPAL, but consider addressing the warnings above.\n\nNext steps:\n1. Install OPAL: pip install -e .\n2. Try the quick start tutorial\n\n\ud83d\udcda For help with setup:\n- Complete Setup Guide: docs/getting-started/complete-setup-guide.md\n- Environment-specific guides: docs/getting-started/environment-guides.md\n- Troubleshooting: docs/user-guide/understanding-errors.md\n\nHappy scraping! \ud83d\ude80\n</code></pre>"},{"location":"getting-started/prerequisites-checker/#common-issues-and-fixes","title":"Common Issues and Fixes","text":""},{"location":"getting-started/prerequisites-checker/#python-version-issues","title":"Python Version Issues","text":"<p>Problem: \"Python 2.7.18 (incompatible)\" Solution: Install Python 3.8+ from python.org</p> <p>Problem: \"Python 3.6.9 (minimum supported, but 3.8+ recommended)\" Solution: Consider upgrading to Python 3.8+ for best compatibility</p>"},{"location":"getting-started/prerequisites-checker/#package-installation-issues","title":"Package Installation Issues","text":"<p>Problem: \"pip not found\" Solution: Install pip with <code>python -m ensurepip --upgrade</code></p> <p>Problem: Multiple package warnings Solution: These will be automatically installed when you run <code>pip install -e .</code></p>"},{"location":"getting-started/prerequisites-checker/#browser-issues","title":"Browser Issues","text":"<p>Problem: \"Google Chrome not found\" Solution: Install Chrome from google.com/chrome</p> <p>Problem: ChromeDriver issues Solution: The webdriver-manager package handles this automatically</p>"},{"location":"getting-started/prerequisites-checker/#network-issues","title":"Network Issues","text":"<p>Problem: Connection failures to websites Solution:  - Check your internet connection - Try again later (websites may be temporarily down) - Use a VPN if you're in a restricted network</p>"},{"location":"getting-started/prerequisites-checker/#permission-issues","title":"Permission Issues","text":"<p>Problem: \"Cannot write to current directory\" Solution:  - Run from a directory where you have write permissions - On Linux/Mac: <code>chmod 755 .</code> to fix permissions - On Windows: Run as administrator if needed</p>"},{"location":"getting-started/prerequisites-checker/#when-to-run-the-checker","title":"When to Run the Checker","text":""},{"location":"getting-started/prerequisites-checker/#before-first-installation","title":"Before First Installation","text":"<p>Run the checker before installing OPAL to catch issues early:</p> <pre><code># Download OPAL\ngit clone https://github.com/your-repo/opal_beautifulsoup\ncd opal_beautifulsoup\n\n# Check prerequisites first\npython check_prerequisites.py\n\n# Then install if checks pass\npip install -e .\n</code></pre>"},{"location":"getting-started/prerequisites-checker/#after-system-changes","title":"After System Changes","text":"<p>Re-run the checker if you: - Update Python - Change virtual environments - Update your operating system - Move OPAL to a different computer</p>"},{"location":"getting-started/prerequisites-checker/#troubleshooting-issues","title":"Troubleshooting Issues","text":"<p>If OPAL isn't working properly, run the checker to identify potential causes:</p> <pre><code>python check_prerequisites.py\n</code></pre>"},{"location":"getting-started/prerequisites-checker/#advanced-usage","title":"Advanced Usage","text":""},{"location":"getting-started/prerequisites-checker/#verbose-output","title":"Verbose Output","text":"<p>For more detailed information during checks:</p> <pre><code>python check_prerequisites.py --verbose\n</code></pre>"},{"location":"getting-started/prerequisites-checker/#save-results-to-file","title":"Save Results to File","text":"<p>To save the check results for troubleshooting:</p> <pre><code>python check_prerequisites.py &gt; system_check.txt 2&gt;&amp;1\n</code></pre>"},{"location":"getting-started/prerequisites-checker/#cicd-integration","title":"CI/CD Integration","text":"<p>Use in automated testing:</p> <pre><code># Exit code 0 = all critical checks passed\n# Exit code 1 = critical issues found\npython check_prerequisites.py\nif [ $? -eq 0 ]; then\n    echo \"System ready for OPAL\"\n    pip install -e .\nelse\n    echo \"Prerequisites not met\"\n    exit 1\nfi\n</code></pre>"},{"location":"getting-started/prerequisites-checker/#getting-help","title":"Getting Help","text":"<p>If the prerequisites checker shows issues you can't resolve:</p> <ol> <li>Check the suggested fixes - Each failed check includes specific solutions</li> <li>Review the setup guide - Complete Setup Guide has detailed instructions</li> <li>Check environment-specific guides - Environment Guides for your OS</li> <li>Look for error solutions - Understanding Errors for common problems</li> </ol> <p>The prerequisites checker is designed to catch 95% of setup issues before they cause problems. Running it first will save you time and frustration during installation!</p>"},{"location":"getting-started/quickstart-tutorial/","title":"Quick Start Tutorial","text":"<p>Welcome! This hands-on tutorial will walk you through your first OPAL scraping tasks. By the end, you'll have successfully scraped both news articles and court cases.</p>"},{"location":"getting-started/quickstart-tutorial/#before-you-begin","title":"Before You Begin","text":"<p>Make sure you've completed the Complete Setup Guide. You should have: - \u2705 Python installed - \u2705 Virtual environment activated (you see <code>(venv)</code> in your terminal) - \u2705 OPAL installed - \u2705 Google Chrome installed (for court scraping)</p>"},{"location":"getting-started/quickstart-tutorial/#tutorial-1-your-first-news-scrape","title":"Tutorial 1: Your First News Scrape","text":"<p>Let's start by scraping a few articles from 1819 News.</p>"},{"location":"getting-started/quickstart-tutorial/#step-1-understand-the-command","title":"Step 1: Understand the Command","text":"<p>Here's what we'll run: <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 2\n</code></pre></p> <p>Let's break this down: - <code>python -m opal</code> - Runs OPAL as a module - <code>--url https://1819news.com/</code> - The website to scrape - <code>--parser Parser1819</code> - Which parser to use (specific to this news site) - <code>--suffix /news/item</code> - Only scrape URLs containing this pattern (helps identify articles) - <code>--max_pages 2</code> - Limit to 2 pages (keeps it quick for testing)</p>"},{"location":"getting-started/quickstart-tutorial/#step-2-run-the-command","title":"Step 2: Run the Command","text":"<ol> <li>Make sure your virtual environment is activated</li> <li>Copy and paste the command above</li> <li>Press Enter</li> </ol>"},{"location":"getting-started/quickstart-tutorial/#step-3-what-youll-see","title":"Step 3: What You'll See","text":"<pre><code>Starting OPAL web scraper...\nUsing Parser1819 for https://1819news.com/\nCollecting article URLs...\nFound 15 article URLs\nProcessing articles...\n[1/15] Scraping: https://1819news.com/news/item/...\n[2/15] Scraping: https://1819news.com/news/item/...\n...\nScraping complete!\nOutput saved to: 2024-01-15_Parser1819.json\nTotal articles scraped: 15\n</code></pre>"},{"location":"getting-started/quickstart-tutorial/#step-4-check-your-output","title":"Step 4: Check Your Output","text":"<ol> <li>Look in your project folder for a file like <code>2024-01-15_Parser1819.json</code></li> <li>Open it with a text editor</li> <li>You'll see structured data for each article</li> </ol> <p>Tip: If the file looks messy, try opening it in a web browser for better formatting!</p>"},{"location":"getting-started/quickstart-tutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>\"No articles found\" - The website might have changed its structure - Try without the <code>--suffix</code> parameter - Check if the website is accessible in your browser</p> <p>\"Connection error\" - Check your internet connection - The website might be temporarily down - Try again in a few minutes</p>"},{"location":"getting-started/quickstart-tutorial/#tutorial-2-scraping-court-cases","title":"Tutorial 2: Scraping Court Cases","text":"<p>Now let's scrape some court case data. This uses Selenium, so it might take a bit longer.</p>"},{"location":"getting-started/quickstart-tutorial/#step-1-basic-court-scrape","title":"Step 1: Basic Court Scrape","text":"<p>Run this command: <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court --max_pages 2\n</code></pre></p> <p>Parameters explained: - <code>--parser court</code> - Uses the court-specific parser - <code>--max_pages 2</code> - Limits to 2 pages of results</p>"},{"location":"getting-started/quickstart-tutorial/#step-2-what-happens-during-court-scraping","title":"Step 2: What Happens During Court Scraping","text":"<p>You'll see: <pre><code>Starting OPAL web scraper...\nInitializing Chrome browser (headless mode)...\nLoading court portal...\nWaiting for page to render...\nFound 30 cases per page\nProcessing page 1 of 2...\nProcessing page 2 of 2...\nSaving results...\nOutput saved to: 2024-01-15_court_cases.json\nCSV output saved to: 2024-01-15_143022_court_cases_all.csv\nTotal cases scraped: 60\n</code></pre></p> <p>Note: Court scraping is slower because: - It launches a real Chrome browser (in hidden mode) - It waits for JavaScript to load - The court website has rate limiting</p>"},{"location":"getting-started/quickstart-tutorial/#step-3-check-both-output-files","title":"Step 3: Check Both Output Files","text":"<p>You now have two files: 1. JSON file: Complete data with all details 2. CSV file: Same data in spreadsheet format</p> <p>Open the CSV file in Excel to see a nice table of court cases!</p>"},{"location":"getting-started/quickstart-tutorial/#tutorial-3-advanced-court-search","title":"Tutorial 3: Advanced Court Search","text":"<p>Let's search for specific types of cases using the configurable extractor.</p>"},{"location":"getting-started/quickstart-tutorial/#step-1-search-recent-civil-appeals","title":"Step 1: Search Recent Civil Appeals","text":"<pre><code>python -m opal.configurable_court_extractor --court civil --date-period 7d --exclude-closed\n</code></pre> <p>This searches for: - Civil court cases only - Filed in the last 7 days - Excluding closed cases</p>"},{"location":"getting-started/quickstart-tutorial/#step-2-understanding-date-periods","title":"Step 2: Understanding Date Periods","text":"<p>You can use these date period options: - <code>7d</code> - Last 7 days - <code>1m</code> - Last month - <code>3m</code> - Last 3 months - <code>6m</code> - Last 6 months - <code>1y</code> - Last year</p>"},{"location":"getting-started/quickstart-tutorial/#step-3-custom-date-range","title":"Step 3: Custom Date Range","text":"<p>For specific dates: <pre><code>python -m opal.configurable_court_extractor --court criminal --start-date 2024-01-01 --end-date 2024-01-15\n</code></pre></p>"},{"location":"getting-started/quickstart-tutorial/#understanding-success","title":"Understanding Success","text":"<p>You know your scrape was successful when: 1. \u2705 No error messages appear 2. \u2705 Output files are created 3. \u2705 The files contain data (not empty) 4. \u2705 File sizes are reasonable (&gt;1KB)</p>"},{"location":"getting-started/quickstart-tutorial/#practice-exercises","title":"Practice Exercises","text":"<p>Try these on your own:</p>"},{"location":"getting-started/quickstart-tutorial/#exercise-1-scrape-more-articles","title":"Exercise 1: Scrape More Articles","text":"<p><pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 5\n</code></pre> - How many articles did you get? - How long did it take?</p>"},{"location":"getting-started/quickstart-tutorial/#exercise-2-different-news-source","title":"Exercise 2: Different News Source","text":"<p><pre><code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 3\n</code></pre> - Compare the output structure - Are the fields the same?</p>"},{"location":"getting-started/quickstart-tutorial/#exercise-3-search-specific-court-cases","title":"Exercise 3: Search Specific Court Cases","text":"<p><pre><code>python -m opal.configurable_court_extractor --court supreme --date-period 1m --max-pages 3\n</code></pre> - How many Supreme Court cases did you find? - What classifications do you see?</p>"},{"location":"getting-started/quickstart-tutorial/#whats-next","title":"What's Next?","text":"<p>Congratulations! You've successfully: - \u2705 Scraped news articles from two sources - \u2705 Extracted court case data - \u2705 Used advanced search parameters - \u2705 Generated both JSON and CSV output</p>"},{"location":"getting-started/quickstart-tutorial/#next-steps","title":"Next Steps:","text":"<ol> <li>Review the Output Examples to better understand your data</li> <li>Learn about Common Use Cases</li> <li>Set up Automated Daily Scraping</li> </ol>"},{"location":"getting-started/quickstart-tutorial/#pro-tips","title":"Pro Tips:","text":"<ul> <li>Start with small <code>--max_pages</code> values while learning</li> <li>Always check the first few results before scraping everything</li> <li>Save important command variations in a text file for reuse</li> <li>Court scraping is slower - be patient!</li> </ul>"},{"location":"getting-started/quickstart-tutorial/#need-help","title":"Need Help?","text":"<p>If something isn't working: 1. Make sure your virtual environment is activated 2. Check your internet connection 3. Verify the website is accessible in your browser 4. Review error messages carefully - they often explain the issue 5. Try with <code>--max_pages 1</code> first to isolate problems</p> <p>Remember: Every expert was once a beginner. Keep experimenting, and you'll be a pro in no time!</p>"},{"location":"reference/base-parser/","title":"BaseParser API Reference","text":"<p>The <code>BaseParser</code> class provides the foundation for all OPAL parsers.</p>"},{"location":"reference/base-parser/#class-definition","title":"Class Definition","text":"<pre><code>class BaseParser:\n    def __init__(self, url, suffix=\"\", max_pages=5)\n</code></pre>"},{"location":"reference/base-parser/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str required Base URL to scrape <code>suffix</code> str <code>\"\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"reference/base-parser/#attributes","title":"Attributes","text":""},{"location":"reference/base-parser/#headers","title":"headers","text":"<p><pre><code>self.headers = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n}\n</code></pre> HTTP headers for requests</p>"},{"location":"reference/base-parser/#logger","title":"logger","text":"<p><pre><code>self.logger: logging.Logger\n</code></pre> Logger instance for debugging</p>"},{"location":"reference/base-parser/#methods","title":"Methods","text":""},{"location":"reference/base-parser/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Main method to extract articles. Must be implemented by subclasses.</p> <p>Returns: List of dictionaries containing article data</p>"},{"location":"reference/base-parser/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extract article URLs from a page. Must be implemented by subclasses.</p> <p>Parameters: - <code>page_url</code>: URL of the page to extract links from</p> <p>Returns: List of article URLs</p>"},{"location":"reference/base-parser/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parse individual article data. Must be implemented by subclasses.</p> <p>Parameters: - <code>article_url</code>: URL of the article to parse</p> <p>Returns: Dictionary with article data</p>"},{"location":"reference/base-parser/#save_to_json","title":"save_to_json()","text":"<p><pre><code>def save_to_json(self, data: List[Dict], filename: str = \"opal_output.json\")\n</code></pre> Save extracted data to JSON file.</p> <p>Parameters: - <code>data</code>: List of article dictionaries - <code>filename</code>: Output filename</p>"},{"location":"reference/base-parser/#usage-example","title":"Usage Example","text":"<pre><code>from opal.BaseParser import BaseParser\n\nclass MyParser(BaseParser):\n    def __init__(self, url, suffix=\"\", max_pages=5):\n        super().__init__(url, suffix, max_pages)\n        self.name = \"MyParser\"\n\n    def extract_article_data(self):\n        # Implementation\n        pass\n\n    def get_article_links(self, page_url):\n        # Implementation\n        pass\n\n    def parse_article(self, article_url):\n        # Implementation\n        pass\n\n# Use the parser\nparser = MyParser(\"https://example.com\", suffix=\"/articles\", max_pages=10)\ndata = parser.extract_article_data()\nparser.save_to_json(data)\n</code></pre>"},{"location":"reference/court-extractor-classes/","title":"Configurable Court Extractor Class Reference","text":"<p>This document provides the complete class reference for the Configurable Court Extractor, including class definitions, method signatures, and usage examples.</p>"},{"location":"reference/court-extractor-classes/#overview","title":"Overview","text":"<p>The Configurable Court Extractor provides a flexible interface for searching and extracting court case data from the Alabama Appeals Court portal with various filtering options.</p>"},{"location":"reference/court-extractor-classes/#classes","title":"Classes","text":""},{"location":"reference/court-extractor-classes/#courtsearchbuilder","title":"CourtSearchBuilder","text":"<p>A builder class that constructs URLs for searching the Alabama Appeals Court portal with configurable parameters.</p> <pre><code>class CourtSearchBuilder:\n    \"\"\"\n    Builder class for constructing court search URLs with configurable parameters.\n\n    This class provides a flexible interface for building search queries for the\n    Alabama Appeals Court portal with various filtering options.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the search builder with default values.\"\"\"\n        self.base_url = \"https://publicportal.alappeals.gov/portal/search/case/results\"\n        self.params = {}\n\n        # Court type mapping with their IDs\n        self.courts = {\n            'civil': 'ac1e3f4f-c03f-4fea-ad09-b83a0501c99f',\n            'criminal': '2390512e-59b3-481a-8d15-a4377c12e5e0'\n        }\n\n        # Predefined date periods\n        self.date_periods = {\n            '1d': 1, '7d': 7, '1m': 30, '3m': 90, '6m': 180, '1y': 365\n        }\n</code></pre>"},{"location":"reference/court-extractor-classes/#methods","title":"Methods","text":""},{"location":"reference/court-extractor-classes/#set_courtcourt_type-str-courtsearchbuilder","title":"set_court(court_type: str) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_court(self, court_type: str) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set the court type to search.\n\n    Args:\n        court_type: Either 'civil', 'criminal', or 'all'\n\n    Returns:\n        Self for method chaining\n\n    Raises:\n        ValueError: If invalid court type provided\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#set_case_numbercase_number-str-courtsearchbuilder","title":"set_case_number(case_number: str) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_case_number(self, case_number: str) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set a specific case number to search for.\n\n    Args:\n        case_number: The case number (e.g., \"2024-CA-001\")\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#set_case_categorycategory-str-courtsearchbuilder","title":"set_case_category(category: str) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_case_category(self, category: str) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set the case category to filter by.\n\n    Common categories:\n    - CV (Civil)\n    - DV (Domestic Violence)\n    - MC (Municipal Court)\n    - PR (Probate)\n    - SM (Small Claims)\n    - TP (Traffic/Parking)\n\n    Args:\n        category: The case category code\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#set_date_rangestart_date-str-none-end_date-str-none-courtsearchbuilder","title":"set_date_range(start_date: str = None, end_date: str = None) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_date_range(self, start_date: str = None, end_date: str = None) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set a custom date range for filed dates.\n\n    Args:\n        start_date: Start date in MM/DD/YYYY format\n        end_date: End date in MM/DD/YYYY format\n\n    Returns:\n        Self for method chaining\n\n    Example:\n        builder.set_date_range(\"01/01/2024\", \"12/31/2024\")\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#set_date_periodperiod-str-courtsearchbuilder","title":"set_date_period(period: str) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_date_period(self, period: str) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set a predefined date period counting back from today.\n\n    Args:\n        period: One of '1d', '7d', '1m', '3m', '6m', '1y'\n\n    Returns:\n        Self for method chaining\n\n    Raises:\n        ValueError: If invalid period provided\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#set_case_titletitle-str-courtsearchbuilder","title":"set_case_title(title: str) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_case_title(self, title: str) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set a case title to search for (partial match).\n\n    Args:\n        title: Part of the case title to search\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#set_case_statusstatus-str-courtsearchbuilder","title":"set_case_status(status: str) -&gt; 'CourtSearchBuilder'","text":"<pre><code>def set_case_status(self, status: str) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Set the case status to filter by.\n\n    Common statuses:\n    - OPEN\n    - CLOSED\n    - PENDING\n    - DISMISSED\n\n    Args:\n        status: The case status\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#exclude_closed_cases-courtsearchbuilder","title":"exclude_closed_cases() -&gt; 'CourtSearchBuilder'","text":"<pre><code>def exclude_closed_cases(self) -&gt; 'CourtSearchBuilder':\n    \"\"\"\n    Exclude closed cases from results.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#build-str","title":"build() -&gt; str","text":"<pre><code>def build(self) -&gt; str:\n    \"\"\"\n    Build the final URL with all configured parameters.\n\n    Returns:\n        Complete URL with encoded parameters\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#get_params-dict","title":"get_params() -&gt; dict","text":"<pre><code>def get_params(self) -&gt; dict:\n    \"\"\"\n    Get the current parameters dictionary.\n\n    Returns:\n        Dictionary of current search parameters\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#functions","title":"Functions","text":""},{"location":"reference/court-extractor-classes/#extract_court_cases_with_params","title":"extract_court_cases_with_params","text":"<p>Main extraction function that uses the builder to fetch and parse court cases.</p> <pre><code>def extract_court_cases_with_params(\n    court_type: str = 'all',\n    case_number: str = None,\n    case_category: str = None,\n    filed_after: str = None,\n    filed_before: str = None,\n    date_period: str = None,\n    case_title: str = None,\n    case_status: str = None,\n    exclude_closed: bool = False,\n    output_csv: bool = False,\n    debug: bool = False\n) -&gt; dict:\n    \"\"\"\n    Extract court cases with configurable search parameters.\n\n    Args:\n        court_type: 'civil', 'criminal', or 'all'\n        case_number: Specific case number to search\n        case_category: Case category (CV, DV, MC, etc.)\n        filed_after: Start date (MM/DD/YYYY)\n        filed_before: End date (MM/DD/YYYY)\n        date_period: Predefined period ('1d', '7d', '1m', '3m', '6m', '1y')\n        case_title: Partial case title to search\n        case_status: Case status filter\n        exclude_closed: Whether to exclude closed cases\n        output_csv: Whether to also save as CSV\n        debug: Whether to print debug information\n\n    Returns:\n        Dictionary containing:\n            - status: 'success' or 'error'\n            - total_cases: Number of cases found\n            - cases: List of case dictionaries\n            - extraction_date: When data was extracted\n            - search_params: Parameters used for search\n            - files_created: List of output files created\n    \"\"\"\n</code></pre>"},{"location":"reference/court-extractor-classes/#command-line-interface","title":"Command Line Interface","text":"<p>The module can be run directly from the command line:</p> <pre><code>python -m opal.configurable_court_extractor [options]\n</code></pre>"},{"location":"reference/court-extractor-classes/#cli-arguments","title":"CLI Arguments","text":"Argument Description Default <code>--court</code> Court type: civil, criminal, or all all <code>--case-number</code> Specific case number to search None <code>--case-category</code> Case category (CV, DV, MC, etc.) None <code>--filed-after</code> Cases filed after date (MM/DD/YYYY) None <code>--filed-before</code> Cases filed before date (MM/DD/YYYY) None <code>--date-period</code> Predefined period: 1d, 7d, 1m, 3m, 6m, 1y None <code>--case-title</code> Search in case title None <code>--case-status</code> Filter by status None <code>--exclude-closed</code> Exclude closed cases False <code>--output-csv</code> Also save as CSV False <code>--debug</code> Show debug information False"},{"location":"reference/court-extractor-classes/#cli-examples","title":"CLI Examples","text":"<pre><code># Search civil cases from last 7 days\npython -m opal.configurable_court_extractor --court civil --date-period 7d\n\n# Search for specific case number\npython -m opal.configurable_court_extractor --case-number \"2024-CA-001\"\n\n# Search criminal cases excluding closed, save as CSV\npython -m opal.configurable_court_extractor --court criminal --exclude-closed --output-csv\n\n# Search by date range\npython -m opal.configurable_court_extractor --filed-after 01/01/2024 --filed-before 12/31/2024\n\n# Complex search with multiple filters\npython -m opal.configurable_court_extractor \\\n    --court civil \\\n    --case-category CV \\\n    --date-period 1m \\\n    --case-status OPEN \\\n    --exclude-closed \\\n    --output-csv\n</code></pre>"},{"location":"reference/court-extractor-classes/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/court-extractor-classes/#basic-usage","title":"Basic Usage","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder, extract_court_cases_with_params\n\n# Using the builder directly\nbuilder = CourtSearchBuilder()\nurl = builder.set_court('civil').set_date_period('7d').build()\nprint(f\"Search URL: {url}\")\n\n# Using the extraction function\nresult = extract_court_cases_with_params(\n    court_type='civil',\n    date_period='7d',\n    exclude_closed=True\n)\n\nprint(f\"Found {result['total_cases']} cases\")\n</code></pre>"},{"location":"reference/court-extractor-classes/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Complex search with multiple filters\nresult = extract_court_cases_with_params(\n    court_type='criminal',\n    case_category='CV',\n    filed_after='01/01/2024',\n    filed_before='06/30/2024',\n    case_status='OPEN',\n    exclude_closed=True,\n    output_csv=True,\n    debug=True\n)\n\n# Process results\nfor case in result['cases']:\n    print(f\"Case: {case['case_number']['text']}\")\n    print(f\"Title: {case['case_title']}\")\n    print(f\"Status: {case['status']}\")\n    print(\"---\")\n</code></pre>"},{"location":"reference/court-extractor-classes/#integration-with-opal","title":"Integration with OPAL","text":"<pre><code>from opal.integrated_parser import IntegratedParser\nfrom opal.court_case_parser import ParserAppealsAL\nfrom opal.configurable_court_extractor import CourtSearchBuilder\n\n# Build custom search URL\nbuilder = CourtSearchBuilder()\nsearch_url = builder.set_court('civil').set_date_period('1m').build()\n\n# Use with integrated parser\nparser = IntegratedParser(ParserAppealsAL)\nresult = parser.process_site(search_url)\n</code></pre>"},{"location":"reference/court-extractor-classes/#output-format","title":"Output Format","text":"<p>The extractor returns data in the following format:</p> <pre><code>{\n    \"status\": \"success\",\n    \"total_cases\": 150,\n    \"extraction_date\": \"2024-01-15\",\n    \"search_params\": {\n        \"court_type\": \"civil\",\n        \"date_period\": \"7d\",\n        \"exclude_closed\": true\n    },\n    \"cases\": [\n        {\n            \"court\": \"Court of Civil Appeals\",\n            \"case_number\": {\n                \"text\": \"2024-CA-001\",\n                \"link\": \"https://...\"\n            },\n            \"case_title\": \"Smith v. Jones\",\n            \"classification\": \"Civil\",\n            \"filed_date\": \"01/10/2024\",\n            \"status\": \"OPEN\"\n        }\n    ],\n    \"files_created\": [\n        \"2024-01-15_court_extractor_civil.json\",\n        \"2024-01-15_court_extractor_civil.csv\"\n    ]\n}\n</code></pre>"},{"location":"reference/court-extractor-classes/#error-handling","title":"Error Handling","text":"<p>The extractor includes comprehensive error handling:</p> <ul> <li>Network errors are caught and reported</li> <li>Invalid parameters raise <code>ValueError</code> with descriptive messages</li> <li>Partial failures (some pages fail) still return successful results</li> <li>All errors are logged with context</li> </ul>"},{"location":"reference/court-extractor-classes/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The extractor includes automatic rate limiting (3 seconds between requests)</li> <li>Large result sets may take several minutes to complete</li> <li>Progress is printed to console during extraction</li> <li>Consider using date filters to limit result size</li> </ul>"},{"location":"reference/court_url_paginator/","title":"Court URL Paginator","text":"<p>The Court URL Paginator module (<code>opal.court_url_paginator</code>) provides utilities for handling pagination in the Alabama Appeals Court Public Portal. It includes functions for parsing, building, and generating paginated URLs.</p>"},{"location":"reference/court_url_paginator/#overview","title":"Overview","text":"<p>The Alabama Appeals Court portal (<code>publicportal.alappeals.gov</code>) uses URL-encoded pagination parameters. This module handles:</p> <ul> <li>Parsing page numbers from encoded URLs</li> <li>Building URLs for specific pages</li> <li>Extracting total page count from initial loads</li> <li>Generating complete sets of paginated URLs</li> <li>Validating Appeals Court portal URLs</li> </ul>"},{"location":"reference/court_url_paginator/#functions","title":"Functions","text":""},{"location":"reference/court_url_paginator/#parse_court_urlurl","title":"<code>parse_court_url(url)</code>","text":"<p>Extracts current page number and total pages from a court URL.</p> <pre><code>from opal.court_url_paginator import parse_court_url\n\nurl = \"https://publicportal.alappeals.gov/portal/search/case/results?...\"\ncurrent_page, total_pages = parse_court_url(url)\n\nprint(f\"Current page: {current_page}, Total pages: {total_pages}\")\n# Current page: 0, Total pages: 5\n</code></pre> <p>Parameters: - <code>url</code> (str): The court URL to parse</p> <p>Returns: - <code>Tuple[Optional[int], Optional[int]]</code>: (current_page, total_pages) or (None, None) if parsing fails</p> <p>URL Pattern Parsed: The function looks for these patterns in decoded URLs: - <code>page~(.*?number~(\\d+)</code> - extracts current page number - <code>totalPages~(\\d+)</code> - extracts total page count</p>"},{"location":"reference/court_url_paginator/#build_court_urlbase_url-page_number","title":"<code>build_court_url(base_url, page_number)</code>","text":"<p>Constructs a URL for a specific page number.</p> <pre><code>from opal.court_url_paginator import build_court_url\n\nbase_url = \"https://publicportal.alappeals.gov/portal/search/case/results?...\"\npage_2_url = build_court_url(base_url, 2)\n</code></pre> <p>Parameters: - <code>base_url</code> (str): The original court URL (any page) - <code>page_number</code> (int): The desired page number (0-indexed)</p> <p>Returns: - <code>str</code>: URL for the specified page</p> <p>Implementation: Uses regex to replace the page number in the pattern: <code>page~%28.*?number~X</code></p>"},{"location":"reference/court_url_paginator/#extract_total_pages_from_first_loadurl-parser","title":"<code>extract_total_pages_from_first_load(url, parser)</code>","text":"<p>Extracts the total number of pages by loading the first page and checking for JavaScript updates.</p> <pre><code>from opal.court_url_paginator import extract_total_pages_from_first_load\nfrom opal.parser_appeals_al import ParserAppealsAL\n\nparser = ParserAppealsAL()\ntotal_pages = extract_total_pages_from_first_load(court_url, parser)\nprint(f\"Total pages: {total_pages}\")\n</code></pre> <p>Parameters: - <code>url</code> (str): Initial URL (typically page 0) - <code>parser</code>: ParserAppealsAL instance to make the request</p> <p>Returns: - <code>int</code>: Total number of pages (1 if extraction fails)</p> <p>Process: 1. Makes request using the parser 2. Waits for JavaScript to update the URL 3. Parses the updated URL for total page count 4. Falls back to 1 if unable to determine</p>"},{"location":"reference/court_url_paginator/#paginate_court_urlsbase_url-parsernone","title":"<code>paginate_court_urls(base_url, parser=None)</code>","text":"<p>Generates a list of URLs for all pages in the search results.</p> <pre><code>from opal.court_url_paginator import paginate_court_urls\nfrom opal.parser_appeals_al import ParserAppealsAL\n\nparser = ParserAppealsAL()\n\n# With parser for dynamic total page detection\nurls = paginate_court_urls(first_url, parser)\n\n# Without parser (uses URL info only)\nurls = paginate_court_urls(first_url)\n\nfor i, url in enumerate(urls):\n    print(f\"Page {i}: {url}\")\n</code></pre> <p>Parameters: - <code>base_url</code> (str): Initial court search URL - <code>parser</code> (optional): ParserAppealsAL instance for dynamic page detection</p> <p>Returns: - <code>List[str]</code>: List of URLs for all pages (0-indexed)</p> <p>Logic: 1. Try to parse total pages from URL 2. If not available and parser provided, load first page to detect 3. Generate URLs for all pages (0 to total_pages-1) 4. Return just base URL if pagination cannot be determined</p>"},{"location":"reference/court_url_paginator/#is_court_urlurl","title":"<code>is_court_url(url)</code>","text":"<p>Validates if a URL is from the Alabama Appeals Court portal.</p> <pre><code>from opal.court_url_paginator import is_court_url\n\nif is_court_url(url):\n    print(\"Valid Appeals Court URL\")\nelse:\n    print(\"Not an Appeals Court URL\")\n</code></pre> <p>Parameters: - <code>url</code> (str): URL to validate</p> <p>Returns: - <code>bool</code>: True if URL contains both <code>publicportal.alappeals.gov</code> and <code>/portal/search/case/results</code></p>"},{"location":"reference/court_url_paginator/#url-structure","title":"URL Structure","text":"<p>Appeals Court URLs use encoded pagination parameters:</p> <pre><code>https://publicportal.alappeals.gov/portal/search/case/results?searchParams=...page~%28size~25~number~0~totalElements~125~totalPages~5%29\n</code></pre> <p>Key components: - <code>page~%28</code> - Start of page parameter block - <code>size~25</code> - Results per page - <code>number~0</code> - Current page (0-indexed) - <code>totalElements~125</code> - Total result count - <code>totalPages~5</code> - Total number of pages</p>"},{"location":"reference/court_url_paginator/#integration-examples","title":"Integration Examples","text":""},{"location":"reference/court_url_paginator/#with-parserappealsal","title":"With ParserAppealsAL","text":"<pre><code>from opal.parser_appeals_al import ParserAppealsAL\nfrom opal.court_url_paginator import paginate_court_urls, extract_total_pages_from_first_load\n\nparser = ParserAppealsAL()\n\n# Get total pages dynamically\ntotal_pages = extract_total_pages_from_first_load(search_url, parser)\nprint(f\"Found {total_pages} pages\")\n\n# Generate all page URLs\nall_urls = paginate_court_urls(search_url, parser)\n\n# Process each page\nall_cases = []\nfor i, url in enumerate(all_urls):\n    print(f\"Processing page {i+1}/{len(all_urls)}\")\n    cases = parser.extract_page_data(url)\n    all_cases.extend(cases)\n</code></pre>"},{"location":"reference/court_url_paginator/#with-configurable-court-extractor","title":"With Configurable Court Extractor","text":"<p>The configurable court extractor uses these functions internally:</p> <pre><code># Internal usage in configurable_court_extractor.py\ndef _process_paginated_results(self, first_page_url):\n    # Generate URLs for all pages\n    page_urls = paginate_court_urls(first_page_url, self.parser)\n\n    # Process each page\n    for url in page_urls:\n        self._process_page(url)\n</code></pre>"},{"location":"reference/court_url_paginator/#manual-pagination-handling","title":"Manual Pagination Handling","text":"<pre><code>from opal.court_url_paginator import parse_court_url, build_court_url\n\n# Parse current state\ncurrent_page, total_pages = parse_court_url(search_url)\n\nif total_pages and total_pages &gt; 1:\n    # Process remaining pages\n    for page_num in range(current_page + 1, total_pages):\n        next_url = build_court_url(search_url, page_num)\n        # Process next_url...\n</code></pre>"},{"location":"reference/court_url_paginator/#error-handling","title":"Error Handling","text":"<p>The paginator functions are designed to fail gracefully:</p> <ul> <li><code>parse_court_url</code>: Returns (None, None) if parsing fails</li> <li><code>extract_total_pages_from_first_load</code>: Returns 1 if extraction fails</li> <li><code>build_court_url</code>: Returns original URL if building fails</li> <li><code>paginate_court_urls</code>: Returns single-item list with base URL if pagination fails</li> </ul> <pre><code># Safe usage pattern\nfrom opal.court_url_paginator import paginate_court_urls\n\ntry:\n    urls = paginate_court_urls(court_url, parser)\n    if len(urls) == 1:\n        print(\"Single page or pagination detection failed\")\nexcept Exception as e:\n    print(f\"Pagination error: {e}\")\n    urls = [court_url]  # Fallback to original URL\n</code></pre>"},{"location":"reference/court_url_paginator/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>URL parsing is fast and doesn't require network requests</li> <li>Dynamic page detection requires loading the first page</li> <li>Consider caching total page counts for repeated searches</li> <li>Use with rate limiting to avoid overwhelming the server</li> </ul>"},{"location":"reference/court_url_paginator/#debugging","title":"Debugging","text":"<p>Enable debug output by checking the console messages:</p> <pre><code>from opal.court_url_paginator import extract_total_pages_from_first_load\n\n# Function prints debug messages:\n# \"Detected X total pages from URL\"\n# \"Error parsing URL: ...\"\n# \"Error extracting total pages: ...\"\n\ntotal_pages = extract_total_pages_from_first_load(url, parser)\n</code></pre>"},{"location":"reference/court_url_paginator/#limitations","title":"Limitations","text":"<ol> <li>Appeals Court Specific: Only works with <code>publicportal.alappeals.gov</code> URLs</li> <li>JavaScript Dependency: Requires browser/parser for dynamic page detection</li> <li>URL Structure Dependency: May break if portal changes URL encoding</li> <li>0-Based Indexing: Page numbers are 0-indexed (page 0 is first page)</li> <li>Session Dependency: URLs may be session-based and expire</li> </ol>"},{"location":"reference/court_url_paginator/#complete-example","title":"Complete Example","text":"<pre><code>from opal.court_url_paginator import (\n    is_court_url, \n    parse_court_url,\n    paginate_court_urls,\n    extract_total_pages_from_first_load\n)\nfrom opal.parser_appeals_al import ParserAppealsAL\n\ndef process_all_appeals_court_pages(search_url):\n    # Validate URL\n    if not is_court_url(search_url):\n        raise ValueError(\"Not a valid Appeals Court URL\")\n\n    # Parse initial URL\n    current_page, total_pages = parse_court_url(search_url)\n    print(f\"Starting from page {current_page}, total: {total_pages}\")\n\n    # Setup parser\n    parser = ParserAppealsAL()\n\n    # Get total pages if not in URL\n    if total_pages is None:\n        total_pages = extract_total_pages_from_first_load(search_url, parser)\n        print(f\"Detected {total_pages} total pages\")\n\n    # Generate all URLs\n    all_urls = paginate_court_urls(search_url, parser)\n\n    # Process each page\n    results = []\n    for i, url in enumerate(all_urls):\n        print(f\"Processing page {i}/{len(all_urls)-1}\")\n        page_data = parser.extract_page_data(url)\n        results.extend(page_data)\n\n    return results\n\n# Usage\nsearch_url = \"https://publicportal.alappeals.gov/portal/search/case/results?...\"\nall_cases = process_all_appeals_court_pages(search_url)\nprint(f\"Extracted {len(all_cases)} total cases\")\n</code></pre>"},{"location":"reference/court_url_paginator/#key-differences-from-other-court-systems","title":"Key Differences from Other Court Systems","text":"<p>This module is specifically designed for the Alabama Appeals Court portal, which differs from other Alabama court systems:</p> <ul> <li>URL Domain: <code>publicportal.alappeals.gov</code> (not <code>alacourt.gov</code>)</li> <li>Pagination: URL-encoded parameters (not JavaScript/AJAX)</li> <li>Page Indexing: 0-based (page 0 is first page)</li> <li>Search Path: <code>/portal/search/case/results</code> (not <code>/ajax/courts.aspx</code>)</li> </ul> <p>Make sure you're using the correct parser and URLs for the Appeals Court system.</p>"},{"location":"reference/data_structures/","title":"Data Structures","text":"<p>This document describes the data structures returned by OPAL parsers and extractors.</p>"},{"location":"reference/data_structures/#court-parser-data-structures","title":"Court Parser Data Structures","text":""},{"location":"reference/data_structures/#individual-court-case","title":"Individual Court Case","text":"<p>Each court case is represented as a dictionary with the following structure:</p> <pre><code>{\n    \"court\": str,              # Court name (e.g., \"Alabama Civil Appeals\")\n    \"case_number\": {\n        \"text\": str,           # Case number (e.g., \"2190259\")\n        \"link\": str            # URL to case details\n    },\n    \"case_title\": str,         # Full case title\n    \"classification\": str,     # Case type (e.g., \"Appeal\", \"Certiorari\")\n    \"filed_date\": str,         # Filing date (MM/DD/YYYY format)\n    \"status\": str              # Case status (e.g., \"Open\", \"Closed\")\n}\n</code></pre>"},{"location":"reference/data_structures/#court-search-results","title":"Court Search Results","text":"<p>When using the court parser or configurable extractor, the complete results structure is:</p> <pre><code>{\n    \"status\": str,                    # \"success\" or \"error\"\n    \"search_parameters\": {            # Parameters used for search\n        \"court\": str,\n        \"date_period\": str,\n        \"start_date\": str | None,\n        \"end_date\": str | None,\n        \"case_number\": str | None,\n        \"case_title\": str | None,\n        \"case_category\": str | None,\n        \"exclude_closed\": bool\n    },\n    \"total_cases\": int,               # Total number of cases found\n    \"extraction_date\": str,           # Date of extraction (YYYY-MM-DD)\n    \"extraction_time\": str,           # Time of extraction (HH:MM:SS)\n    \"pages_processed\": int,           # Number of pages processed\n    \"cases\": List[Dict]               # List of case dictionaries (see above)\n}\n</code></pre>"},{"location":"reference/data_structures/#paginated-results","title":"Paginated Results","text":"<p>For paginated court results:</p> <pre><code>{\n    \"page\": int,                      # Current page number\n    \"total_pages\": int,               # Total number of pages\n    \"cases_on_page\": int,             # Number of cases on this page\n    \"cases\": List[Dict]               # List of cases for this page\n}\n</code></pre>"},{"location":"reference/data_structures/#news-parser-data-structures","title":"News Parser Data Structures","text":""},{"location":"reference/data_structures/#news-article","title":"News Article","text":"<p>News articles from alabamanewscenter.com:</p> <pre><code>{\n    \"title\": str,                     # Article title\n    \"link\": str,                      # Article URL\n    \"date\": str,                      # Publication date\n    \"author\": str | None,             # Article author\n    \"summary\": str | None,            # Article summary/excerpt\n    \"content\": str,                   # Full article content\n    \"tags\": List[str],                # Article tags/categories\n    \"image_url\": str | None           # Main article image URL\n}\n</code></pre>"},{"location":"reference/data_structures/#news-search-results","title":"News Search Results","text":"<pre><code>{\n    \"status\": str,                    # \"success\" or \"error\"\n    \"search_term\": str | None,        # Search term used (if any)\n    \"category\": str | None,           # Category filter (if any)\n    \"total_articles\": int,            # Total articles found\n    \"articles\": List[Dict]            # List of article dictionaries\n}\n</code></pre>"},{"location":"reference/data_structures/#integrated-parser-results","title":"Integrated Parser Results","text":"<p>When using the integrated parser, results include a parser type indicator:</p> <pre><code>{\n    \"parser_type\": str,               # \"court\" or \"news\"\n    \"url\": str,                       # Original URL processed\n    \"data\": Dict                      # Parser-specific results (see above)\n}\n</code></pre>"},{"location":"reference/data_structures/#csv-output-formats","title":"CSV Output Formats","text":""},{"location":"reference/data_structures/#court-cases-csv","title":"Court Cases CSV","text":"<p>When exporting court cases to CSV:</p> Column Description Example court Court name Alabama Civil Appeals case_number Case number text 2190259 case_number_link URL to case https://... case_title Full case title Smith v. Jones classification Case type Appeal filed_date Filing date 01/15/2024 status Case status Open"},{"location":"reference/data_structures/#news-articles-csv","title":"News Articles CSV","text":"<p>When exporting news articles to CSV:</p> Column Description Example title Article title Breaking News... link Article URL https://... date Publication date 2024-01-15 author Author name John Doe summary Article excerpt This article... tags Comma-separated tags politics,local"},{"location":"reference/data_structures/#error-response-structure","title":"Error Response Structure","text":"<p>When errors occur:</p> <pre><code>{\n    \"status\": \"error\",\n    \"error_type\": str,                # Type of error\n    \"error_message\": str,             # Detailed error message\n    \"timestamp\": str,                 # When error occurred\n    \"context\": Dict | None            # Additional error context\n}\n</code></pre>"},{"location":"reference/data_structures/#metadata-fields","title":"Metadata Fields","text":"<p>Common metadata fields across parsers:</p> <pre><code>{\n    \"extraction_date\": str,           # YYYY-MM-DD format\n    \"extraction_time\": str,           # HH:MM:SS format\n    \"parser_version\": str,            # OPAL version\n    \"processing_time_seconds\": float, # Time taken to process\n    \"source_url\": str                 # Original URL processed\n}\n</code></pre>"},{"location":"reference/data_structures/#type-definitions","title":"Type Definitions","text":"<p>For TypeScript or type-aware Python development:</p> <pre><code>from typing import TypedDict, List, Optional, Union\n\nclass CaseNumber(TypedDict):\n    text: str\n    link: str\n\nclass CourtCase(TypedDict):\n    court: str\n    case_number: CaseNumber\n    case_title: str\n    classification: str\n    filed_date: str\n    status: str\n\nclass SearchParameters(TypedDict):\n    court: str\n    date_period: Optional[str]\n    start_date: Optional[str]\n    end_date: Optional[str]\n    case_number: Optional[str]\n    case_title: Optional[str]\n    case_category: Optional[str]\n    exclude_closed: bool\n\nclass CourtSearchResults(TypedDict):\n    status: str\n    search_parameters: SearchParameters\n    total_cases: int\n    extraction_date: str\n    extraction_time: str\n    pages_processed: int\n    cases: List[CourtCase]\n\nclass NewsArticle(TypedDict):\n    title: str\n    link: str\n    date: str\n    author: Optional[str]\n    summary: Optional[str]\n    content: str\n    tags: List[str]\n    image_url: Optional[str]\n</code></pre>"},{"location":"reference/data_structures/#data-validation","title":"Data Validation","text":""},{"location":"reference/data_structures/#required-fields","title":"Required Fields","text":"<p>Court Cases: All fields are required except <code>case_number.link</code> may be empty for some cases.</p> <p>News Articles: Required fields are <code>title</code>, <code>link</code>, <code>date</code>, and <code>content</code>. Other fields may be null/empty.</p>"},{"location":"reference/data_structures/#date-formats","title":"Date Formats","text":"<ul> <li>Court filing dates: <code>MM/DD/YYYY</code> format</li> <li>News publication dates: <code>YYYY-MM-DD</code> format</li> <li>Extraction timestamps: ISO 8601 format</li> </ul>"},{"location":"reference/data_structures/#status-values","title":"Status Values","text":"<p>Court case status values: - <code>Open</code> - Active case - <code>Closed</code> - Completed case - <code>Unknown</code> - Status not determined</p>"},{"location":"reference/data_structures/#classification-values","title":"Classification Values","text":"<p>Common court case classifications: - <code>Appeal</code> - <code>Certiorari</code> - <code>Original Proceeding</code> - <code>Petition</code> - <code>Certified Question</code> - <code>Other</code></p>"},{"location":"reference/data_structures/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/data_structures/#accessing-court-case-data","title":"Accessing Court Case Data","text":"<pre><code>results = parser.extract_all_court_cases()\n\nfor case in results['cases']:\n    print(f\"Case: {case['case_number']['text']}\")\n    print(f\"Title: {case['case_title']}\")\n    print(f\"Filed: {case['filed_date']}\")\n    print(f\"Status: {case['status']}\")\n\n    # Access case details URL\n    if case['case_number']['link']:\n        print(f\"Details: {case['case_number']['link']}\")\n</code></pre>"},{"location":"reference/data_structures/#working-with-search-parameters","title":"Working with Search Parameters","text":"<pre><code># Access search parameters used\nparams = results['search_parameters']\nif params['date_period'] == 'custom':\n    print(f\"Date range: {params['start_date']} to {params['end_date']}\")\n\n# Check filters applied\nif params['exclude_closed']:\n    print(\"Closed cases were excluded\")\n</code></pre>"},{"location":"reference/data_structures/#error-handling","title":"Error Handling","text":"<pre><code>if results['status'] == 'error':\n    print(f\"Error: {results['error_message']}\")\n    if 'context' in results:\n        print(f\"Context: {results['context']}\")\nelse:\n    process_cases(results['cases'])\n</code></pre>"},{"location":"reference/parser-1819/","title":"Parser1819 API Reference","text":"<p>Parser for 1819 News website.</p>"},{"location":"reference/parser-1819/#class-definition","title":"Class Definition","text":"<pre><code>class Parser1819(BaseParser):\n    def __init__(self, url=\"https://1819news.com/\", suffix=\"/news/item\", max_pages=5)\n</code></pre>"},{"location":"reference/parser-1819/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str <code>\"https://1819news.com/\"</code> Base URL for 1819 News <code>suffix</code> str <code>\"/news/item\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"reference/parser-1819/#methods","title":"Methods","text":""},{"location":"reference/parser-1819/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Extracts articles from multiple pages of 1819 News.</p> <p>Returns: List of article dictionaries with keys: - <code>title</code>: Article headline - <code>content</code>: Full article text - <code>date</code>: Publication date - <code>author</code>: Article author - <code>url</code>: Article URL - <code>tags</code>: List of article tags</p>"},{"location":"reference/parser-1819/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extracts article URLs from a 1819 News page.</p> <p>Parameters: - <code>page_url</code>: URL of the page to scrape</p> <p>Returns: List of article URLs matching the suffix pattern</p>"},{"location":"reference/parser-1819/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parses individual 1819 News article.</p> <p>Parameters: - <code>article_url</code>: URL of the article</p> <p>Returns: Dictionary with article data</p>"},{"location":"reference/parser-1819/#usage-example","title":"Usage Example","text":"<pre><code>from opal.Parser1819 import Parser1819\n\n# Create parser instance\nparser = Parser1819(\n    url=\"https://1819news.com/\",\n    suffix=\"/news/item\",\n    max_pages=10\n)\n\n# Extract articles\narticles = parser.extract_article_data()\n\n# Save to JSON\nparser.save_to_json(articles, \"1819_news_articles.json\")\n\n# Access article data\nfor article in articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"Date: {article['date']}\")\n    print(f\"Author: {article['author']}\")\n</code></pre>"},{"location":"reference/parser-1819/#output-format","title":"Output Format","text":"<pre><code>{\n  \"title\": \"Alabama Legislature Passes New Education Bill\",\n  \"content\": \"Full article text...\",\n  \"date\": \"2024-01-15\",\n  \"author\": \"John Smith\",\n  \"url\": \"https://1819news.com/news/item/education-bill-2024\",\n  \"tags\": [\"education\", \"legislature\", \"alabama\"]\n}\n</code></pre>"},{"location":"reference/parser-1819/#notes","title":"Notes","text":"<ul> <li>Uses BeautifulSoup for HTML parsing</li> <li>Handles pagination automatically</li> <li>Filters links by suffix to ensure only articles are scraped</li> <li>Includes error handling for missing elements</li> </ul>"},{"location":"reference/parser-appeals-al/","title":"ParserAppealsAL API Reference","text":"<p>Parser for Alabama Appeals Court Public Portal.</p>"},{"location":"reference/parser-appeals-al/#class-definition","title":"Class Definition","text":"<pre><code>class ParserAppealsAL(BaseParser):\n    def __init__(self, url=\"https://publicportal.alappeals.gov/portal/search/case/results\")\n</code></pre>"},{"location":"reference/parser-appeals-al/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str Portal URL Base URL for court portal <code>headless</code> bool True Run browser in headless mode <code>rate_limit_seconds</code> float 1.0 Delay between requests"},{"location":"reference/parser-appeals-al/#methods","title":"Methods","text":""},{"location":"reference/parser-appeals-al/#setup_driver","title":"setup_driver()","text":"<p><pre><code>def setup_driver(self) -&gt; webdriver.Chrome\n</code></pre> Sets up Chrome WebDriver with appropriate options.</p> <p>Returns: Configured Chrome WebDriver instance</p>"},{"location":"reference/parser-appeals-al/#extract_court_data","title":"extract_court_data()","text":"<p><pre><code>def extract_court_data(self) -&gt; List[Dict]\n</code></pre> Main method to extract court case data.</p> <p>Returns: List of court case dictionaries</p>"},{"location":"reference/parser-appeals-al/#parse_case_row","title":"parse_case_row()","text":"<p><pre><code>def parse_case_row(self, row: WebElement) -&gt; Dict\n</code></pre> Parses individual case row from search results.</p> <p>Parameters: - <code>row</code>: Selenium WebElement representing a case row</p> <p>Returns: Dictionary with case information</p>"},{"location":"reference/parser-appeals-al/#get_case_details","title":"get_case_details()","text":"<p><pre><code>def get_case_details(self, case_url: str) -&gt; Dict\n</code></pre> Fetches detailed information for a specific case.</p> <p>Parameters: - <code>case_url</code>: URL to the case details page</p> <p>Returns: Dictionary with detailed case information</p>"},{"location":"reference/parser-appeals-al/#usage-example","title":"Usage Example","text":"<pre><code>from opal.ParserAppealsAL import ParserAppealsAL\n\n# Create parser instance\nparser = ParserAppealsAL()\n\n# Extract court cases\ncases = parser.extract_court_data()\n\n# Save to JSON\nparser.save_to_json(cases, \"court_cases.json\")\n\n# Process cases\nfor case in cases:\n    print(f\"Case: {case['case_number']}\")\n    print(f\"Title: {case['case_title']}\")\n    print(f\"Status: {case['status']}\")\n</code></pre>"},{"location":"reference/parser-appeals-al/#output-format","title":"Output Format","text":"<pre><code>{\n  \"case_number\": \"2024-CV-001234\",\n  \"case_title\": \"State of Alabama v. John Doe\",\n  \"court\": \"Alabama Court of Civil Appeals\",\n  \"date_filed\": \"2024-01-10\",\n  \"status\": \"Active\",\n  \"judge\": \"Hon. Jane Smith\",\n  \"parties\": {\n    \"appellant\": \"John Doe\",\n    \"appellee\": \"State of Alabama\"\n  },\n  \"attorneys\": [\n    {\n      \"name\": \"James Johnson\",\n      \"role\": \"Attorney for Appellant\"\n    }\n  ],\n  \"docket_entries\": [\n    {\n      \"date\": \"2024-01-10\",\n      \"description\": \"Notice of Appeal Filed\",\n      \"document_url\": \"https://publicportal.alappeals.gov/document/12345\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/parser-appeals-al/#special-features","title":"Special Features","text":""},{"location":"reference/parser-appeals-al/#selenium-webdriver","title":"Selenium WebDriver","text":"<ul> <li>Automatically installs ChromeDriver</li> <li>Handles JavaScript-rendered content</li> <li>Supports dynamic page interactions</li> </ul>"},{"location":"reference/parser-appeals-al/#error-handling","title":"Error Handling","text":"<ul> <li>Retries failed page loads</li> <li>Handles stale element exceptions</li> <li>Logs detailed error information</li> </ul>"},{"location":"reference/parser-appeals-al/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Includes delays between requests</li> <li>Respects server load</li> </ul>"},{"location":"reference/parser-appeals-al/#integration-with-other-components","title":"Integration with Other Components","text":""},{"location":"reference/parser-appeals-al/#with-configurable-court-extractor","title":"With Configurable Court Extractor","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nfrom opal.court_case_parser import ParserAppealsAL\n\n# The configurable extractor uses ParserAppealsAL internally\nresults = extract_court_cases_with_params(\n    court=\"civil\",\n    date_period=\"1m\",\n    max_pages=5\n)\n\n# Direct parser usage\nparser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\nresult = parser.parse_article(url)\n</code></pre>"},{"location":"reference/parser-appeals-al/#with-court-url-paginator","title":"With Court URL Paginator","text":"<pre><code>from opal.court_url_paginator import paginate_court_urls\nfrom opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL()\n\n# Get all page URLs (requires parser for dynamic page detection)\npage_urls = paginate_court_urls(search_url, parser)\n\n# Process each page\nall_cases = []\nfor url in page_urls:\n    result = parser.parse_article(url)\n    if 'cases' in result:\n        all_cases.extend(result['cases'])\n\nparser._close_driver()\n</code></pre>"},{"location":"reference/parser-appeals-al/#with-integrated-parser","title":"With Integrated Parser","text":"<pre><code>from opal.integrated_parser import IntegratedParser\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Integrated parser requires parser class as parameter\nintegrated = IntegratedParser(ParserAppealsAL)\nresult = integrated.process_site(\"https://publicportal.alappeals.gov/...\")\n\n# Result contains processed data\nif result:\n    data = json.loads(result)\n</code></pre>"},{"location":"reference/parser-appeals-al/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/parser-appeals-al/#custom-chrome-options","title":"Custom Chrome Options","text":"<pre><code>from selenium import webdriver\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Note: Chrome options are set in the _setup_driver method\n# To customize, you would need to modify the parser's _setup_driver method\nparser = ParserAppealsAL(headless=False)  # Run with visible browser\n</code></pre>"},{"location":"reference/parser-appeals-al/#error-handling-integration","title":"Error Handling Integration","text":"<pre><code>from opal.court_case_parser import ParserAppealsAL\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\nparser = ParserAppealsAL(\n    headless=False,  # For debugging\n    rate_limit_seconds=2  # Slower for observation (int, not float)\n)\n\ntry:\n    result = parser.parse_article(url)\n    cases = result.get('cases', [])\nexcept Exception as e:\n    logging.error(f\"Extraction failed: {e}\")\n    # Handle error appropriately\nfinally:\n    parser._close_driver()\n</code></pre>"},{"location":"reference/parser-appeals-al/#notes","title":"Notes","text":"<ul> <li>Requires Chrome browser installed</li> <li>Uses Selenium for JavaScript support</li> <li>Handles pagination automatically</li> <li>Extracts both case list and detailed case information</li> <li>Includes comprehensive error handling for web automation</li> <li>Integrates with all other OPAL components</li> <li>Supports extensive customization through configuration</li> </ul>"},{"location":"reference/parser-classes-overview/","title":"Parser Classes Overview","text":"<p>Complete reference for all OPAL parser classes and their integration points.</p>"},{"location":"reference/parser-classes-overview/#core-components","title":"Core Components","text":""},{"location":"reference/parser-classes-overview/#parserappealsal","title":"ParserAppealsAL","text":"<p>The main court parser for Alabama Appeals Court system.</p> <pre><code>from opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL(\n    headless=True,\n    rate_limit_seconds=3\n)\n\n# Parse a specific URL\nresult = parser.parse_article(url)\n\n# Process all court cases from a search\ncases = parser.parse_all_cases(base_url, page_urls)\n</code></pre> <p>Constructor Parameters: - <code>headless</code> (bool): Run browser in headless mode (default: True) - <code>rate_limit_seconds</code> (int): Seconds between requests (default: 3)</p> <p>Key Methods: - <code>parse_article(url)</code> - Parse single page/URL - <code>parse_all_cases(base_url, page_urls)</code> - Parse multiple pages - <code>_setup_driver()</code> - Initialize WebDriver - <code>_close_driver()</code> - Clean up WebDriver</p>"},{"location":"reference/parser-classes-overview/#courtsearchbuilder","title":"CourtSearchBuilder","text":"<p>Builder class for constructing Alabama Court search URLs with court-specific parameters.</p> <pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\n\n# Create builder\nbuilder = CourtSearchBuilder()\n\n# Configure search\nbuilder.set_court(\"civil\")\nbuilder.set_date_range(period=\"1m\")\nbuilder.set_case_category(\"Appeal\")\nbuilder.set_exclude_closed(True)\n\n# Build URL\nsearch_url = builder.build_url(page_number=0)\n</code></pre> <p>Available Methods:</p> <p>Court Configuration: - <code>set_court(court_key)</code> - Set court ('civil', 'criminal', 'supreme') - <code>get_court_info()</code> - Get current court information - <code>discover_court_ids(parser_instance)</code> - Auto-discover court IDs - <code>set_court_id_manually(court_key, court_id)</code> - Manual court ID override</p> <p>Search Parameters: - <code>set_date_range(start_date=None, end_date=None, period='1y')</code> - Date filtering - <code>set_case_category(category_name=None)</code> - Case type filtering - <code>set_case_number_filter(case_number=None)</code> - Case number filtering - <code>set_case_title_filter(title=None)</code> - Case title filtering - <code>set_exclude_closed(exclude=False)</code> - Exclude closed cases</p> <p>URL Building: - <code>build_url(page_number=0)</code> - Build complete search URL - <code>build_criteria_string()</code> - Build URL criteria parameters</p>"},{"location":"reference/parser-classes-overview/#extract_court_cases_with_params","title":"extract_court_cases_with_params()","text":"<p>Main extraction function supporting both parameter-based and URL-based searches.</p> <pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\n# Parameter-based search\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='1m',\n    case_category='Appeal',\n    exclude_closed=True,\n    max_pages=5,\n    output_prefix=\"civil_appeals\"\n)\n\n# Custom URL search\nresults = extract_court_cases_with_params(\n    custom_url=\"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\",\n    max_pages=5\n)\n</code></pre> <p>Parameters: - <code>court</code> (str): Court type ('civil', 'criminal', 'supreme') - <code>date_period</code> (str): Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - <code>start_date</code>, <code>end_date</code> (str): Custom date range (YYYY-MM-DD) - <code>case_number</code>, <code>case_title</code>, <code>case_category</code> (str): Filtering options - <code>exclude_closed</code> (bool): Exclude closed cases - <code>max_pages</code> (int): Maximum pages to process - <code>output_prefix</code> (str): Output file prefix - <code>custom_url</code> (str): Pre-built search URL (overrides search params)</p>"},{"location":"reference/parser-classes-overview/#courturlpaginator-functions","title":"CourtURLPaginator Functions","text":"<p>Utilities for handling court system pagination.</p> <pre><code>from opal.court_url_paginator import (\n    parse_court_url,\n    build_court_url,\n    paginate_court_urls,\n    extract_total_pages_from_first_load,\n    is_court_url\n)\n\n# Parse pagination info\ncurrent_page, total_pages = parse_court_url(court_url)\n\n# Build page-specific URL\npage_3_url = build_court_url(base_url, 3)\n\n# Generate all page URLs\nall_urls = paginate_court_urls(first_page_url, parser)\n</code></pre> <p>Key Functions: - <code>parse_court_url(url)</code> - Extract page info from URL \u2192 (current_page, total_pages) - <code>build_court_url(base_url, page_number)</code> - Build URL for specific page - <code>paginate_court_urls(base_url, parser=None)</code> - Generate all page URLs - <code>extract_total_pages_from_first_load(url, parser)</code> - Get total pages dynamically - <code>is_court_url(url)</code> - Validate Appeals Court URL</p>"},{"location":"reference/parser-classes-overview/#integratedparser","title":"IntegratedParser","text":"<p>Unified interface for court and news parsing with automatic detection.</p> <pre><code>from opal.integrated_parser import IntegratedParser\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Create integrated parser with specific parser class\nparser = IntegratedParser(ParserAppealsAL)\n\n# Process URL (auto-detects court vs news)\nresult = parser.process_site(url)\n</code></pre> <p>Constructor: - <code>parser_class</code> (Type[BaseParser]): Parser class to use</p> <p>Key Methods: - <code>process_site(base_url, suffix=\"\", max_pages=None)</code> - Process entire site</p>"},{"location":"reference/parser-classes-overview/#extract_all_court_cases","title":"extract_all_court_cases()","text":"<p>Standalone script function for extracting all available court cases.</p> <pre><code>from opal.extract_all_court_cases import extract_all_court_cases\n\n# Extract all cases (hardcoded pagination)\nresults = extract_all_court_cases()\n</code></pre>"},{"location":"reference/parser-classes-overview/#integration-patterns","title":"Integration Patterns","text":""},{"location":"reference/parser-classes-overview/#1-basic-court-extraction","title":"1. Basic Court Extraction","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\n# Simple extraction\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='7d',\n    exclude_closed=True\n)\n\nif results and results['status'] == 'success':\n    cases = results['cases']\n    print(f\"Found {len(cases)} cases\")\n</code></pre>"},{"location":"reference/parser-classes-overview/#2-advanced-search-building","title":"2. Advanced Search Building","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Create components\nbuilder = CourtSearchBuilder()\nparser = ParserAppealsAL(headless=True)\n\n# Auto-discover court IDs\nbuilder.discover_court_ids(parser)\n\n# Build complex search\nbuilder.set_court('supreme')\nbuilder.set_date_range(start_date='2024-01-01', end_date='2024-12-31', period='custom')\nbuilder.set_case_category('Certiorari')\n\n# Get URL and extract\nsearch_url = builder.build_url()\nresults = extract_court_cases_with_params(custom_url=search_url)\n</code></pre>"},{"location":"reference/parser-classes-overview/#3-manual-pagination-processing","title":"3. Manual Pagination Processing","text":"<pre><code>from opal.court_url_paginator import paginate_court_urls, parse_court_url\nfrom opal.court_case_parser import ParserAppealsAL\n\nparser = ParserAppealsAL()\n\n# Get all page URLs\npage_urls = paginate_court_urls(search_url, parser)\n\n# Process each page manually\nall_cases = []\nfor i, url in enumerate(page_urls):\n    print(f\"Processing page {i+1}/{len(page_urls)}\")\n    result = parser.parse_article(url)\n    if 'cases' in result:\n        all_cases.extend(result['cases'])\n\nparser._close_driver()\n</code></pre>"},{"location":"reference/parser-classes-overview/#4-integration-with-error-handling","title":"4. Integration with Error Handling","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef safe_extract(court_type, **kwargs):\n    \"\"\"Extract with comprehensive error handling\"\"\"\n    try:\n        results = extract_court_cases_with_params(\n            court=court_type,\n            **kwargs\n        )\n\n        if results and results['status'] == 'success':\n            return results['cases']\n        else:\n            logging.error(\"Extraction failed\")\n            return []\n\n    except Exception as e:\n        logging.error(f\"Extraction error: {e}\")\n        return []\n\n# Usage\ncases = safe_extract('civil', date_period='1m', max_pages=5)\n</code></pre>"},{"location":"reference/parser-classes-overview/#5-batch-processing-multiple-courts","title":"5. Batch Processing Multiple Courts","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nimport json\nfrom datetime import datetime\n\ndef batch_extract_courts(courts, date_period=\"1m\", max_pages=5):\n    \"\"\"Extract data for multiple courts\"\"\"\n    results = {}\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n    for court in courts:\n        print(f\"Processing {court} court...\")\n\n        try:\n            court_results = extract_court_cases_with_params(\n                court=court,\n                date_period=date_period,\n                max_pages=max_pages,\n                output_prefix=f\"{court}_{timestamp}\"\n            )\n\n            if court_results:\n                results[court] = {\n                    'status': 'success',\n                    'total_cases': court_results['total_cases'],\n                    'pages_processed': court_results['pages_processed']\n                }\n            else:\n                results[court] = {'status': 'failed'}\n\n        except Exception as e:\n            results[court] = {'status': 'error', 'error': str(e)}\n\n    # Save summary\n    with open(f\"batch_summary_{timestamp}.json\", 'w') as f:\n        json.dump(results, f, indent=2)\n\n    return results\n\n# Usage\nresults = batch_extract_courts(['civil', 'criminal', 'supreme'])\n</code></pre>"},{"location":"reference/parser-classes-overview/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"reference/parser-classes-overview/#standard-extraction-flow","title":"Standard Extraction Flow","text":"<pre><code>graph TD\n    A[User Input] --&gt; B{Use Custom URL?}\n    B --&gt;|Yes| C[Extract with Custom URL]\n    B --&gt;|No| D[CourtSearchBuilder]\n    D --&gt; E[Discover Court IDs]\n    E --&gt; F[Build Search Parameters]\n    F --&gt; G[Generate Search URL]\n    C --&gt; H[ParserAppealsAL]\n    G --&gt; H\n    H --&gt; I[Extract First Page]\n    I --&gt; J[Determine Total Pages]\n    J --&gt; K[Process All Pages]\n    K --&gt; L[Aggregate Results]\n    L --&gt; M[Generate Outputs]</code></pre>"},{"location":"reference/parser-classes-overview/#component-dependencies","title":"Component Dependencies","text":"<pre><code>graph TD\n    A[configurable_court_extractor] --&gt; B[court_case_parser]\n    A --&gt; C[court_url_paginator]\n    B --&gt; D[BaseParser]\n    E[integrated_parser] --&gt; B\n    E --&gt; F[url_catcher_module]\n    G[extract_all_court_cases] --&gt; B\n    G --&gt; C</code></pre>"},{"location":"reference/parser-classes-overview/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"reference/parser-classes-overview/#custom-extraction-pipeline","title":"Custom Extraction Pipeline","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\nfrom opal.court_url_paginator import paginate_court_urls\nimport asyncio\nimport json\n\nclass AdvancedCourtExtractor:\n    def __init__(self):\n        self.builder = CourtSearchBuilder()\n        self.parser = ParserAppealsAL(headless=True, rate_limit_seconds=1)\n\n    def setup(self):\n        \"\"\"Initialize court IDs\"\"\"\n        self.builder.discover_court_ids(self.parser)\n\n    def create_search(self, court, filters):\n        \"\"\"Create configured search URL\"\"\"\n        self.builder.set_court(court)\n\n        if 'date_period' in filters:\n            self.builder.set_date_range(period=filters['date_period'])\n        if 'case_category' in filters:\n            self.builder.set_case_category(filters['case_category'])\n        if 'exclude_closed' in filters:\n            self.builder.set_exclude_closed(filters['exclude_closed'])\n\n        return self.builder.build_url()\n\n    def extract_with_progress(self, court, filters, max_pages=None):\n        \"\"\"Extract with progress reporting\"\"\"\n        search_url = self.create_search(court, filters)\n        page_urls = paginate_court_urls(search_url, self.parser)\n\n        if max_pages:\n            page_urls = page_urls[:max_pages]\n\n        all_cases = []\n        for i, url in enumerate(page_urls):\n            print(f\"Page {i+1}/{len(page_urls)}: \", end='', flush=True)\n\n            result = self.parser.parse_article(url)\n            if 'cases' in result:\n                cases = result['cases']\n                all_cases.extend(cases)\n                print(f\"{len(cases)} cases\")\n            else:\n                print(\"0 cases\")\n\n        return all_cases\n\n    def cleanup(self):\n        \"\"\"Clean up resources\"\"\"\n        self.parser._close_driver()\n\n# Usage\nextractor = AdvancedCourtExtractor()\nextractor.setup()\n\ntry:\n    cases = extractor.extract_with_progress(\n        'civil', \n        {'date_period': '1m', 'case_category': 'Appeal'},\n        max_pages=3\n    )\n    print(f\"Total extracted: {len(cases)} cases\")\nfinally:\n    extractor.cleanup()\n</code></pre>"},{"location":"reference/parser-classes-overview/#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nfrom datetime import datetime, timedelta\nimport time\nimport json\n\nclass CourtCaseMonitor:\n    def __init__(self, courts=['civil', 'criminal', 'supreme']):\n        self.courts = courts\n        self.last_check = {}\n\n    def check_new_cases(self, hours_back=1):\n        \"\"\"Check for new cases in the last N hours\"\"\"\n        new_cases = {}\n\n        for court in self.courts:\n            print(f\"Checking {court} court...\")\n\n            try:\n                results = extract_court_cases_with_params(\n                    court=court,\n                    date_period='7d',  # Check recent cases\n                    max_pages=3,\n                    output_prefix=f\"monitor_{court}\"\n                )\n\n                if results and results['status'] == 'success':\n                    # Filter for truly new cases\n                    cutoff_time = datetime.now() - timedelta(hours=hours_back)\n                    recent_cases = []\n\n                    for case in results['cases']:\n                        try:\n                            filed_date = datetime.strptime(case['filed_date'], '%m/%d/%Y')\n                            if filed_date &gt; cutoff_time:\n                                recent_cases.append(case)\n                        except:\n                            continue\n\n                    new_cases[court] = recent_cases\n                    print(f\"  Found {len(recent_cases)} new cases\")\n                else:\n                    new_cases[court] = []\n\n            except Exception as e:\n                print(f\"  Error checking {court}: {e}\")\n                new_cases[court] = []\n\n        return new_cases\n\n    def monitor_continuously(self, check_interval_minutes=30):\n        \"\"\"Continuously monitor for new cases\"\"\"\n        while True:\n            print(f\"\\n--- Checking at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\")\n\n            new_cases = self.check_new_cases(hours_back=1)\n\n            # Report findings\n            total_new = sum(len(cases) for cases in new_cases.values())\n            if total_new &gt; 0:\n                print(f\"\ud83d\udea8 Found {total_new} new cases!\")\n                for court, cases in new_cases.items():\n                    if cases:\n                        print(f\"  {court}: {len(cases)} new cases\")\n            else:\n                print(\"\u2713 No new cases found\")\n\n            # Wait for next check\n            print(f\"Next check in {check_interval_minutes} minutes...\")\n            time.sleep(check_interval_minutes * 60)\n\n# Usage\nmonitor = CourtCaseMonitor()\nmonitor.check_new_cases(hours_back=24)  # One-time check\n# monitor.monitor_continuously()  # Continuous monitoring\n</code></pre>"},{"location":"reference/parser-classes-overview/#best-practices","title":"Best Practices","text":""},{"location":"reference/parser-classes-overview/#1-resource-management","title":"1. Resource Management","text":"<pre><code>from opal.court_case_parser import ParserAppealsAL\n\n# Always use try/finally for cleanup\nparser = ParserAppealsAL()\ntry:\n    result = parser.parse_article(url)\n    # Process result...\nfinally:\n    parser._close_driver()\n</code></pre>"},{"location":"reference/parser-classes-overview/#2-error-handling","title":"2. Error Handling","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nimport logging\n\ndef robust_extract(court, **kwargs):\n    max_retries = 3\n\n    for attempt in range(max_retries):\n        try:\n            return extract_court_cases_with_params(court=court, **kwargs)\n        except Exception as e:\n            logging.warning(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(5)  # Wait before retry\n</code></pre>"},{"location":"reference/parser-classes-overview/#3-configuration-management","title":"3. Configuration Management","text":"<pre><code>import json\n\n# Use configuration files\nconfig = {\n    \"courts\": [\"civil\", \"criminal\"],\n    \"default_date_period\": \"1m\",\n    \"max_pages\": 10,\n    \"rate_limit\": 2,\n    \"output_dir\": \"./results\"\n}\n\ndef extract_with_config(config_path):\n    with open(config_path) as f:\n        config = json.load(f)\n\n    results = {}\n    for court in config['courts']:\n        results[court] = extract_court_cases_with_params(\n            court=court,\n            date_period=config['default_date_period'],\n            max_pages=config['max_pages']\n        )\n    return results\n</code></pre>"},{"location":"reference/parser-classes-overview/#4-logging-and-monitoring","title":"4. Logging and Monitoring","text":"<pre><code>import logging\nfrom datetime import datetime\n\n# Set up structured logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(f\"court_extraction_{datetime.now().strftime('%Y%m%d')}.log\"),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger(__name__)\n\n# Log extraction metrics\ndef log_extraction_stats(results):\n    if results and results['status'] == 'success':\n        logger.info(f\"Extraction successful: {results['total_cases']} cases, \"\n                   f\"{results['pages_processed']} pages, \"\n                   f\"completed at {results['extraction_time']}\")\n    else:\n        logger.error(\"Extraction failed\")\n</code></pre> <p>This API reference provides the correct imports, class names, and usage patterns for all OPAL components based on the actual codebase implementation.</p>"},{"location":"reference/parser-daily-news/","title":"ParserDailyNews API Reference","text":"<p>Parser for Alabama Daily News website.</p>"},{"location":"reference/parser-daily-news/#class-definition","title":"Class Definition","text":"<pre><code>class ParserDailyNews(BaseParser):\n    def __init__(self, url=\"https://www.aldailynews.com/\", suffix=\"/news/item\", max_pages=5)\n</code></pre>"},{"location":"reference/parser-daily-news/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>url</code> str <code>\"https://www.aldailynews.com/\"</code> Base URL for Alabama Daily News <code>suffix</code> str <code>\"/news/item\"</code> URL suffix for article links <code>max_pages</code> int <code>5</code> Maximum pages to scrape"},{"location":"reference/parser-daily-news/#methods","title":"Methods","text":""},{"location":"reference/parser-daily-news/#extract_article_data","title":"extract_article_data()","text":"<p><pre><code>def extract_article_data(self) -&gt; List[Dict]\n</code></pre> Extracts articles from Alabama Daily News.</p> <p>Returns: List of article dictionaries with keys: - <code>title</code>: Article headline - <code>content</code>: Full article text - <code>date</code>: Publication date - <code>author</code>: Article author - <code>url</code>: Article URL - <code>category</code>: Article category</p>"},{"location":"reference/parser-daily-news/#get_article_links","title":"get_article_links()","text":"<p><pre><code>def get_article_links(self, page_url: str) -&gt; List[str]\n</code></pre> Extracts article URLs from an Alabama Daily News page.</p> <p>Parameters: - <code>page_url</code>: URL of the page to scrape</p> <p>Returns: List of article URLs</p>"},{"location":"reference/parser-daily-news/#parse_article","title":"parse_article()","text":"<p><pre><code>def parse_article(self, article_url: str) -&gt; Dict\n</code></pre> Parses individual Alabama Daily News article.</p> <p>Parameters: - <code>article_url</code>: URL of the article</p> <p>Returns: Dictionary with article data</p>"},{"location":"reference/parser-daily-news/#usage-example","title":"Usage Example","text":"<pre><code>from opal.ParserDailyNews import ParserDailyNews\n\n# Create parser instance\nparser = ParserDailyNews(\n    url=\"https://www.aldailynews.com/\",\n    suffix=\"/news/item\",\n    max_pages=5\n)\n\n# Extract articles\narticles = parser.extract_article_data()\n\n# Save to JSON\nparser.save_to_json(articles, \"adn_articles.json\")\n\n# Process articles\nfor article in articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"Category: {article['category']}\")\n    print(f\"Date: {article['date']}\")\n</code></pre>"},{"location":"reference/parser-daily-news/#output-format","title":"Output Format","text":"<pre><code>{\n  \"title\": \"Governor Announces Infrastructure Plan\",\n  \"content\": \"Full article text...\",\n  \"date\": \"2024-01-15\",\n  \"author\": \"Jane Doe\",\n  \"url\": \"https://www.aldailynews.com/news/item/infrastructure-plan\",\n  \"category\": \"Politics\"\n}\n</code></pre>"},{"location":"reference/parser-daily-news/#notes","title":"Notes","text":"<ul> <li>Handles Alabama Daily News specific HTML structure</li> <li>Extracts category information when available</li> <li>Supports pagination through page parameters</li> <li>Includes robust error handling for missing elements</li> </ul>"},{"location":"user-guide/cli-reference/","title":"CLI Reference","text":"<p>OPAL provides command-line tools for scraping Alabama news sites and court records. For the easiest way to build commands, use our Interactive Command Builder which provides a visual interface with examples and real-time validation.</p>"},{"location":"user-guide/cli-reference/#command-structure","title":"Command Structure","text":""},{"location":"user-guide/cli-reference/#basic-syntax","title":"Basic Syntax","text":"<pre><code>python -m opal --url &lt;URL&gt; --parser &lt;PARSER&gt; [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli-reference/#available-tools","title":"Available Tools","text":"<ol> <li> <p>Main OPAL CLI - For news articles and basic court scraping    <pre><code>python -m opal --url &lt;URL&gt; --parser &lt;PARSER&gt; [options]\n</code></pre></p> </li> <li> <p>Configurable Court Extractor - For advanced court searches with filters    <pre><code>python -m opal.configurable_court_extractor --court &lt;TYPE&gt; [options]\n</code></pre></p> </li> </ol>"},{"location":"user-guide/cli-reference/#common-examples","title":"Common Examples","text":""},{"location":"user-guide/cli-reference/#news-scraping","title":"News Scraping","text":"<p>1819 News Articles: <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5\n</code></pre></p> <p>Alabama Daily News: <pre><code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 3\n</code></pre></p>"},{"location":"user-guide/cli-reference/#court-records","title":"Court Records","text":"<p>Basic Court Scraping: <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre></p> <p>Advanced Court Search: <pre><code># Last 7 days of civil cases, excluding closed\npython -m opal.configurable_court_extractor --court civil --date-period 7d --exclude-closed\n\n# Last month of criminal cases, CSV output\npython -m opal.configurable_court_extractor --court criminal --date-period 1m --output-csv\n</code></pre></p>"},{"location":"user-guide/cli-reference/#complete-parameter-reference","title":"Complete Parameter Reference","text":""},{"location":"user-guide/cli-reference/#main-opal-parameters","title":"Main OPAL Parameters","text":"Parameter Description Required Example <code>--url</code> Base URL of the website to scrape Yes <code>https://1819news.com/</code> <code>--parser</code> Parser to use (<code>Parser1819</code>, <code>ParserDailyNews</code>, <code>court</code>) Yes <code>Parser1819</code> <code>--suffix</code> URL suffix to filter articles No <code>/news/item</code> <code>--max_pages</code> Maximum number of pages to scrape No <code>5</code>"},{"location":"user-guide/cli-reference/#court-extractor-parameters","title":"Court Extractor Parameters","text":"Parameter Description Default Example <code>--court</code> Court type (<code>civil</code>, <code>criminal</code>, <code>all</code>) <code>all</code> <code>civil</code> <code>--date-period</code> Time period (<code>7d</code>, <code>1m</code>, <code>3m</code>, <code>6m</code>, <code>1y</code>) <code>1m</code> <code>7d</code> <code>--exclude-closed</code> Exclude closed cases False <code>--exclude-closed</code> <code>--output-csv</code> Also save as CSV file False <code>--output-csv</code> <code>--case-number</code> Search specific case number None <code>2024-CA-001</code> <code>--filed-after</code> Cases filed after date (YYYY-MM-DD) None <code>2024-01-01</code> <code>--filed-before</code> Cases filed before date (YYYY-MM-DD) None <code>2024-12-31</code>"},{"location":"user-guide/cli-reference/#getting-help","title":"Getting Help","text":""},{"location":"user-guide/cli-reference/#command-line-help","title":"Command-Line Help","text":"<pre><code># Main OPAL help\npython -m opal --help\n\n# Court extractor help\npython -m opal.configurable_court_extractor --help\n</code></pre>"},{"location":"user-guide/cli-reference/#interactive-resources","title":"Interactive Resources","text":"<ul> <li>\ud83d\udee0\ufe0f Interactive Command Builder - Build commands visually with examples</li> <li>\ud83d\udccb Common Use Cases - Real-world scenarios and workflows</li> <li>\ud83d\udcbe Output Examples - See what data you'll get</li> <li>\ud83d\ude80 Quick Start Tutorial - Step-by-step first scrape</li> </ul>"},{"location":"user-guide/cli-reference/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Start Small: Use <code>--max_pages 1</code> when testing to verify your command works</li> <li>Use Suffixes: For news sites, use <code>--suffix</code> to filter only article pages</li> <li>Check Output: Review the JSON output format in Output Examples</li> <li>Rate Limiting: The court scraper includes automatic delays to respect server limits</li> <li>Virtual Environment: Always run OPAL in a virtual environment to avoid conflicts</li> </ol>"},{"location":"user-guide/cli-reference/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter errors: 1. Check your URL is correct and accessible 2. Verify you're using the right parser for your target site 3. Ensure Chrome is installed (for court scraping) 4. See Understanding Errors for detailed error explanations</p>"},{"location":"user-guide/command-builder/","title":"Interactive Command Builder","text":"<p>Build your OPAL commands with this interactive tool. Select your options and get the exact command to run.</p>"},{"location":"user-guide/command-builder/#quick-examples-gallery","title":"Quick Examples Gallery","text":"<p>Click any example to load it into the builder:</p> \ud83d\udcf0 Basic News Scraping <p>Scrape recent articles from 1819 News</p> <code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 5</code> \u2696\ufe0f Basic Court Cases <p>Extract court cases from Alabama Appeals</p> <code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court --max_pages 3</code> \ud83d\udcc5 Daily News Digest <p>Quick daily update from Alabama Daily News</p> <code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 2</code> \ud83c\udfdb\ufe0f Civil Court Search <p>Search civil court cases from last week</p> <code>python -m opal.configurable_court_extractor --court civil --date-period 7d --exclude-closed</code> \ud83d\udd0d Custom Court Search <p>Advanced court search with filters</p> <code>python -m opal.configurable_court_extractor --court civil --date-period 1m --case-category Appeal --max-pages 10</code> \ud83d\udd2c Research Mode <p>Comprehensive data collection for analysis</p> <code>python -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 20</code>"},{"location":"user-guide/command-builder/#interactive-command-builder_1","title":"Interactive Command Builder","text":"Build Your Command Command Type: Basic OPAL (python -m opal) Court Extractor (python -m opal.configurable_court_extractor) URL (required)                     The website URL to scrape. Must be a supported site. Select a website... 1819 News Alabama Daily News Alabama Appeals Court Custom URL... Parser (required)                     The parser that matches your chosen website. Auto-detect from URL Parser1819 (for 1819news.com) ParserDailyNews (for aldailynews.com) court (for court portal) URL Suffix (optional)                     Filter URLs containing this pattern. Helps identify article pages. Max Pages (optional)                     Maximum number of pages to process. Default is 5 for basic, unlimited for court extractor. Output File (optional)                     Where to save results. Default generates timestamped filename. Court Type (required)                     Which Alabama court to search: civil, criminal, or supreme. Select court... Civil Appeals Court Criminal Appeals Court Supreme Court Date Period                     How far back to search for cases. Recent periods are faster. No date filter Last 7 days Last month Last 3 months Last 6 months Last year Custom date range... Start Date (YYYY-MM-DD): End Date (YYYY-MM-DD): Case Category (optional)                     Filter by type of legal proceeding. All categories Appeal Certiorari Original Proceeding Petition Certified Question Case Number Filter (optional)                     Filter by case number pattern. Use * for wildcards. Case Title Filter (optional)                     Filter by words in case title. Exclude Closed Cases                     Only show active/pending cases, skip completed ones. Max Pages (optional)                     Limit pages processed. Useful for large result sets. Output Prefix (optional)                     Prefix for output filenames. Will generate both JSON and CSV."},{"location":"user-guide/command-builder/#generated-command","title":"Generated Command","text":"Copy Select options above to generate your command... \ud83d\udca1 Pro Tip: After copying your command, open your terminal, activate your virtual environment with <code>source venv/bin/activate</code> (or <code>venv\\Scripts\\activate</code> on Windows), then paste and run the command."},{"location":"user-guide/command-builder/#command-validation","title":"Command Validation","text":""},{"location":"user-guide/command-builder/#what-happens-next","title":"What Happens Next?","text":"<p>After running your command:</p> <ol> <li>For News Scraping: You'll get a JSON file with articles including title, author, date, and full content</li> <li>For Court Scraping: You'll get both JSON and CSV files with case details, plus progress updates in the terminal</li> <li>Files are named with timestamps for easy organization</li> <li>Check the terminal output for any warnings or helpful information</li> </ol>"},{"location":"user-guide/command-builder/#advanced-options","title":"Advanced Options","text":""},{"location":"user-guide/command-builder/#environment-variables","title":"Environment Variables","text":"<p>You can set these environment variables to customize behavior:</p> <pre><code># Set default output directory\nexport OPAL_OUTPUT_DIR=\"/path/to/outputs\"\n\n# Set default rate limiting (seconds between requests)\nexport OPAL_RATE_LIMIT=2.0\n\n# Enable debug mode\nexport OPAL_DEBUG=1\n</code></pre>"},{"location":"user-guide/command-builder/#combining-with-other-tools","title":"Combining with Other Tools","text":"<p>Process results with jq (JSON processor): <pre><code># Extract just article titles\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 2 \\\n  &amp;&amp; cat *Parser1819*.json | jq '.articles[].title'\n\n# Count court cases by status\npython -m opal.configurable_court_extractor --court civil --date-period 7d \\\n  &amp;&amp; cat *civil*.json | jq '.cases | group_by(.status) | map({status: .[0].status, count: length})'\n</code></pre></p> <p>Save to specific directory: <pre><code>mkdir -p ~/opal-results/$(date +%Y-%m)\ncd ~/opal-results/$(date +%Y-%m)\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 5\n</code></pre></p>"},{"location":"user-guide/command-builder/#need-help","title":"Need Help?","text":"<ul> <li>Command not working? Check the Understanding Errors guide</li> <li>Want to see examples? Visit Common Use Cases </li> <li>Need setup help? Try the Prerequisites Checker</li> </ul>"},{"location":"user-guide/common-use-cases/","title":"Common Use Cases","text":"<p>This guide shows you how to use OPAL for specific real-world scenarios. Each use case includes the exact commands, expected outputs, and practical tips.</p>"},{"location":"user-guide/common-use-cases/#news-monitoring-use-cases","title":"News Monitoring Use Cases","text":""},{"location":"user-guide/common-use-cases/#use-case-1-track-weekly-political-news","title":"Use Case 1: Track Weekly Political News","text":"<p>Scenario: You want to monitor Alabama political news from both major news sources for the past week.</p> <p>Commands: <pre><code># Scrape 1819 News for political articles\npython -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 10 --output weekly_1819_politics.json\n\n# Scrape Alabama Daily News for political articles  \npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /news/item --max_pages 10 --output weekly_daily_politics.json\n</code></pre></p> <p>Expected Output: - Two JSON files with 50-100 articles each - Articles from the past week (news sites typically show recent content first) - Processing time: 5-10 minutes per source</p> <p>Pro Tips: - Run these commands on the same day each week for consistency - Use <code>--max_pages 5</code> for faster results if you just want recent highlights - Save files with dates: <code>weekly_1819_2024-01-15.json</code></p>"},{"location":"user-guide/common-use-cases/#use-case-2-research-specific-topics","title":"Use Case 2: Research Specific Topics","text":"<p>Scenario: You're researching coverage of education policy in Alabama news.</p> <p>Commands: <pre><code># Collect all recent articles from both sources\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 20 --output education_research_1819.json\n\npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 20 --output education_research_daily.json\n</code></pre></p> <p>Post-Processing (filter for education topics): <pre><code>import json\n\n# Load the data\nwith open('education_research_1819.json', 'r') as f:\n    data = json.load(f)\n\n# Filter for education-related articles\neducation_keywords = ['education', 'school', 'teacher', 'student', 'classroom', 'university', 'college']\neducation_articles = []\n\nfor article in data['articles']:\n    title_lower = article['title'].lower()\n    content_lower = article['content'].lower()\n\n    if any(keyword in title_lower or keyword in content_lower for keyword in education_keywords):\n        education_articles.append(article)\n\nprint(f\"Found {len(education_articles)} education-related articles out of {len(data['articles'])} total\")\n\n# Save filtered results\nfiltered_data = {\n    'articles': education_articles,\n    'metadata': data['metadata'],\n    'filter_applied': 'education keywords',\n    'original_count': len(data['articles']),\n    'filtered_count': len(education_articles)\n}\n\nwith open('education_articles_filtered.json', 'w') as f:\n    json.dump(filtered_data, f, indent=2)\n</code></pre></p> <p>Expected Output: - 200-500 total articles collected - 20-50 education-related articles after filtering - Research-ready dataset for analysis</p>"},{"location":"user-guide/common-use-cases/#use-case-3-daily-news-digest","title":"Use Case 3: Daily News Digest","text":"<p>Scenario: You want a daily digest of top stories from Alabama news sources.</p> <p>Commands: <pre><code># Get just the latest articles (first 2-3 pages usually contain today's news)\npython -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 3 --output daily_digest_1819.json\n\npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /news/item --max_pages 3 --output daily_digest_daily.json\n</code></pre></p> <p>Automation Setup (Linux/Mac): <pre><code># Create a daily script\ncat &lt;&lt; 'EOF' &gt; daily_digest.sh\n#!/bin/bash\nDATE=$(date +%Y-%m-%d)\ncd /path/to/opal_project\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Run daily scrapes\npython -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 3 --output \"daily_${DATE}_1819.json\"\npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /news/item --max_pages 3 --output \"daily_${DATE}_daily.json\"\n\necho \"Daily digest complete for $DATE\"\nEOF\n\n# Make executable and add to cron\nchmod +x daily_digest.sh\n\n# Add to crontab (runs every day at 8 AM)\necho \"0 8 * * * /path/to/daily_digest.sh\" | crontab -\n</code></pre></p> <p>Expected Output: - 20-40 articles per source - Processing time: 2-3 minutes total - Consistent daily data collection</p>"},{"location":"user-guide/common-use-cases/#court-monitoring-use-cases","title":"Court Monitoring Use Cases","text":""},{"location":"user-guide/common-use-cases/#use-case-4-weekly-court-case-review","title":"Use Case 4: Weekly Court Case Review","text":"<p>Scenario: You want to track new court cases filed each week.</p> <p>Commands: <pre><code># Basic weekly court scraping\npython -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court --max_pages 5 --output weekly_court_cases.json\n\n# Or use the configurable extractor for more control\npython -m opal.configurable_court_extractor --court civil --date-period 7d --exclude-closed --max-pages 10 --output-prefix weekly_civil\n</code></pre></p> <p>Expected Output: - 100-300 court cases - Both JSON and CSV formats - Cases from the past week - Processing time: 10-15 minutes (court scraping is slower due to JavaScript)</p>"},{"location":"user-guide/common-use-cases/#use-case-5-research-specific-case-types","title":"Use Case 5: Research Specific Case Types","text":"<p>Scenario: You're researching civil appeals related to business disputes.</p> <p>Commands: <pre><code># Search civil court for appeals\npython -m opal.configurable_court_extractor \\\n    --court civil \\\n    --date-period 6m \\\n    --case-category Appeal \\\n    --exclude-closed \\\n    --max-pages 15 \\\n    --output-prefix business_appeals_research\n</code></pre></p> <p>Post-Processing (filter for business cases): <pre><code>import json\n\n# Load court case data\nwith open('business_appeals_research_civil.json', 'r') as f:\n    data = json.load(f)\n\n# Filter for business-related cases\nbusiness_keywords = ['llc', 'corp', 'company', 'business', 'contract', 'commercial', 'partnership']\nbusiness_cases = []\n\nfor case in data['cases']:\n    title_lower = case['case_title'].lower()\n\n    if any(keyword in title_lower for keyword in business_keywords):\n        business_cases.append(case)\n\nprint(f\"Found {len(business_cases)} business-related cases out of {len(data['cases'])} total\")\n\n# Save filtered results\nfiltered_data = {\n    'cases': business_cases,\n    'metadata': data.get('metadata', {}),\n    'filter_applied': 'business keywords',\n    'search_parameters': data.get('search_parameters', {}),\n    'original_count': len(data['cases']),\n    'filtered_count': len(business_cases)\n}\n\nwith open('business_cases_filtered.json', 'w') as f:\n    json.dump(filtered_data, f, indent=2)\n</code></pre></p> <p>Expected Output: - 500-1000 total civil appeals - 50-150 business-related cases after filtering - Detailed case information for legal research</p>"},{"location":"user-guide/common-use-cases/#use-case-6-monitor-specific-courts","title":"Use Case 6: Monitor Specific Courts","text":"<p>Scenario: You need to track all activity in the Criminal Appeals Court.</p> <p>Commands: <pre><code># Criminal court cases from the last month\npython -m opal.configurable_court_extractor \\\n    --court criminal \\\n    --date-period 1m \\\n    --max-pages 20 \\\n    --output-prefix monthly_criminal\n</code></pre></p> <p>Expected Output: - 200-600 criminal cases - All case types (appeals, petitions, etc.) - Complete case information - Processing time: 15-25 minutes</p>"},{"location":"user-guide/common-use-cases/#analysis-and-research-use-cases","title":"Analysis and Research Use Cases","text":""},{"location":"user-guide/common-use-cases/#use-case-7-comparative-news-analysis","title":"Use Case 7: Comparative News Analysis","text":"<p>Scenario: Compare how different news sources cover the same topics.</p> <p>Commands: <pre><code># Collect comprehensive data from both sources\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 25 --output analysis_1819_full.json\npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 25 --output analysis_daily_full.json\n</code></pre></p> <p>Analysis Script: <pre><code>import json\nfrom collections import Counter\nimport re\n\ndef analyze_news_coverage(file1, file2, source1_name, source2_name):\n    # Load both datasets\n    with open(file1, 'r') as f:\n        data1 = json.load(f)\n    with open(file2, 'r') as f:\n        data2 = json.load(f)\n\n    # Extract common keywords\n    def extract_keywords(articles):\n        all_text = ' '.join([article['title'] + ' ' + article['content'] for article in articles])\n        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_text.lower())\n        return Counter(words)\n\n    keywords1 = extract_keywords(data1['articles'])\n    keywords2 = extract_keywords(data2['articles'])\n\n    # Find common topics\n    common_keywords = set(keywords1.keys()) &amp; set(keywords2.keys())\n\n    print(f\"\\n=== NEWS COVERAGE COMPARISON ===\")\n    print(f\"{source1_name}: {len(data1['articles'])} articles\")\n    print(f\"{source2_name}: {len(data2['articles'])} articles\")\n    print(f\"Common topics: {len(common_keywords)}\")\n\n    # Top topics by source\n    print(f\"\\nTop topics in {source1_name}:\")\n    for word, count in keywords1.most_common(10):\n        print(f\"  {word}: {count}\")\n\n    print(f\"\\nTop topics in {source2_name}:\")\n    for word, count in keywords2.most_common(10):\n        print(f\"  {word}: {count}\")\n\n# Run the analysis\nanalyze_news_coverage('analysis_1819_full.json', 'analysis_daily_full.json', '1819 News', 'Alabama Daily News')\n</code></pre></p> <p>Expected Output: - 500-1000 articles per source - Keyword frequency analysis - Topic comparison between sources - Research insights</p>"},{"location":"user-guide/common-use-cases/#use-case-8-long-term-trend-monitoring","title":"Use Case 8: Long-term Trend Monitoring","text":"<p>Scenario: Track political discourse trends over time.</p> <p>Setup: Run this monthly for trend analysis:</p> <pre><code># Monthly data collection script\n#!/bin/bash\nMONTH=$(date +%Y-%m)\n\n# Create monthly folder\nmkdir -p \"monthly_data/$MONTH\"\ncd \"monthly_data/$MONTH\"\n\n# Collect comprehensive data\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 50 --output \"1819_${MONTH}.json\"\npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 50 --output \"daily_${MONTH}.json\"\n\n# Court data\npython -m opal.configurable_court_extractor --court civil --date-period 1m --max-pages 20 --output-prefix \"court_${MONTH}\"\n\necho \"Monthly data collection complete for $MONTH\"\n</code></pre> <p>Trend Analysis: <pre><code>import json\nimport os\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndef analyze_monthly_trends(data_folder):\n    monthly_data = {}\n\n    # Load all monthly files\n    for month_folder in os.listdir(data_folder):\n        month_path = os.path.join(data_folder, month_folder)\n        if os.path.isdir(month_path):\n            # Load 1819 News data\n            file_path = os.path.join(month_path, f\"1819_{month_folder}.json\")\n            if os.path.exists(file_path):\n                with open(file_path, 'r') as f:\n                    data = json.load(f)\n                    monthly_data[month_folder] = len(data['articles'])\n\n    # Create trend chart\n    months = sorted(monthly_data.keys())\n    article_counts = [monthly_data[month] for month in months]\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(months, article_counts, marker='o')\n    plt.title('Monthly Article Count Trends')\n    plt.xlabel('Month')\n    plt.ylabel('Number of Articles')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig('monthly_trends.png')\n    plt.show()\n\n    print(f\"Trend analysis complete. Chart saved as 'monthly_trends.png'\")\n\n# Run trend analysis\nanalyze_monthly_trends('monthly_data')\n</code></pre></p>"},{"location":"user-guide/common-use-cases/#performance-and-efficiency-tips","title":"Performance and Efficiency Tips","text":""},{"location":"user-guide/common-use-cases/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Start Small: Always test with <code>--max_pages 2-3</code> first</li> <li>Peak Hours: Avoid scraping during business hours (9 AM - 5 PM CST) for better performance</li> <li>Batch Processing: Run multiple scrapers in sequence, not parallel</li> <li>Storage: Use dated folders to organize output files</li> </ol>"},{"location":"user-guide/common-use-cases/#resource-management","title":"Resource Management","text":"<pre><code># Good practice: organized data collection\nDATE=$(date +%Y-%m-%d)\nmkdir -p \"data/$DATE\"\ncd \"data/$DATE\"\n\n# Run scrapers with reasonable limits\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 10 --output \"1819_${DATE}.json\"\n# Wait between scrapers to be respectful\nsleep 30\npython -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --max_pages 10 --output \"daily_${DATE}.json\"\n</code></pre>"},{"location":"user-guide/common-use-cases/#error-recovery","title":"Error Recovery","text":"<pre><code># Robust scraping with retry logic\n#!/bin/bash\nMAX_RETRIES=3\nRETRY_COUNT=0\n\nwhile [ $RETRY_COUNT -lt $MAX_RETRIES ]; do\n    if python -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 5 --output \"test_output.json\"; then\n        echo \"Scraping successful!\"\n        break\n    else\n        RETRY_COUNT=$((RETRY_COUNT + 1))\n        echo \"Attempt $RETRY_COUNT failed. Retrying in 60 seconds...\"\n        sleep 60\n    fi\ndone\n\nif [ $RETRY_COUNT -eq $MAX_RETRIES ]; then\n    echo \"Scraping failed after $MAX_RETRIES attempts\"\n    exit 1\nfi\n</code></pre>"},{"location":"user-guide/common-use-cases/#next-steps","title":"Next Steps","text":"<p>After implementing these use cases:</p> <ol> <li>Automate Regular Collection: Set up cron jobs or scheduled tasks</li> <li>Data Analysis: Use Python libraries like pandas for deeper analysis</li> <li>Visualization: Create charts and graphs from your collected data</li> <li>Integration: Connect OPAL data to databases or other analysis tools</li> </ol> <p>For troubleshooting specific issues, see Understanding Errors.</p> <p>For working with the collected data, see Working with Output Data.</p>"},{"location":"user-guide/configurable_court_extractor/","title":"Configurable Court Extractor","text":"<p>The Configurable Court Extractor (<code>opal.configurable_court_extractor</code>) provides advanced searching and filtering capabilities for the Alabama Appeals Court portal. It includes both programmatic and command-line interfaces for extracting court data with precise control over search parameters.</p>"},{"location":"user-guide/configurable_court_extractor/#overview","title":"Overview","text":"<p>The configurable court extractor consists of:</p> <ul> <li><code>CourtSearchBuilder</code> class for building complex search parameters</li> <li><code>extract_court_cases_with_params()</code> function for programmatic extraction</li> <li>Command-line interface for terminal-based extraction</li> </ul> <p>It supports: - Dynamic court selection (Civil, Criminal, Supreme) - Advanced search filtering - Custom URL support for pre-built searches - Multiple output formats (JSON, CSV)</p>"},{"location":"user-guide/configurable_court_extractor/#classes-and-functions","title":"Classes and Functions","text":""},{"location":"user-guide/configurable_court_extractor/#courtsearchbuilder","title":"CourtSearchBuilder","text":"<p>A builder class for constructing Alabama Court search URLs with court-specific parameters.</p> <pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\n\n# Create search builder\nbuilder = CourtSearchBuilder()\n\n# Configure search parameters\nbuilder.set_court(\"civil\")\nbuilder.set_date_range(period=\"1m\")  # Last month\nbuilder.set_case_category(\"Appeal\")\nbuilder.set_exclude_closed(True)\n\n# Build the search URL\nsearch_url = builder.build_url(page_number=0)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#available-methods","title":"Available Methods","text":"<p>Court Configuration: - <code>set_court(court_key)</code> - Set court type ('civil', 'criminal', 'supreme') - <code>get_court_info()</code> - Get information about current court - <code>discover_court_ids(parser_instance)</code> - Auto-discover court IDs from website - <code>set_court_id_manually(court_key, court_id)</code> - Manually set court ID</p> <p>Date Filtering: - <code>set_date_range(start_date=None, end_date=None, period='1y')</code> - Set date filter   - Periods: '7d', '1m', '3m', '6m', '1y', 'custom'   - Custom requires start_date and end_date</p> <p>Case Filtering: - <code>set_case_category(category_name=None)</code> - Filter by case type   - Categories: 'Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question' - <code>set_case_number_filter(case_number=None)</code> - Filter by case number - <code>set_case_title_filter(title=None)</code> - Filter by case title - <code>set_exclude_closed(exclude=False)</code> - Exclude closed cases</p> <p>URL Building: - <code>build_url(page_number=0)</code> - Build complete search URL - <code>build_criteria_string()</code> - Build URL criteria parameters</p>"},{"location":"user-guide/configurable_court_extractor/#extract_court_cases_with_params","title":"extract_court_cases_with_params()","text":"<p>Main extraction function that supports both parameter-based and URL-based searches.</p> <pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\n\n# Parameter-based search\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='1m',\n    case_category='Appeal',\n    exclude_closed=True,\n    max_pages=5,\n    output_prefix=\"civil_appeals\"\n)\n\n# Custom URL search\nresults = extract_court_cases_with_params(\n    custom_url=\"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\",\n    max_pages=5,\n    output_prefix=\"custom_search\"\n)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#parameters","title":"Parameters","text":"<p>Search Parameters (ignored if custom_url provided): - <code>court</code> (str): Court type ('civil', 'criminal', 'supreme') - <code>date_period</code> (str): Date period ('7d', '1m', '3m', '6m', '1y', 'custom') - <code>start_date</code> (str): Start date for custom range (YYYY-MM-DD) - <code>end_date</code> (str): End date for custom range (YYYY-MM-DD) - <code>case_number</code> (str): Case number filter (partial match) - <code>case_title</code> (str): Case title filter (partial match) - <code>case_category</code> (str): Case category filter - <code>exclude_closed</code> (bool): Whether to exclude closed cases</p> <p>Processing Options: - <code>max_pages</code> (int): Maximum pages to process (None for all) - <code>output_prefix</code> (str): Prefix for output files - <code>custom_url</code> (str): Pre-built search URL (overrides all search params)</p>"},{"location":"user-guide/configurable_court_extractor/#court-definitions","title":"Court Definitions","text":"<p>The system supports three Alabama Appeals Courts:</p> <pre><code>courts = {\n    'civil': {\n        'name': 'Alabama Civil Court of Appeals',\n        'case_prefix': 'CL',\n        'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n    },\n    'criminal': {\n        'name': 'Alabama Court of Criminal Appeals', \n        'case_prefix': 'CR',\n        'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition']\n    },\n    'supreme': {\n        'name': 'Alabama Supreme Court',\n        'case_prefix': 'SC',\n        'categories': ['Appeal', 'Certiorari', 'Original Proceeding', 'Petition', 'Certified Question']\n    }\n}\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#command-line-interface","title":"Command-Line Interface","text":""},{"location":"user-guide/configurable_court_extractor/#basic-usage","title":"Basic Usage","text":"<pre><code># Extract civil court cases from last month\npython -m opal.configurable_court_extractor --court civil --date-period 1m\n\n# Extract criminal appeals with custom date range\npython -m opal.configurable_court_extractor \\\n    --court criminal \\\n    --date-period custom \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-31 \\\n    --case-category Appeal\n\n# Use custom URL\npython -m opal.configurable_court_extractor \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#cli-options","title":"CLI Options","text":"<p>URL Option (overrides all search parameters): - <code>--url</code> - Pre-built search URL with embedded parameters</p> <p>Search Parameters (ignored if --url provided): - <code>--court {civil,criminal,supreme}</code> - Court to search (default: civil) - <code>--date-period {7d,1m,3m,6m,1y,custom}</code> - Date period (default: 1y) - <code>--start-date YYYY-MM-DD</code> - Start date for custom range - <code>--end-date YYYY-MM-DD</code> - End date for custom range - <code>--case-number TEXT</code> - Case number filter - <code>--case-title TEXT</code> - Case title filter - <code>--case-category {Appeal,Certiorari,Original Proceeding,Petition,Certified Question}</code> - Case category - <code>--exclude-closed</code> - Exclude closed cases</p> <p>Output Options: - <code>--max-pages INT</code> - Maximum pages to process - <code>--output-prefix TEXT</code> - Prefix for output files (default: court_cases)</p>"},{"location":"user-guide/configurable_court_extractor/#programmatic-usage-examples","title":"Programmatic Usage Examples","text":""},{"location":"user-guide/configurable_court_extractor/#basic-search","title":"Basic Search","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder, extract_court_cases_with_params\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Simple extraction\nresults = extract_court_cases_with_params(\n    court='civil',\n    date_period='7d',\n    exclude_closed=True\n)\n\nif results and results['status'] == 'success':\n    print(f\"Found {results['total_cases']} cases\")\n    for case in results['cases']:\n        print(f\"- {case['case_number']['text']}: {case['case_title']}\")\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#advanced-search-with-builder","title":"Advanced Search with Builder","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\n\n# Create builder and parser\nbuilder = CourtSearchBuilder()\nparser = ParserAppealsAL(headless=True)\n\n# Discover court IDs\nbuilder.discover_court_ids(parser)\n\n# Configure search\nbuilder.set_court('supreme')\nbuilder.set_date_range(start_date='2024-01-01', end_date='2024-03-31', period='custom')\nbuilder.set_case_category('Certiorari')\nbuilder.set_exclude_closed(True)\n\n# Get search URL and extract\nsearch_url = builder.build_url()\nresults = extract_court_cases_with_params(custom_url=search_url)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#processing-custom-urls","title":"Processing Custom URLs","text":"<pre><code># Handle session-based URLs from website\ncustom_url = \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n\nresults = extract_court_cases_with_params(\n    custom_url=custom_url,\n    max_pages=10,\n    output_prefix=\"session_search\"\n)\n\n# The function will warn about session expiration\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#court-id-discovery","title":"Court ID Discovery","text":"<p>The system automatically discovers court IDs from the website:</p> <pre><code>builder = CourtSearchBuilder()\nparser = ParserAppealsAL()\n\n# Auto-discover court IDs\nbuilder.discover_court_ids(parser)\n\n# Check discovery results\nfor court_key, court_info in builder.courts.items():\n    print(f\"{court_info['name']}: {court_info['id']}\")\n\n# Manual override if needed\nbuilder.set_court_id_manually('civil', '68f021c4-6a44-4735-9a76-5360b2e8af13')\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#output-format","title":"Output Format","text":""},{"location":"user-guide/configurable_court_extractor/#json-output-structure","title":"JSON Output Structure","text":"<pre><code>{\n    \"status\": \"success\",\n    \"search_parameters\": {\n        \"court\": \"civil\",\n        \"date_period\": \"1m\",\n        \"case_category\": \"Appeal\",\n        \"exclude_closed\": true\n    },\n    \"total_cases\": 25,\n    \"extraction_date\": \"2024-01-15\",\n    \"extraction_time\": \"14:30:22\",\n    \"pages_processed\": 2,\n    \"cases\": [\n        {\n            \"court\": \"Alabama Civil Court of Appeals\",\n            \"case_number\": {\n                \"text\": \"CL-2024-0123\",\n                \"link\": \"/portal/case/detail/12345\"\n            },\n            \"case_title\": \"Smith v. Jones\",\n            \"classification\": \"Appeal\",\n            \"filed_date\": \"01/10/2024\",\n            \"status\": \"Open\"\n        }\n    ]\n}\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#csv-output","title":"CSV Output","text":"<p>Automatically generated alongside JSON: - Court, Case Number, Case Title, Classification, Filed Date, Status, Case Link</p>"},{"location":"user-guide/configurable_court_extractor/#error-handling-and-warnings","title":"Error Handling and Warnings","text":""},{"location":"user-guide/configurable_court_extractor/#session-url-warnings","title":"Session URL Warnings","text":"<p>When using custom URLs:</p> <pre><code>\u26a0\ufe0f  WARNING: Custom URLs contain session-specific parameters that expire.\n   This URL will only work temporarily and may become invalid after your browser session ends.\n   For reliable, repeatable searches, use the CLI search parameters instead of --url option.\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#court-id-discovery-failures","title":"Court ID Discovery Failures","text":"<pre><code># Graceful fallback to known IDs\nif court_info['id'] is None:\n    raise ValueError(f\"Could not discover court ID for {court_name}. \"\n                   \"Try using the --url option with a pre-built search URL instead.\")\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#integration-with-other-components","title":"Integration with Other Components","text":""},{"location":"user-guide/configurable_court_extractor/#with-parserappealsal","title":"With ParserAppealsAL","text":"<pre><code>from opal.configurable_court_extractor import CourtSearchBuilder\nfrom opal.court_case_parser import ParserAppealsAL\n\n# The extractor uses ParserAppealsAL internally\nparser = ParserAppealsAL(headless=True, rate_limit_seconds=2)\nbuilder = CourtSearchBuilder()\n\n# Discovery requires parser instance\nbuilder.discover_court_ids(parser)\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#with-court-url-paginator","title":"With Court URL Paginator","text":"<pre><code>from opal.configurable_court_extractor import extract_court_cases_with_params\nfrom opal.court_url_paginator import parse_court_url\n\n# Extract cases and check pagination\nresults = extract_court_cases_with_params(court='civil', date_period='1m')\n\n# The function internally handles pagination automatically\nprint(f\"Processed {results['pages_processed']} pages\")\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Court ID Discovery: Requires web scraping, cached after first discovery</li> <li>Rate Limiting: Built-in 2-second delays between requests</li> <li>Memory Usage: Processes pages sequentially to manage memory</li> <li>Session Management: Custom URLs expire, use parameters for reliability</li> </ul>"},{"location":"user-guide/configurable_court_extractor/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configurable_court_extractor/#common-issues","title":"Common Issues","text":"<ol> <li>Court ID Discovery Fails</li> <li>Use <code>--url</code> option with pre-built search URL</li> <li> <p>Manually set court IDs with <code>set_court_id_manually()</code></p> </li> <li> <p>Custom URL Stops Working</p> </li> <li>URLs are session-based and expire</li> <li>Switch to parameter-based search</li> <li> <p>Create new search on website to get fresh URL</p> </li> <li> <p>No Results Found</p> </li> <li>Check date range (default is last year)</li> <li>Verify court has cases in date range</li> <li>Try broader search criteria</li> </ol>"},{"location":"user-guide/configurable_court_extractor/#debug-mode","title":"Debug Mode","text":"<pre><code># Run with headless=False to see browser\n# Modify the script to set headless=False in ParserAppealsAL\n</code></pre>"},{"location":"user-guide/configurable_court_extractor/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\nfrom opal.configurable_court_extractor import extract_court_cases_with_params\n\ndef main():\n    # Extract recent civil appeals\n    results = extract_court_cases_with_params(\n        court='civil',\n        date_period='1m',\n        case_category='Appeal',\n        exclude_closed=True,\n        max_pages=5,\n        output_prefix='civil_appeals_recent'\n    )\n\n    if results and results['status'] == 'success':\n        print(f\"\u2713 Extracted {results['total_cases']} cases\")\n        print(f\"\u2713 Processed {results['pages_processed']} pages\")\n\n        # Show sample cases\n        for case in results['cases'][:3]:\n            print(f\"- {case['case_number']['text']}: {case['case_title']}\")\n    else:\n        print(\"\u274c Extraction failed\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This extractor provides a powerful interface for accessing Alabama Appeals Court data with flexible search capabilities and robust error handling.</p>"},{"location":"user-guide/output-examples/","title":"Output Examples","text":"<p>This page shows you exactly what data OPAL produces when scraping different sources. Understanding the output format helps you plan how to use the data.</p>"},{"location":"user-guide/output-examples/#output-file-naming","title":"Output File Naming","text":"<p>OPAL automatically names output files with timestamps: - Format: <code>YYYY-MM-DD_ParserName.json</code> - Example: <code>2024-01-15_Parser1819.json</code> - CSV files (court data): <code>YYYY-MM-DD_HH-MM-SS_court_cases_[court_type].csv</code></p>"},{"location":"user-guide/output-examples/#news-article-output","title":"News Article Output","text":""},{"location":"user-guide/output-examples/#1819-news-example","title":"1819 News Example","text":"<p>When you run: <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item --max_pages 2\n</code></pre></p> <p>You get a JSON file like this:</p> <pre><code>{\n  \"articles\": [\n    {\n      \"title\": \"Alabama lawmakers consider education reform bill\",\n      \"author\": \"Jane Smith\",\n      \"date\": \"January 15, 2024\",\n      \"line_count\": 45,\n      \"content\": \"Full article text appears here...\\n\\nThe article continues with multiple paragraphs...\\n\\nAll the content from the webpage is captured.\"\n    },\n    {\n      \"title\": \"Local community rallies to support food bank\",\n      \"author\": \"John Doe\", \n      \"date\": \"January 14, 2024\",\n      \"line_count\": 32,\n      \"content\": \"The complete article text...\\n\\nEvery paragraph is preserved...\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"https://1819news.com/\",\n    \"parser\": \"Parser1819\",\n    \"total_articles\": 2,\n    \"scrape_date\": \"2024-01-15T10:30:45\"\n  }\n}\n</code></pre>"},{"location":"user-guide/output-examples/#alabama-daily-news-example","title":"Alabama Daily News Example","text":"<p>When you run: <pre><code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /articles --max_pages 1\n</code></pre></p> <p>Output structure is identical, but content comes from Alabama Daily News:</p> <pre><code>{\n  \"articles\": [\n    {\n      \"title\": \"Birmingham announces new business development initiative\",\n      \"author\": \"Sarah Johnson\",\n      \"date\": \"January 15, 2024\", \n      \"line_count\": 38,\n      \"content\": \"Birmingham city officials announced today...\\n\\nThe full article text appears here...\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"https://www.aldailynews.com/\",\n    \"parser\": \"ParserDailyNews\",\n    \"total_articles\": 1,\n    \"scrape_date\": \"2024-01-15T11:15:22\"\n  }\n}\n</code></pre>"},{"location":"user-guide/output-examples/#understanding-news-output-fields","title":"Understanding News Output Fields","text":"Field Description Example <code>title</code> Article headline \"Alabama lawmakers consider...\" <code>author</code> Article author \"Jane Smith\" <code>date</code> Publication date \"January 15, 2024\" <code>line_count</code> Number of lines in content 45 <code>content</code> Full article text with line breaks \"Full article text...\""},{"location":"user-guide/output-examples/#court-case-output","title":"Court Case Output","text":""},{"location":"user-guide/output-examples/#json-format","title":"JSON Format","text":"<p>When you run: <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre></p> <p>You get both JSON and CSV files. Here's the JSON structure:</p> <pre><code>{\n  \"status\": \"success\",\n  \"court\": \"civil\",\n  \"extraction_time\": \"2024-01-15 14:30:22\",\n  \"total_cases\": 150,\n  \"pages_processed\": 5,\n  \"cases\": [\n    {\n      \"court\": \"Court of Civil Appeals\",\n      \"case_number\": {\n        \"text\": \"CL-2024-0001\",\n        \"link\": \"https://publicportal.alappeals.gov/portal/home/case/caseid/CL-2024-0001\"\n      },\n      \"case_title\": \"Smith v. Jones Construction Company, LLC\",\n      \"classification\": \"Appeal\",\n      \"filed_date\": \"01/10/2024\",\n      \"status\": \"Pending\"\n    },\n    {\n      \"court\": \"Court of Civil Appeals\",\n      \"case_number\": {\n        \"text\": \"CL-2024-0002\", \n        \"link\": \"https://publicportal.alappeals.gov/portal/home/case/caseid/CL-2024-0002\"\n      },\n      \"case_title\": \"Johnson Family Trust v. State of Alabama Department of Revenue\",\n      \"classification\": \"Petition\",\n      \"filed_date\": \"01/11/2024\",\n      \"status\": \"Active\"\n    }\n  ],\n  \"search_parameters\": {\n    \"court_type\": \"civil\",\n    \"date_range\": \"last_30_days\",\n    \"exclude_closed\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/output-examples/#csv-format","title":"CSV Format","text":"<p>The same data in CSV format (easier for Excel):</p> <pre><code>Court,Case Number,Case Title,Classification,Filed Date,Status,Case Link\nCourt of Civil Appeals,CL-2024-0001,\"Smith v. Jones Construction Company, LLC\",Appeal,01/10/2024,Pending,https://publicportal.alappeals.gov/portal/home/case/caseid/CL-2024-0001\nCourt of Civil Appeals,CL-2024-0002,\"Johnson Family Trust v. State of Alabama Department of Revenue\",Petition,01/11/2024,Active,https://publicportal.alappeals.gov/portal/home/case/caseid/CL-2024-0002\n</code></pre>"},{"location":"user-guide/output-examples/#understanding-court-output-fields","title":"Understanding Court Output Fields","text":"Field Description Example <code>court</code> Which court \"Court of Civil Appeals\" <code>case_number</code> Case identifier with link {\"text\": \"CL-2024-0001\", \"link\": \"...\"} <code>case_title</code> Full case name \"Smith v. Jones Construction...\" <code>classification</code> Type of case \"Appeal\", \"Petition\", \"Writ\" <code>filed_date</code> Date case was filed \"01/10/2024\" <code>status</code> Current case status \"Active\", \"Pending\", \"Closed\""},{"location":"user-guide/output-examples/#working-with-output-files","title":"Working with Output Files","text":""},{"location":"user-guide/output-examples/#opening-json-files","title":"Opening JSON Files","text":"<p>In a Text Editor: - Right-click the file \u2192 Open with \u2192 Notepad (Windows) or TextEdit (Mac) - For better formatting, use Notepad++ or VS Code</p> <p>In a Web Browser: - Drag the JSON file into Chrome or Firefox - Many browsers will format it nicely</p> <p>In Python: <pre><code>import json\n\n# Read the file\nwith open('2024-01-15_Parser1819.json', 'r') as file:\n    data = json.load(file)\n\n# Access the data\nfor article in data['articles']:\n    print(f\"Title: {article['title']}\")\n    print(f\"Author: {article['author']}\")\n    print(f\"Date: {article['date']}\")\n    print(\"---\")\n</code></pre></p>"},{"location":"user-guide/output-examples/#opening-csv-files","title":"Opening CSV Files","text":"<p>In Excel: 1. Double-click the CSV file 2. Excel will open it automatically 3. Columns will be properly separated</p> <p>In Google Sheets: 1. Go to sheets.google.com 2. File \u2192 Import \u2192 Upload 3. Select your CSV file</p>"},{"location":"user-guide/output-examples/#common-output-scenarios","title":"Common Output Scenarios","text":"<p>Scenario 1: No Articles Found <pre><code>{\n  \"articles\": [],\n  \"metadata\": {\n    \"source\": \"https://example.com/\",\n    \"parser\": \"Parser1819\",\n    \"total_articles\": 0,\n    \"scrape_date\": \"2024-01-15T10:30:45\",\n    \"note\": \"No articles found matching the criteria\"\n  }\n}\n</code></pre></p> <p>Scenario 2: Partial Data (Missing Author) <pre><code>{\n  \"articles\": [\n    {\n      \"title\": \"Breaking News Article\",\n      \"author\": \"Unknown\",\n      \"date\": \"January 15, 2024\",\n      \"line_count\": 25,\n      \"content\": \"Article content here...\"\n    }\n  ]\n}\n</code></pre></p> <p>Scenario 3: Error During Scraping <pre><code>{\n  \"status\": \"partial_success\",\n  \"message\": \"Completed 3 of 5 pages before encountering error\",\n  \"cases\": [...],\n  \"error_details\": \"Connection timeout on page 4\"\n}\n</code></pre></p>"},{"location":"user-guide/output-examples/#file-size-expectations","title":"File Size Expectations","text":"<ul> <li>News Articles: </li> <li>~1-3 KB per article</li> <li> <p>100 articles \u2248 200-300 KB</p> </li> <li> <p>Court Cases:</p> </li> <li>~500 bytes per case</li> <li>1000 cases \u2248 500 KB</li> </ul>"},{"location":"user-guide/output-examples/#next-steps","title":"Next Steps","text":"<p>Now that you understand the output format: 1. Try the Quick Start Tutorial to generate your own output 2. Learn about Working with Output Data 3. Explore Common Use Cases for practical applications</p>"},{"location":"user-guide/parser-appeals-al-cli/","title":"ParserAppealsAL CLI Reference","text":"<p>Complete command-line reference for the Alabama Appeals Court parser.</p>"},{"location":"user-guide/parser-appeals-al-cli/#overview","title":"Overview","text":"<p>ParserAppealsAL is the specialized parser for extracting case data from the Alabama Appeals Court Public Portal. It's accessed through the main OPAL CLI using <code>--parser ParserAppealsAL</code>.</p>"},{"location":"user-guide/parser-appeals-al-cli/#quick-start","title":"Quick Start","text":"<pre><code># Basic court case extraction\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser ParserAppealsAL \\\n    --output my_cases.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#command-syntax","title":"Command Syntax","text":"<pre><code>python -m opal --url &lt;COURT_URL&gt; --parser ParserAppealsAL [OPTIONS]\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#required-arguments","title":"Required Arguments","text":""},{"location":"user-guide/parser-appeals-al-cli/#-url-url","title":"<code>--url &lt;URL&gt;</code>","text":"<p>The Alabama Appeals Court portal URL to scrape.</p> <p>Supported URL Types: - Search results pages with case listings - Individual case detail pages - Paginated search results</p> <p>Examples: <pre><code>--url \"https://publicportal.alappeals.gov/portal/search/case/results\"\n--url \"https://publicportal.alappeals.gov/portal/search/case/results?criteria=...\"\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#-parser-parserappealsal","title":"<code>--parser ParserAppealsAL</code>","text":"<p>Specifies to use the ParserAppealsAL court parser.</p> <p>Must be exactly: <code>ParserAppealsAL</code></p>"},{"location":"user-guide/parser-appeals-al-cli/#optional-arguments","title":"Optional Arguments","text":""},{"location":"user-guide/parser-appeals-al-cli/#-output-filename","title":"<code>--output &lt;FILENAME&gt;</code>","text":"<p>Specify the output JSON file path.</p> <p>Default: <code>opal_output.json</code></p> <p>Examples: <pre><code>--output court_cases.json\n--output /path/to/appeals_data.json\n--output \"cases_$(date +%Y%m%d).json\"\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#-max_pages-number","title":"<code>--max_pages &lt;NUMBER&gt;</code>","text":"<p>Limit the number of pages to process from paginated results.</p> <p>Default: <code>5</code> Range: 1 or higher</p> <p>Examples: <pre><code>--max_pages 1      # Process only first page\n--max_pages 10     # Process up to 10 pages\n--max_pages 50     # Process up to 50 pages\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#-log-level-level","title":"<code>--log-level &lt;LEVEL&gt;</code>","text":"<p>Set the logging verbosity level.</p> <p>Default: <code>INFO</code> Options: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code></p> <p>Examples: <pre><code>--log-level DEBUG    # Detailed progress information\n--log-level INFO     # Standard progress updates\n--log-level WARNING  # Only warnings and errors\n--log-level ERROR    # Only error messages\n</code></pre></p>"},{"location":"user-guide/parser-appeals-al-cli/#complete-examples","title":"Complete Examples","text":""},{"location":"user-guide/parser-appeals-al-cli/#basic-extraction","title":"Basic Extraction","text":"<pre><code># Extract court cases with default settings\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser ParserAppealsAL\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#custom-output-file","title":"Custom Output File","text":"<pre><code># Save to specific file\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --output alabama_appeals_2024.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#limited-page-processing","title":"Limited Page Processing","text":"<pre><code># Process only first 3 pages for faster results\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --max_pages 3 \\\n    --output quick_sample.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable detailed logging to monitor progress\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --log-level DEBUG \\\n    --output debug_cases.json\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#production-extraction","title":"Production Extraction","text":"<pre><code># Extract with timestamp in filename\npython -m opal \\\n    --url \"https://publicportal.alappeals.gov/portal/search/case/results\" \\\n    --parser court \\\n    --max_pages 20 \\\n    --output \"court_cases_$(date +%Y%m%d_%H%M%S).json\" \\\n    --log-level INFO\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#output-structure","title":"Output Structure","text":"<p>The parser generates JSON output with this structure:</p> <pre><code>{\n  \"cases\": [\n    {\n      \"court\": \"Alabama Civil Court of Appeals\",\n      \"case_number\": {\n        \"text\": \"CL-2024-0123\",\n        \"link\": \"/portal/case/detail/12345\"\n      },\n      \"case_title\": \"Smith v. Jones Corporation\",\n      \"classification\": \"Appeal\",\n      \"filed_date\": \"01/15/2024\",\n      \"status\": \"Open\"\n    }\n  ],\n  \"metadata\": {\n    \"extraction_date\": \"2024-01-20\",\n    \"extraction_time\": \"14:30:22\", \n    \"total_cases\": 156,\n    \"pages_processed\": 7,\n    \"parser\": \"ParserAppealsAL\",\n    \"source_url\": \"https://publicportal.alappeals.gov/...\"\n  }\n}\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#field-descriptions","title":"Field Descriptions","text":"<p>Case Fields: - <code>court</code>: Which Appeals Court handled the case - <code>case_number.text</code>: The case identifier (e.g., \"CL-2024-0123\") - <code>case_number.link</code>: Relative URL to case details page - <code>case_title</code>: Full case title/name - <code>classification</code>: Type of case (Appeal, Certiorari, etc.) - <code>filed_date</code>: Date case was filed (MM/DD/YYYY format) - <code>status</code>: Current case status (Open, Closed, etc.)</p> <p>Metadata Fields: - <code>extraction_date</code>: Date of extraction (YYYY-MM-DD) - <code>extraction_time</code>: Time of extraction (HH:MM:SS) - <code>total_cases</code>: Number of cases extracted - <code>pages_processed</code>: Number of pages processed - <code>parser</code>: Parser used (always \"ParserAppealsAL\") - <code>source_url</code>: Original URL scraped</p>"},{"location":"user-guide/parser-appeals-al-cli/#working-with-results","title":"Working with Results","text":""},{"location":"user-guide/parser-appeals-al-cli/#view-case-count","title":"View Case Count","text":"<pre><code># Extract and show total cases\npython -m opal --url \"...\" --parser ParserAppealsAL --output cases.json\ncat cases.json | jq '.metadata.total_cases'\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#extract-specific-fields","title":"Extract Specific Fields","text":"<pre><code># Get just case numbers\ncat cases.json | jq '.cases[].case_number.text'\n\n# Get case titles\ncat cases.json | jq '.cases[].case_title'\n\n# Get cases by status\ncat cases.json | jq '.cases[] | select(.status == \"Open\")'\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#convert-to-csv","title":"Convert to CSV","text":"<pre><code># Basic CSV conversion\ncat cases.json | jq -r '.cases[] | [.case_number.text, .case_title, .status, .filed_date] | @csv'\n\n# CSV with headers\necho \"Case Number,Title,Status,Filed Date\" &gt; cases.csv\ncat cases.json | jq -r '.cases[] | [.case_number.text, .case_title, .status, .filed_date] | @csv' &gt;&gt; cases.csv\n</code></pre>"},{"location":"user-guide/parser-appeals-al-cli/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/parser-appeals-al-cli/#processing-speed","title":"Processing Speed","text":"<ul> <li>Typical rate: 1-2 pages per minute</li> <li>Factors: Network speed, page complexity, rate limiting</li> <li>Recommendation: Use <code>--max_pages</code> for initial testing</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#memory-usage","title":"Memory Usage","text":"<ul> <li>Low memory: Processes pages sequentially</li> <li>Scalable: Can handle large result sets</li> <li>Storage: JSON output size varies by case count</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Built-in delays: 3-second delays between requests</li> <li>Respectful: Designed not to overwhelm the server</li> <li>Adjustable: Modify in source code if needed</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/parser-appeals-al-cli/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/parser-appeals-al-cli/#no-cases-found","title":"\"No cases found\"","text":"<p>Cause: URL doesn't contain case listings Solution: Verify URL shows cases in browser first</p>"},{"location":"user-guide/parser-appeals-al-cli/#webdriver-error","title":"\"WebDriver error\"","text":"<p>Cause: Chrome browser not installed or outdated Solution: Install/update Chrome browser</p>"},{"location":"user-guide/parser-appeals-al-cli/#connection-timeout","title":"\"Connection timeout\"","text":"<p>Cause: Network issues or server problems Solution: Check internet connection, try again later</p>"},{"location":"user-guide/parser-appeals-al-cli/#permission-denied-writing-output","title":"\"Permission denied writing output\"","text":"<p>Cause: No write access to output directory Solution: Use different output path or fix permissions</p>"},{"location":"user-guide/parser-appeals-al-cli/#debug-steps","title":"Debug Steps","text":"<ol> <li>Test URL in browser: Verify cases are visible</li> <li>Enable debug logging: Use <code>--log-level DEBUG</code></li> <li>Limit pages: Use <code>--max_pages 1</code> for testing</li> <li>Check output: Verify JSON file is created and valid</li> </ol>"},{"location":"user-guide/parser-appeals-al-cli/#getting-help","title":"Getting Help","text":"<p>If issues persist: 1. Check the Error Handling Guide 2. Review Troubleshooting Documentation 3. Enable debug logging to see detailed error messages</p>"},{"location":"user-guide/parser-appeals-al-cli/#advanced-usage","title":"Advanced Usage","text":"<p>For more sophisticated court case extraction with search parameters, filtering, and automation, see:</p> <ul> <li>Configurable Court Extractor: Advanced search parameters</li> <li>Command Line Tools: Complete CLI reference</li> <li>API Reference: Programmatic usage</li> </ul>"},{"location":"user-guide/parser-appeals-al-cli/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature Basic CLI (<code>--parser ParserAppealsAL</code>) Configurable Extractor Search filters \u274c None \u2705 Date, category, court, case number Output formats JSON only JSON + CSV Multiple courts Single URL only All courts in one command Automation Manual URLs Parameterized searches Complexity Simple Advanced <p>Recommendation: Use basic CLI for simple extractions, Configurable Extractor for advanced needs.</p>"},{"location":"user-guide/parsers/","title":"Available Parsers","text":"<p>OPAL includes several parsers for different Alabama news and government websites.</p>"},{"location":"user-guide/parsers/#news-parsers","title":"News Parsers","text":""},{"location":"user-guide/parsers/#parser1819","title":"Parser1819","text":"<ul> <li>Website: 1819 News</li> <li>Content: Conservative news outlet covering Alabama politics and culture</li> <li>Usage:    <pre><code>python -m opal --url https://1819news.com/ --parser Parser1819 --suffix /news/item\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#parserdailynews","title":"ParserDailyNews","text":"<ul> <li>Website: Alabama Daily News</li> <li>Content: Daily news covering Alabama politics, business, and current events</li> <li>Usage:   <pre><code>python -m opal --url https://www.aldailynews.com/ --parser ParserDailyNews --suffix /news/item\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#government-parsers","title":"Government Parsers","text":""},{"location":"user-guide/parsers/#court-parser-parserappealsal","title":"Court Parser (ParserAppealsAL)","text":"<ul> <li>Website: Alabama Appeals Court Public Portal</li> <li>Content: Court cases, opinions, and legal documents</li> <li>Features:</li> <li>Automated Chrome WebDriver handling</li> <li>JavaScript-rendered content support</li> <li>Case details extraction</li> <li>Usage:   <pre><code>python -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre></li> </ul>"},{"location":"user-guide/parsers/#parser-capabilities","title":"Parser Capabilities","text":"Parser Pagination JavaScript Support Authentication Parser1819 \u2713 \u2717 \u2717 ParserDailyNews \u2713 \u2717 \u2717 ParserAppealsAL \u2713 \u2713 \u2717"},{"location":"user-guide/parsers/#adding-new-parsers","title":"Adding New Parsers","text":"<p>To add support for a new website, see the Creating New Parsers guide.</p>"},{"location":"user-guide/understanding-errors/","title":"Understanding Errors","text":"<p>This guide helps you understand common error messages when using OPAL and provides solutions to fix them. Don't worry - most errors have simple solutions!</p> <p>For developers: See Error Handling for technical implementation details and error handling strategies.</p>"},{"location":"user-guide/understanding-errors/#common-installation-and-setup-errors","title":"Common Installation and Setup Errors","text":""},{"location":"user-guide/understanding-errors/#error-python-is-not-recognized-windows","title":"Error: \"python is not recognized\" (Windows)","text":"<p>What it means: Windows can't find Python because it's not in your system PATH.</p> <p>Example: <pre><code>'python' is not recognized as an internal or external command,\noperable program or batch file.\n</code></pre></p> <p>Solutions: 1. Reinstall Python with PATH:    - Download Python again from python.org    - \u2705 Check \"Add Python to PATH\" during installation    - Complete the installation</p> <ol> <li>Add Python to PATH manually:</li> <li>Find your Python installation (usually <code>C:\\Users\\YourName\\AppData\\Local\\Programs\\Python\\</code>)</li> <li> <p>Add it to your system PATH in Environment Variables</p> </li> <li> <p>Use Python Launcher (Windows):    <pre><code>py -m opal --help\n</code></pre></p> </li> </ol>"},{"location":"user-guide/understanding-errors/#error-no-module-named-opal","title":"Error: \"No module named 'opal'\"","text":"<p>What it means: OPAL isn't installed in your current Python environment.</p> <p>Example: <pre><code>ModuleNotFoundError: No module named 'opal'\n</code></pre></p> <p>Solutions: 1. Make sure virtual environment is activated:    <pre><code># You should see (venv) in your prompt\n# If not, activate it:\nsource venv/bin/activate  # Mac/Linux\nvenv\\Scripts\\activate     # Windows\n</code></pre></p> <ol> <li> <p>Install OPAL:    <pre><code>pip install -e .\n</code></pre></p> </li> <li> <p>Check you're in the right directory:    <pre><code># Make sure you're in the opal_beautifulsoup folder\nls -la  # Should see setup.py or pyproject.toml\n</code></pre></p> </li> </ol>"},{"location":"user-guide/understanding-errors/#error-pip-command-not-found","title":"Error: \"pip: command not found\"","text":"<p>What it means: pip (Python package installer) isn't available.</p> <p>Solutions: 1. Install pip:    <pre><code>python -m ensurepip --upgrade\n</code></pre></p> <ol> <li> <p>Use alternative installation:    <pre><code>python -m pip install -e .\n</code></pre></p> </li> <li> <p>Check Python installation:    <pre><code>python --version\npython -m pip --version\n</code></pre></p> </li> </ol>"},{"location":"user-guide/understanding-errors/#network-and-connection-errors","title":"Network and Connection Errors","text":""},{"location":"user-guide/understanding-errors/#error-connection-timeout-or-failed-to-establish-connection","title":"Error: \"Connection timeout\" or \"Failed to establish connection\"","text":"<p>What it means: OPAL can't reach the website you're trying to scrape.</p> <p>Example: <pre><code>requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='1819news.com', port=443): \nRead timed out. (read timeout=30)\n</code></pre></p> <p>Solutions: 1. Check your internet connection:    - Open the website in your browser    - Try a different website to confirm internet works</p> <ol> <li> <p>Wait and retry:    <pre><code># Wait 5 minutes and try again\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 2\n</code></pre></p> </li> <li> <p>Try a different time:</p> </li> <li>Websites may be slower during peak hours</li> <li> <p>Try early morning or late evening</p> </li> <li> <p>Reduce concurrent requests:</p> </li> <li>OPAL includes built-in rate limiting</li> <li>The delay between requests helps prevent timeouts</li> </ol>"},{"location":"user-guide/understanding-errors/#error-ssl-certificate-verification-failed","title":"Error: \"SSL Certificate verification failed\"","text":"<p>What it means: Your system can't verify the website's security certificate.</p> <p>Example: <pre><code>requests.exceptions.SSLError: HTTPSConnectionPool(host='example.com', port=443): \nMax retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError))\n</code></pre></p> <p>Solutions: 1. Update your system:    - Update Python: <code>pip install --upgrade pip</code>    - Update certificates on Mac: Run \"Install Certificates.command\" in Python folder</p> <ol> <li>Check system date/time:</li> <li>Incorrect system time can cause SSL errors</li> <li>Sync your system clock</li> </ol>"},{"location":"user-guide/understanding-errors/#error-too-many-requests-or-rate-limited","title":"Error: \"Too Many Requests\" or \"Rate Limited\"","text":"<p>What it means: The website is blocking you for making too many requests too quickly.</p> <p>Example: <pre><code>HTTP Error 429: Too Many Requests\n</code></pre></p> <p>Solutions: 1. Wait before retrying:    <pre><code># Wait 10-15 minutes, then try again\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 3\n</code></pre></p> <ol> <li> <p>Reduce the scope:    <pre><code># Use fewer pages\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 2\n</code></pre></p> </li> <li> <p>Spread out your scraping:</p> </li> <li>Don't run multiple scrapers simultaneously</li> <li>Wait between different scraping sessions</li> </ol>"},{"location":"user-guide/understanding-errors/#browser-and-selenium-errors-court-scraping","title":"Browser and Selenium Errors (Court Scraping)","text":""},{"location":"user-guide/understanding-errors/#error-chrome-driver-not-found-or-webdriver-errors","title":"Error: \"Chrome driver not found\" or \"WebDriver errors\"","text":"<p>What it means: OPAL can't find or start the Chrome browser for court scraping.</p> <p>Example: <pre><code>selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH\n</code></pre></p> <p>Solutions: 1. Install Google Chrome:    - Download from google.com/chrome    - Make sure it's the latest version</p> <ol> <li> <p>Let webdriver-manager handle it:    <pre><code># OPAL should automatically download the right ChromeDriver\n# If it doesn't work, try updating:\npip install --upgrade webdriver-manager\n</code></pre></p> </li> <li> <p>Check Chrome version compatibility:    <pre><code># Check your Chrome version\ngoogle-chrome --version  # Linux\n# Or check in Chrome browser: Menu &gt; Help &gt; About Google Chrome\n</code></pre></p> </li> </ol>"},{"location":"user-guide/understanding-errors/#error-element-not-found-or-no-such-element","title":"Error: \"Element not found\" or \"No such element\"","text":"<p>What it means: The court website's structure changed, and OPAL can't find expected elements.</p> <p>Example: <pre><code>selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element\n</code></pre></p> <p>Solutions: 1. Check if the court website is working:    - Visit https://publicportal.alappeals.gov/ in your browser    - Make sure the search results page loads correctly</p> <ol> <li> <p>Try a different URL:    <pre><code># Use the basic court URL\npython -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court --max_pages 2\n</code></pre></p> </li> <li> <p>Website may have changed:</p> </li> <li>The court website occasionally updates its structure</li> <li>This may require updates to OPAL's parser</li> <li>Report the issue if the website works fine in your browser</li> </ol>"},{"location":"user-guide/understanding-errors/#error-chrome-crashed-or-browser-process-ended","title":"Error: \"Chrome crashed\" or \"Browser process ended\"","text":"<p>What it means: The Chrome browser used for court scraping stopped working.</p> <p>Solutions: 1. Restart and try again:    <pre><code># Simply run the command again\npython -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court\n</code></pre></p> <ol> <li>Check available memory:</li> <li>Close other applications to free up RAM</li> <li> <p>Court scraping uses more memory than news scraping</p> </li> <li> <p>Reduce page count:    <pre><code># Process fewer pages at once\npython -m opal --url https://publicportal.alappeals.gov/portal/search/case/results --parser court --max_pages 3\n</code></pre></p> </li> </ol>"},{"location":"user-guide/understanding-errors/#data-and-parsing-errors","title":"Data and Parsing Errors","text":""},{"location":"user-guide/understanding-errors/#error-no-articles-found-or-empty-results","title":"Error: \"No articles found\" or \"Empty results\"","text":"<p>What it means: OPAL ran successfully but didn't find any content to scrape.</p> <p>Example Output: <pre><code>{\n  \"articles\": [],\n  \"metadata\": {\n    \"total_articles\": 0,\n    \"note\": \"No articles found matching the criteria\"\n  }\n}\n</code></pre></p> <p>Solutions: 1. Check the website manually:    - Visit the URL in your browser    - Confirm there are actually articles/cases to scrape</p> <ol> <li> <p>Adjust the suffix parameter:    <pre><code># Try without suffix\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 3\n\n# Or try a different suffix\npython -m opal --url https://1819news.com/ --parser Parser1819 --suffix /article --max_pages 3\n</code></pre></p> </li> <li> <p>Website structure changed:</p> </li> <li>News websites sometimes change their URL patterns</li> <li>The parser may need updates for new website structures</li> </ol>"},{"location":"user-guide/understanding-errors/#error-permission-denied-when-saving-files","title":"Error: \"Permission denied\" when saving files","text":"<p>What it means: OPAL can't save the output file to your chosen location.</p> <p>Example: <pre><code>PermissionError: [Errno 13] Permission denied: 'output.json'\n</code></pre></p> <p>Solutions: 1. Check file permissions:    <pre><code># Make sure you have write permissions in the current directory\nls -la\n</code></pre></p> <ol> <li> <p>Choose a different output location:    <pre><code># Save to your home directory\npython -m opal --url https://1819news.com/ --parser Parser1819 --output ~/my_output.json\n</code></pre></p> </li> <li> <p>Close the file if it's open:</p> </li> <li>Make sure the output file isn't open in Excel or another program</li> </ol>"},{"location":"user-guide/understanding-errors/#virtual-environment-errors","title":"Virtual Environment Errors","text":""},{"location":"user-guide/understanding-errors/#error-virtual-environment-not-activated","title":"Error: \"Virtual environment not activated\"","text":"<p>What it means: You're not using your virtual environment, so packages aren't available.</p> <p>Signs: - You don't see <code>(venv)</code> in your command prompt - Getting \"module not found\" errors - Commands that worked before now fail</p> <p>Solutions: 1. Activate virtual environment:    <pre><code># Mac/Linux\nsource venv/bin/activate\n\n# Windows\nvenv\\Scripts\\activate\n\n# You should see (venv) appear in your prompt\n</code></pre></p> <ol> <li>Recreate virtual environment if needed:    <pre><code># If activation fails, recreate it\npython -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\npip install -r requirements.txt\n</code></pre></li> </ol>"},{"location":"user-guide/understanding-errors/#json-and-output-errors","title":"JSON and Output Errors","text":""},{"location":"user-guide/understanding-errors/#error-invalid-json-when-opening-output-files","title":"Error: \"Invalid JSON\" when opening output files","text":"<p>What it means: The output file is corrupted or incomplete.</p> <p>Solutions: 1. Check if scraping completed successfully:    - Look for \"Scraping complete!\" message    - Check file size (should be &gt; 1KB if successful)</p> <ol> <li> <p>Re-run the scraping:    <pre><code># Try a smaller scope first\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 2\n</code></pre></p> </li> <li> <p>Validate JSON:    <pre><code>import json\n\ntry:\n    with open('your_output.json', 'r') as f:\n        data = json.load(f)\n    print(\"JSON is valid!\")\nexcept json.JSONDecodeError as e:\n    print(f\"JSON error: {e}\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/understanding-errors/#how-to-report-issues","title":"How to Report Issues","text":"<p>If you encounter an error that isn't covered here:</p>"},{"location":"user-guide/understanding-errors/#1-gather-information","title":"1. Gather Information","text":"<ul> <li>Full error message</li> <li>Command you ran</li> <li>Your operating system</li> <li>Python version (<code>python --version</code>)</li> <li>Website you were trying to scrape</li> </ul>"},{"location":"user-guide/understanding-errors/#2-try-basic-troubleshooting","title":"2. Try Basic Troubleshooting","text":"<ul> <li>Update OPAL: <code>pip install --upgrade -e .</code></li> <li>Try with <code>--max_pages 1</code> to isolate the issue</li> <li>Test your internet connection</li> <li>Check if the target website is accessible</li> </ul>"},{"location":"user-guide/understanding-errors/#3-create-a-bug-report","title":"3. Create a Bug Report","text":"<p>Include this information: <pre><code>**Command that failed:**\npython -m opal --url https://example.com --parser Parser1819\n\n**Error message:**\n[Paste the full error here]\n\n**Environment:**\n- OS: Windows 10 / macOS / Linux\n- Python version: 3.9.7\n- OPAL version: [run `pip show opal` if installed]\n\n**Additional context:**\n- Website accessible in browser: Yes/No\n- First time using OPAL: Yes/No\n- Worked before: Yes/No\n</code></pre></p>"},{"location":"user-guide/understanding-errors/#error-prevention-tips","title":"Error Prevention Tips","text":""},{"location":"user-guide/understanding-errors/#1-start-small","title":"1. Start Small","text":"<pre><code># Always test with small runs first\npython -m opal --url https://1819news.com/ --parser Parser1819 --max_pages 2\n</code></pre>"},{"location":"user-guide/understanding-errors/#2-check-your-setup","title":"2. Check Your Setup","text":"<pre><code># Verify your environment before big runs\npython --version\npython -m opal --help\n</code></pre>"},{"location":"user-guide/understanding-errors/#3-monitor-progress","title":"3. Monitor Progress","text":"<ul> <li>Watch the console output for warnings</li> <li>Check file sizes to ensure data is being collected</li> <li>Test with different websites to isolate issues</li> </ul>"},{"location":"user-guide/understanding-errors/#4-keep-backups","title":"4. Keep Backups","text":"<pre><code># Save successful outputs with dates\ncp output.json \"backup_$(date +%Y%m%d).json\"\n</code></pre> <p>Remember: Every developer encounters errors. The key is systematic troubleshooting and learning from each issue. Most OPAL errors have straightforward solutions once you understand what they mean!</p>"},{"location":"user-guide/visual-flow-diagrams/","title":"Visual Flow Diagrams","text":"<p>This page provides visual diagrams to help you understand how OPAL works internally. These flowcharts show the data flow, decision processes, and system architecture.</p>"},{"location":"user-guide/visual-flow-diagrams/#opal-system-overview","title":"OPAL System Overview","text":"<pre><code>graph TD\n    A[User Command] --&gt; B{Parser Type?}\n    B --&gt;|News| C[News Parser Flow]\n    B --&gt;|Court| D[Court Parser Flow]\n\n    C --&gt; E[URL Collection]\n    E --&gt; F[Article Extraction]\n    F --&gt; G[JSON Output]\n\n    D --&gt; H[Court Portal Access]\n    H --&gt; I[Case Data Extraction]\n    I --&gt; J[JSON + CSV Output]\n\n    G --&gt; K[Data Analysis]\n    J --&gt; K\n    K --&gt; L[Insights &amp; Reports]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"user-guide/visual-flow-diagrams/#news-scraping-flow","title":"News Scraping Flow","text":"<pre><code>graph TD\n    A[Website URL] --&gt; B[BaseParser Detection]\n    B --&gt; C{Which Parser?}\n\n    C --&gt;|1819news.com| D[Parser1819]\n    C --&gt;|aldailynews.com| E[ParserDailyNews]\n    C --&gt;|Other| F[Generic Parser]\n\n    D --&gt; G[URL Collection Module]\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H[Paginate Website]\n    H --&gt; I[Extract Article URLs]\n    I --&gt; J[Filter by Suffix]\n    J --&gt; K[Process Each Article]\n\n    K --&gt; L[Parse HTML]\n    L --&gt; M[Extract: Title, Author, Date, Content]\n    M --&gt; N[Validate Data]\n    N --&gt; O[Add to Results]\n\n    O --&gt; P{More Articles?}\n    P --&gt;|Yes| K\n    P --&gt;|No| Q[Generate JSON Output]\n\n    Q --&gt; R[Save to File]\n    R --&gt; S[Display Summary]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#court-scraping-flow","title":"Court Scraping Flow","text":"<pre><code>graph TD\n    A[Court Portal URL] --&gt; B[Launch Chrome Browser]\n    B --&gt; C[Navigate to Search Page]\n    C --&gt; D[Wait for JavaScript Load]\n    D --&gt; E[Discover Court IDs]\n\n    E --&gt; F{Search Method?}\n    F --&gt;|Custom URL| G[Use Provided URL]\n    F --&gt;|Parameters| H[Build Search URL]\n\n    H --&gt; I[CourtSearchBuilder]\n    I --&gt; J[Set Court Type]\n    J --&gt; K[Set Date Range]\n    K --&gt; L[Set Filters]\n    L --&gt; M[Generate URL]\n\n    G --&gt; N[Load First Page]\n    M --&gt; N\n\n    N --&gt; O[Extract Page Count]\n    O --&gt; P[Generate All Page URLs]\n    P --&gt; Q[Process Each Page]\n\n    Q --&gt; R[Parse HTML Table]\n    R --&gt; S[Extract Case Data]\n    S --&gt; T[Validate Fields]\n    T --&gt; U[Add to Results]\n\n    U --&gt; V{More Pages?}\n    V --&gt;|Yes| Q\n    V --&gt;|No| W[Close Browser]\n\n    W --&gt; X[Generate JSON Output]\n    X --&gt; Y[Generate CSV Output]\n    Y --&gt; Z[Display Summary]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#parser-selection-decision-tree","title":"Parser Selection Decision Tree","text":"<pre><code>graph TD\n    A[Start: User Provides URL] --&gt; B{URL Contains?}\n\n    B --&gt;|1819news.com| C[Use Parser1819]\n    B --&gt;|aldailynews.com| D[Use ParserDailyNews]\n    B --&gt;|publicportal.alappeals.gov| E[Use ParserAppealsAL]\n    B --&gt;|Other| F[Error: Unsupported Site]\n\n    C --&gt; G[News Scraping Mode]\n    D --&gt; G\n    E --&gt; H[Court Scraping Mode]\n    F --&gt; I[Show Supported Sites]\n\n    G --&gt; J[Requires: --suffix parameter]\n    H --&gt; K[Requires: Chrome Browser]\n\n    J --&gt; L[Output: JSON with articles]\n    K --&gt; M[Output: JSON + CSV with cases]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#pagination-handling","title":"Pagination Handling","text":""},{"location":"user-guide/visual-flow-diagrams/#news-site-pagination","title":"News Site Pagination","text":"<pre><code>graph TD\n    A[Start: Base URL] --&gt; B[Load Homepage]\n    B --&gt; C[Discover Navigation Pattern]\n    C --&gt; D{Pagination Type?}\n\n    D --&gt;|URL Parameters| E[Extract Page Numbers]\n    D --&gt;|Next/Previous Links| F[Follow Link Pattern]\n    D --&gt;|Load More Button| G[Simulate Clicks]\n\n    E --&gt; H[Generate Page URLs]\n    F --&gt; I[Crawl Sequential Pages]\n    G --&gt; J[Dynamic Loading]\n\n    H --&gt; K[Process All Pages]\n    I --&gt; K\n    J --&gt; K\n\n    K --&gt; L[Extract Article URLs]\n    L --&gt; M[Filter by Suffix]\n    M --&gt; N[Remove Duplicates]\n    N --&gt; O[Return URL List]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#court-portal-pagination","title":"Court Portal Pagination","text":"<pre><code>graph TD\n    A[Search Results URL] --&gt; B[Parse URL Parameters]\n    B --&gt; C[Extract: criteria, page, size]\n    C --&gt; D[Determine Total Pages]\n\n    D --&gt; E{How Many Pages?}\n    E --&gt;|1 Page| F[Process Single Page]\n    E --&gt;|Multiple| G[Generate Page URLs]\n\n    F --&gt; H[Extract Cases]\n    G --&gt; I[For Each Page]\n\n    I --&gt; J[Update Page Parameter]\n    J --&gt; K[Load Page with Selenium]\n    K --&gt; L[Wait for Content]\n    L --&gt; M[Extract Cases]\n    M --&gt; N{More Pages?}\n\n    N --&gt;|Yes| I\n    N --&gt;|No| O[Combine All Results]\n\n    H --&gt; P[Return Cases]\n    O --&gt; P</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#data-transformation-process","title":"Data Transformation Process","text":""},{"location":"user-guide/visual-flow-diagrams/#news-article-transformation","title":"News Article Transformation","text":"<pre><code>graph LR\n    A[Raw HTML] --&gt; B[BeautifulSoup Parse]\n    B --&gt; C[Element Selection]\n    C --&gt; D[Text Extraction]\n    D --&gt; E[Data Cleaning]\n    E --&gt; F[Structure Creation]\n\n    subgraph \"Before\"\n        G[\"&lt;article&gt;&lt;h1&gt;Title&lt;/h1&gt;&lt;p&gt;Content...&lt;/p&gt;&lt;/article&gt;\"]\n    end\n\n    subgraph \"After\"\n        H[\"{\\n  'title': 'Title',\\n  'content': 'Content...',\\n  'author': 'Author',\\n  'date': '2024-01-15'\\n}\"]\n    end\n\n    A -.-&gt; G\n    F -.-&gt; H</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#court-case-transformation","title":"Court Case Transformation","text":"<pre><code>graph LR\n    A[HTML Table] --&gt; B[Row Extraction]\n    B --&gt; C[Cell Processing]\n    C --&gt; D[Link Extraction]\n    D --&gt; E[Date Parsing]\n    E --&gt; F[JSON Structure]\n\n    subgraph \"Before\"\n        G[\"&lt;tr&gt;&lt;td&gt;CL-2024-001&lt;/td&gt;&lt;td&gt;Case Title&lt;/td&gt;&lt;/tr&gt;\"]\n    end\n\n    subgraph \"After\"\n        H[\"{\\n  'case_number': {\\n    'text': 'CL-2024-001',\\n    'link': '/case/detail/123'\\n  },\\n  'case_title': 'Case Title'\\n}\"]\n    end\n\n    A -.-&gt; G\n    F -.-&gt; H</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#error-handling-flow","title":"Error Handling Flow","text":"<pre><code>graph TD\n    A[Operation Start] --&gt; B{Error Occurred?}\n    B --&gt;|No| C[Continue Processing]\n    B --&gt;|Yes| D{Error Type?}\n\n    D --&gt;|Network| E[Retry with Backoff]\n    D --&gt;|Element Not Found| F[Try Alternative Selector]\n    D --&gt;|Browser Crash| G[Restart Browser]\n    D --&gt;|Permission| H[Check File Access]\n    D --&gt;|Rate Limit| I[Wait and Retry]\n\n    E --&gt; J{Retry Successful?}\n    F --&gt; K{Alternative Found?}\n    G --&gt; L[Reinitialize Driver]\n    H --&gt; M[Change Output Location]\n    I --&gt; N[Resume Processing]\n\n    J --&gt;|Yes| C\n    J --&gt;|No| O[Log Error &amp; Continue]\n    K --&gt;|Yes| C\n    K --&gt;|No| O\n    L --&gt; C\n    M --&gt; C\n    N --&gt; C\n\n    C --&gt; P[Complete Successfully]\n    O --&gt; Q[Partial Success]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#integration-architecture","title":"Integration Architecture","text":"<pre><code>graph TD\n    A[CLI Interface] --&gt; B[main.py]\n    B --&gt; C[IntegratedParser]\n    C --&gt; D{URL Analysis}\n\n    D --&gt;|News URL| E[url_catcher_module]\n    D --&gt;|Court URL| F[court_url_paginator]\n\n    E --&gt; G[BaseParser Classes]\n    F --&gt; H[ParserAppealsAL]\n\n    G --&gt; I[Parser1819]\n    G --&gt; J[ParserDailyNews]\n\n    I --&gt; K[Article Processing]\n    J --&gt; K\n    H --&gt; L[Case Processing]\n\n    K --&gt; M[JSON Output]\n    L --&gt; N[JSON + CSV Output]\n\n    M --&gt; O[File System]\n    N --&gt; O\n\n    subgraph \"Configuration\"\n        P[CourtSearchBuilder]\n        Q[Search Parameters]\n        R[URL Building]\n    end\n\n    F --&gt; P\n    P --&gt; Q\n    Q --&gt; R\n    R --&gt; H</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#performance-optimization-flow","title":"Performance Optimization Flow","text":"<pre><code>graph TD\n    A[Start Scraping] --&gt; B[Check Rate Limits]\n    B --&gt; C[Monitor Memory Usage]\n    C --&gt; D[Process in Batches]\n\n    D --&gt; E{Resource Check}\n    E --&gt;|OK| F[Continue Processing]\n    E --&gt;|High Memory| G[Clear Cache]\n    E --&gt;|Rate Limited| H[Implement Delay]\n\n    F --&gt; I[Extract Data]\n    G --&gt; J[Garbage Collection]\n    H --&gt; K[Wait Period]\n\n    J --&gt; F\n    K --&gt; F\n\n    I --&gt; L{More Data?}\n    L --&gt;|Yes| B\n    L --&gt;|No| M[Finalize Output]\n\n    M --&gt; N[Performance Report]\n    N --&gt; O[Complete]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#command-processing-flow","title":"Command Processing Flow","text":"<pre><code>graph TD\n    A[User Command] --&gt; B[Parse Arguments]\n    B --&gt; C[Validate Parameters]\n    C --&gt; D{Validation Result?}\n\n    D --&gt;|Valid| E[Initialize Components]\n    D --&gt;|Invalid| F[Show Error &amp; Help]\n\n    E --&gt; G[Set Up Environment]\n    G --&gt; H[Check Prerequisites]\n    H --&gt; I{Prerequisites Met?}\n\n    I --&gt;|Yes| J[Execute Scraping]\n    I --&gt;|No| K[Show Requirements]\n\n    J --&gt; L[Monitor Progress]\n    L --&gt; M[Handle Errors]\n    M --&gt; N[Generate Output]\n    N --&gt; O[Display Results]\n\n    F --&gt; P[Exit with Error]\n    K --&gt; P\n    O --&gt; Q[Exit Successfully]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#session-management-court-scraping","title":"Session Management (Court Scraping)","text":"<pre><code>graph TD\n    A[Court URL Provided] --&gt; B{URL Type?}\n\n    B --&gt;|Fresh Search| C[Build New Session]\n    B --&gt;|Custom URL| D[Check Session Age]\n\n    C --&gt; E[Navigate to Portal]\n    D --&gt; F{Session Valid?}\n\n    F --&gt;|Yes| G[Use Existing Session]\n    F --&gt;|No| H[Warn User]\n    F --&gt;|Unknown| I[Attempt to Use]\n\n    E --&gt; J[Discover Court IDs]\n    G --&gt; K[Start Extraction]\n    H --&gt; L[Suggest New Search]\n    I --&gt; M[Monitor for Errors]\n\n    J --&gt; N[Build Search Parameters]\n    N --&gt; O[Generate Search URL]\n    O --&gt; K\n\n    K --&gt; P[Extract Data]\n    M --&gt; Q{Session Expired?}\n\n    Q --&gt;|Yes| L\n    Q --&gt;|No| P\n\n    P --&gt; R[Complete Successfully]\n    L --&gt; S[Exit with Message]</code></pre>"},{"location":"user-guide/visual-flow-diagrams/#visual-summary","title":"Visual Summary","text":"<p>These diagrams show how OPAL:</p> <ol> <li>Processes different types of content (news vs court data)</li> <li>Handles complex pagination across different website structures  </li> <li>Makes intelligent parser selections based on URL patterns</li> <li>Transforms raw HTML into structured, usable data</li> <li>Manages errors gracefully with fallback strategies</li> <li>Optimizes performance through batching and rate limiting</li> <li>Handles browser sessions for JavaScript-heavy sites</li> </ol> <p>Understanding these flows helps you: - Debug issues by following the process step-by-step - Optimize your scraping by understanding bottlenecks - Extend OPAL by knowing where to add new functionality - Troubleshoot errors by identifying which component failed</p> <p>For more technical details, see the Developer Guide.</p> <p>For practical usage examples, see Common Use Cases.</p>"},{"location":"user-guide/working-with-output-data/","title":"Working with Output Data","text":"<p>This tutorial shows you how to open, analyze, and work with the data OPAL produces. Whether you're new to data analysis or just getting started with JSON files, this guide will help you unlock the value in your scraped data.</p>"},{"location":"user-guide/working-with-output-data/#opening-and-viewing-data-files","title":"Opening and Viewing Data Files","text":""},{"location":"user-guide/working-with-output-data/#opening-json-files","title":"Opening JSON Files","text":"<p>Method 1: Web Browser (Easiest) 1. Find your OPAL output file (e.g., <code>2024-01-15_Parser1819.json</code>) 2. Drag and drop it into Chrome, Firefox, or Safari 3. The browser will format it nicely for reading</p> <p>Method 2: Text Editor - Windows: Right-click \u2192 Open with \u2192 Notepad - Mac: Right-click \u2192 Open with \u2192 TextEdit - Better option: Use VS Code, Notepad++, or Sublime Text for syntax highlighting</p> <p>Method 3: Online JSON Viewers - Go to jsonviewer.stack.hu or jsonformatter.org - Copy and paste your JSON content - Get a formatted, collapsible view</p>"},{"location":"user-guide/working-with-output-data/#opening-csv-files-court-data","title":"Opening CSV Files (Court Data)","text":"<p>Excel or Google Sheets: 1. Double-click the CSV file 2. It opens automatically with columns properly separated 3. Perfect for sorting, filtering, and creating charts</p> <p>LibreOffice Calc (Free alternative): 1. Open LibreOffice Calc 2. File \u2192 Open \u2192 Select your CSV file 3. Choose appropriate delimiter (usually comma)</p>"},{"location":"user-guide/working-with-output-data/#understanding-json-structure","title":"Understanding JSON Structure","text":""},{"location":"user-guide/working-with-output-data/#news-article-data-structure","title":"News Article Data Structure","text":"<pre><code>{\n  \"articles\": [\n    {\n      \"title\": \"Article headline here\",\n      \"author\": \"Jane Smith\", \n      \"date\": \"January 15, 2024\",\n      \"line_count\": 42,\n      \"content\": \"Full article text...\"\n    }\n  ],\n  \"metadata\": {\n    \"source\": \"https://1819news.com/\",\n    \"parser\": \"Parser1819\",\n    \"total_articles\": 25,\n    \"scrape_date\": \"2024-01-15T10:30:45\"\n  }\n}\n</code></pre> <p>Key Fields Explained: - <code>articles</code>: Array containing all scraped articles - <code>title</code>: Article headline - <code>author</code>: Writer's name (may be \"Unknown\" if not found) - <code>date</code>: Publication date - <code>line_count</code>: Number of lines in the article content - <code>content</code>: Full article text with line breaks preserved - <code>metadata</code>: Information about the scraping process</p>"},{"location":"user-guide/working-with-output-data/#court-case-data-structure","title":"Court Case Data Structure","text":"<pre><code>{\n  \"status\": \"success\",\n  \"total_cases\": 150,\n  \"cases\": [\n    {\n      \"court\": \"Court of Civil Appeals\",\n      \"case_number\": {\n        \"text\": \"CL-2024-0001\",\n        \"link\": \"https://publicportal.alappeals.gov/portal/home/case/caseid/CL-2024-0001\"\n      },\n      \"case_title\": \"Smith v. Jones Construction Company\",\n      \"classification\": \"Appeal\",\n      \"filed_date\": \"01/10/2024\",\n      \"status\": \"Pending\"\n    }\n  ]\n}\n</code></pre> <p>Key Fields Explained: - <code>cases</code>: Array of all court cases found - <code>court</code>: Which appeals court - <code>case_number</code>: Case ID with clickable link to full details - <code>case_title</code>: Full case name (parties involved) - <code>classification</code>: Type of legal proceeding - <code>filed_date</code>: When the case was submitted - <code>status</code>: Current case status</p>"},{"location":"user-guide/working-with-output-data/#basic-data-analysis-with-python","title":"Basic Data Analysis with Python","text":""},{"location":"user-guide/working-with-output-data/#installing-required-packages","title":"Installing Required Packages","text":"<pre><code># Install data analysis packages (in your virtual environment)\npip install pandas matplotlib jupyter\n</code></pre>"},{"location":"user-guide/working-with-output-data/#loading-and-exploring-data","title":"Loading and Exploring Data","text":"<pre><code>import json\nimport pandas as pd\nfrom datetime import datetime\n\n# Load news data\nwith open('2024-01-15_Parser1819.json', 'r') as f:\n    news_data = json.load(f)\n\n# Convert to DataFrame for easier analysis\narticles_df = pd.DataFrame(news_data['articles'])\n\n# Basic exploration\nprint(f\"Total articles: {len(articles_df)}\")\nprint(f\"Date range: {articles_df['date'].min()} to {articles_df['date'].max()}\")\nprint(f\"Authors: {articles_df['author'].nunique()} unique authors\")\n\n# Display first few articles\nprint(\"\\nFirst 3 articles:\")\nfor i, article in articles_df.head(3).iterrows():\n    print(f\"- {article['title']} by {article['author']}\")\n</code></pre>"},{"location":"user-guide/working-with-output-data/#analyzing-news-content","title":"Analyzing News Content","text":"<pre><code>import re\nfrom collections import Counter\n\ndef analyze_news_content(articles_df):\n    \"\"\"Analyze news articles for common topics and trends\"\"\"\n\n    # Combine all article text\n    all_text = ' '.join(articles_df['title'] + ' ' + articles_df['content'])\n\n    # Extract keywords (words 4+ letters, excluding common words)\n    stop_words = {'this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'said', 'would', 'there', 'could', 'what', 'were', 'when'}\n    words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_text.lower())\n    keywords = [word for word in words if word not in stop_words]\n\n    # Count most common topics\n    word_counts = Counter(keywords)\n\n    print(\"=== NEWS CONTENT ANALYSIS ===\")\n    print(f\"Total words analyzed: {len(keywords)}\")\n    print(f\"Unique topics: {len(word_counts)}\")\n\n    print(\"\\nTop 15 topics mentioned:\")\n    for word, count in word_counts.most_common(15):\n        print(f\"  {word}: {count} times\")\n\n    # Analyze by author\n    print(f\"\\nMost prolific authors:\")\n    author_counts = articles_df['author'].value_counts().head(5)\n    for author, count in author_counts.items():\n        print(f\"  {author}: {count} articles\")\n\n    return word_counts, author_counts\n\n# Run the analysis\nkeywords, authors = analyze_news_content(articles_df)\n</code></pre>"},{"location":"user-guide/working-with-output-data/#analyzing-court-data","title":"Analyzing Court Data","text":"<pre><code>def analyze_court_data(json_file):\n    \"\"\"Analyze court case patterns\"\"\"\n\n    # Load court data\n    with open(json_file, 'r') as f:\n        court_data = json.load(f)\n\n    # Convert to DataFrame\n    cases_df = pd.DataFrame(court_data['cases'])\n\n    print(\"=== COURT CASE ANALYSIS ===\")\n    print(f\"Total cases: {len(cases_df)}\")\n\n    # Analyze by court\n    print(\"\\nCases by court:\")\n    court_counts = cases_df['court'].value_counts()\n    for court, count in court_counts.items():\n        print(f\"  {court}: {count} cases\")\n\n    # Analyze by case type\n    print(\"\\nCases by classification:\")\n    classification_counts = cases_df['classification'].value_counts()\n    for classification, count in classification_counts.items():\n        print(f\"  {classification}: {count} cases\")\n\n    # Analyze by status\n    print(\"\\nCases by status:\")\n    status_counts = cases_df['status'].value_counts()\n    for status, count in status_counts.items():\n        print(f\"  {status}: {count} cases\")\n\n    # Date analysis (convert dates to datetime)\n    cases_df['filed_date'] = pd.to_datetime(cases_df['filed_date'], format='%m/%d/%Y', errors='coerce')\n\n    print(f\"\\nDate range: {cases_df['filed_date'].min().strftime('%Y-%m-%d')} to {cases_df['filed_date'].max().strftime('%Y-%m-%d')}\")\n\n    return cases_df\n\n# Analyze court data\ncases_df = analyze_court_data('court_cases.json')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#creating-visualizations","title":"Creating Visualizations","text":""},{"location":"user-guide/working-with-output-data/#news-article-trends","title":"News Article Trends","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_news_charts(articles_df):\n    \"\"\"Create charts from news data\"\"\"\n\n    # Convert dates to datetime\n    articles_df['date'] = pd.to_datetime(articles_df['date'], errors='coerce')\n\n    # Create subplots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('News Article Analysis', fontsize=16)\n\n    # 1. Articles per day\n    daily_counts = articles_df['date'].dt.date.value_counts().sort_index()\n    axes[0, 0].plot(daily_counts.index, daily_counts.values, marker='o')\n    axes[0, 0].set_title('Articles per Day')\n    axes[0, 0].set_xlabel('Date')\n    axes[0, 0].set_ylabel('Number of Articles')\n    axes[0, 0].tick_params(axis='x', rotation=45)\n\n    # 2. Article length distribution\n    axes[0, 1].hist(articles_df['line_count'], bins=20, alpha=0.7)\n    axes[0, 1].set_title('Article Length Distribution')\n    axes[0, 1].set_xlabel('Lines in Article')\n    axes[0, 1].set_ylabel('Frequency')\n\n    # 3. Top authors\n    top_authors = articles_df['author'].value_counts().head(8)\n    axes[1, 0].bar(range(len(top_authors)), top_authors.values)\n    axes[1, 0].set_title('Most Prolific Authors')\n    axes[1, 0].set_xlabel('Author')\n    axes[1, 0].set_ylabel('Number of Articles')\n    axes[1, 0].set_xticks(range(len(top_authors)))\n    axes[1, 0].set_xticklabels(top_authors.index, rotation=45, ha='right')\n\n    # 4. Word cloud of common topics (if wordcloud is installed)\n    try:\n        from wordcloud import WordCloud\n        all_text = ' '.join(articles_df['title'])\n        wordcloud = WordCloud(width=400, height=300, background_color='white').generate(all_text)\n        axes[1, 1].imshow(wordcloud, interpolation='bilinear')\n        axes[1, 1].set_title('Common Topics in Headlines')\n        axes[1, 1].axis('off')\n    except ImportError:\n        # If wordcloud not available, show article count by month\n        monthly_counts = articles_df['date'].dt.to_period('M').value_counts().sort_index()\n        axes[1, 1].bar(range(len(monthly_counts)), monthly_counts.values)\n        axes[1, 1].set_title('Articles per Month')\n        axes[1, 1].set_xlabel('Month')\n        axes[1, 1].set_ylabel('Number of Articles')\n\n    plt.tight_layout()\n    plt.savefig('news_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(\"Charts saved as 'news_analysis.png'\")\n\n# Create charts\ncreate_news_charts(articles_df)\n</code></pre>"},{"location":"user-guide/working-with-output-data/#court-case-visualizations","title":"Court Case Visualizations","text":"<pre><code>def create_court_charts(cases_df):\n    \"\"\"Create charts from court data\"\"\"\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('Court Case Analysis', fontsize=16)\n\n    # 1. Cases by court\n    court_counts = cases_df['court'].value_counts()\n    axes[0, 0].pie(court_counts.values, labels=court_counts.index, autopct='%1.1f%%')\n    axes[0, 0].set_title('Cases by Court')\n\n    # 2. Cases by classification\n    classification_counts = cases_df['classification'].value_counts()\n    axes[0, 1].bar(classification_counts.index, classification_counts.values)\n    axes[0, 1].set_title('Cases by Type')\n    axes[0, 1].set_xlabel('Classification')\n    axes[0, 1].set_ylabel('Number of Cases')\n    axes[0, 1].tick_params(axis='x', rotation=45)\n\n    # 3. Cases over time\n    cases_df['filed_month'] = cases_df['filed_date'].dt.to_period('M')\n    monthly_cases = cases_df['filed_month'].value_counts().sort_index()\n    axes[1, 0].plot(range(len(monthly_cases)), monthly_cases.values, marker='o')\n    axes[1, 0].set_title('Cases Filed Over Time')\n    axes[1, 0].set_xlabel('Month')\n    axes[1, 0].set_ylabel('Number of Cases')\n\n    # 4. Case status\n    status_counts = cases_df['status'].value_counts()\n    axes[1, 1].bar(status_counts.index, status_counts.values)\n    axes[1, 1].set_title('Cases by Status')\n    axes[1, 1].set_xlabel('Status')\n    axes[1, 1].set_ylabel('Number of Cases')\n\n    plt.tight_layout()\n    plt.savefig('court_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(\"Charts saved as 'court_analysis.png'\")\n\n# Create court charts\ncreate_court_charts(cases_df)\n</code></pre>"},{"location":"user-guide/working-with-output-data/#data-export-and-conversion","title":"Data Export and Conversion","text":""},{"location":"user-guide/working-with-output-data/#convert-json-to-csv","title":"Convert JSON to CSV","text":"<pre><code>def json_to_csv(json_file, csv_file):\n    \"\"\"Convert news JSON to CSV format\"\"\"\n\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert articles to DataFrame\n    df = pd.DataFrame(data['articles'])\n\n    # Save to CSV\n    df.to_csv(csv_file, index=False)\n    print(f\"Converted {len(df)} articles to {csv_file}\")\n\n# Convert your news data\njson_to_csv('2024-01-15_Parser1819.json', 'news_articles.csv')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#create-summary-reports","title":"Create Summary Reports","text":"<pre><code>def create_summary_report(articles_df, output_file):\n    \"\"\"Create a summary report of news data\"\"\"\n\n    with open(output_file, 'w') as f:\n        f.write(\"=== NEWS SCRAPING SUMMARY REPORT ===\\n\\n\")\n\n        f.write(f\"Total articles collected: {len(articles_df)}\\n\")\n        f.write(f\"Date range: {articles_df['date'].min()} to {articles_df['date'].max()}\\n\")\n        f.write(f\"Unique authors: {articles_df['author'].nunique()}\\n\")\n        f.write(f\"Average article length: {articles_df['line_count'].mean():.1f} lines\\n\\n\")\n\n        f.write(\"TOP AUTHORS:\\n\")\n        top_authors = articles_df['author'].value_counts().head(5)\n        for author, count in top_authors.items():\n            f.write(f\"  {author}: {count} articles\\n\")\n\n        f.write(\"\\nLONGEST ARTICLES:\\n\")\n        longest = articles_df.nlargest(3, 'line_count')\n        for _, article in longest.iterrows():\n            f.write(f\"  {article['title'][:60]}... ({article['line_count']} lines)\\n\")\n\n        f.write(\"\\nMOST RECENT ARTICLES:\\n\")\n        recent = articles_df.nlargest(5, 'date')\n        for _, article in recent.iterrows():\n            f.write(f\"  {article['date']}: {article['title'][:60]}...\\n\")\n\n    print(f\"Summary report saved to {output_file}\")\n\n# Create summary\ncreate_summary_report(articles_df, 'news_summary.txt')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#advanced-data-analysis","title":"Advanced Data Analysis","text":""},{"location":"user-guide/working-with-output-data/#text-analysis-and-sentiment","title":"Text Analysis and Sentiment","text":"<pre><code># Install required packages first: pip install textblob\nfrom textblob import TextBlob\n\ndef analyze_sentiment(articles_df):\n    \"\"\"Analyze sentiment of news articles\"\"\"\n\n    sentiments = []\n\n    for _, article in articles_df.iterrows():\n        # Analyze title sentiment\n        blob = TextBlob(article['title'])\n        sentiment = blob.sentiment.polarity  # -1 (negative) to 1 (positive)\n        sentiments.append({\n            'title': article['title'][:50] + '...',\n            'sentiment': sentiment,\n            'sentiment_label': 'Positive' if sentiment &gt; 0.1 else 'Negative' if sentiment &lt; -0.1 else 'Neutral'\n        })\n\n    sentiment_df = pd.DataFrame(sentiments)\n\n    print(\"=== SENTIMENT ANALYSIS ===\")\n    sentiment_counts = sentiment_df['sentiment_label'].value_counts()\n    for label, count in sentiment_counts.items():\n        print(f\"{label}: {count} articles ({count/len(sentiment_df)*100:.1f}%)\")\n\n    # Show most positive and negative headlines\n    print(\"\\nMost positive headlines:\")\n    positive = sentiment_df.nlargest(3, 'sentiment')\n    for _, row in positive.iterrows():\n        print(f\"  {row['title']} (score: {row['sentiment']:.2f})\")\n\n    print(\"\\nMost negative headlines:\")\n    negative = sentiment_df.nsmallest(3, 'sentiment')\n    for _, row in negative.iterrows():\n        print(f\"  {row['title']} (score: {row['sentiment']:.2f})\")\n\n    return sentiment_df\n\n# Analyze sentiment\nsentiment_results = analyze_sentiment(articles_df)\n</code></pre>"},{"location":"user-guide/working-with-output-data/#topic-modeling","title":"Topic Modeling","text":"<pre><code># Install required packages: pip install scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\ndef find_topics(articles_df, n_topics=5):\n    \"\"\"Find common topics in articles using clustering\"\"\"\n\n    # Combine title and content for analysis\n    texts = [f\"{row['title']} {row['content'][:200]}\" for _, row in articles_df.iterrows()]\n\n    # Convert text to numerical features\n    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n    text_vectors = vectorizer.fit_transform(texts)\n\n    # Find topics using clustering\n    kmeans = KMeans(n_clusters=n_topics, random_state=42)\n    clusters = kmeans.fit_predict(text_vectors)\n\n    # Get top words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n\n    print(\"=== TOPIC ANALYSIS ===\")\n    for i in range(n_topics):\n        top_words_idx = kmeans.cluster_centers_[i].argsort()[-10:][::-1]\n        top_words = [feature_names[idx] for idx in top_words_idx]\n        print(f\"Topic {i+1}: {', '.join(top_words)}\")\n\n        # Show example articles from this topic\n        topic_articles = articles_df[clusters == i]\n        print(f\"  Example articles ({len(topic_articles)} total):\")\n        for _, article in topic_articles.head(2).iterrows():\n            print(f\"    - {article['title'][:60]}...\")\n        print()\n\n# Find topics\nfind_topics(articles_df)\n</code></pre>"},{"location":"user-guide/working-with-output-data/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":""},{"location":"user-guide/working-with-output-data/#combining-news-sources","title":"Combining News Sources","text":"<pre><code>def combine_news_sources(file1, file2, output_file):\n    \"\"\"Combine data from multiple news sources\"\"\"\n\n    # Load both files\n    with open(file1, 'r') as f:\n        data1 = json.load(f)\n    with open(file2, 'r') as f:\n        data2 = json.load(f)\n\n    # Combine articles\n    all_articles = data1['articles'] + data2['articles']\n\n    # Create combined dataset\n    combined_data = {\n        'articles': all_articles,\n        'metadata': {\n            'combined_from': [file1, file2],\n            'total_articles': len(all_articles),\n            'source1_count': len(data1['articles']),\n            'source2_count': len(data2['articles']),\n            'combined_date': datetime.now().isoformat()\n        }\n    }\n\n    # Save combined data\n    with open(output_file, 'w') as f:\n        json.dump(combined_data, f, indent=2)\n\n    print(f\"Combined {len(all_articles)} articles from 2 sources\")\n    print(f\"Source 1: {len(data1['articles'])} articles\")\n    print(f\"Source 2: {len(data2['articles'])} articles\")\n    print(f\"Saved to: {output_file}\")\n\n# Combine multiple sources\ncombine_news_sources('1819_news.json', 'daily_news.json', 'combined_news.json')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#cross-source-analysis","title":"Cross-Source Analysis","text":"<pre><code>def compare_news_sources(file1, file2, source1_name, source2_name):\n    \"\"\"Compare coverage between two news sources\"\"\"\n\n    # Load and prepare data\n    with open(file1, 'r') as f:\n        data1 = json.load(f)\n    with open(file2, 'r') as f:\n        data2 = json.load(f)\n\n    df1 = pd.DataFrame(data1['articles'])\n    df2 = pd.DataFrame(data2['articles'])\n\n    print(f\"=== COMPARING {source1_name.upper()} vs {source2_name.upper()} ===\")\n\n    # Basic stats\n    print(f\"{source1_name}: {len(df1)} articles\")\n    print(f\"{source2_name}: {len(df2)} articles\")\n\n    # Average article length\n    print(f\"\\nAverage article length:\")\n    print(f\"  {source1_name}: {df1['line_count'].mean():.1f} lines\")\n    print(f\"  {source2_name}: {df2['line_count'].mean():.1f} lines\")\n\n    # Most active authors\n    print(f\"\\nMost active authors:\")\n    print(f\"  {source1_name}: {df1['author'].value_counts().index[0]} ({df1['author'].value_counts().iloc[0]} articles)\")\n    print(f\"  {source2_name}: {df2['author'].value_counts().index[0]} ({df2['author'].value_counts().iloc[0]} articles)\")\n\n    # Find common topics\n    def get_top_words(df, n=10):\n        all_text = ' '.join(df['title'] + ' ' + df['content'])\n        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', all_text.lower())\n        return Counter(words).most_common(n)\n\n    words1 = dict(get_top_words(df1))\n    words2 = dict(get_top_words(df2))\n\n    common_topics = set(words1.keys()) &amp; set(words2.keys())\n    print(f\"\\nCommon topics covered: {len(common_topics)}\")\n    for topic in sorted(common_topics)[:5]:\n        print(f\"  {topic}: {source1_name}({words1[topic]}) vs {source2_name}({words2[topic]})\")\n\n# Compare sources\ncompare_news_sources('1819_news.json', 'daily_news.json', '1819 News', 'Alabama Daily News')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#data-integration-with-other-tools","title":"Data Integration with Other Tools","text":""},{"location":"user-guide/working-with-output-data/#export-to-database","title":"Export to Database","text":"<pre><code>import sqlite3\n\ndef save_to_database(json_file, db_file):\n    \"\"\"Save news data to SQLite database\"\"\"\n\n    # Load JSON data\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Create database connection\n    conn = sqlite3.connect(db_file)\n\n    # Create table\n    conn.execute('''\n        CREATE TABLE IF NOT EXISTS articles (\n            id INTEGER PRIMARY KEY,\n            title TEXT,\n            author TEXT,\n            date TEXT,\n            line_count INTEGER,\n            content TEXT,\n            source TEXT\n        )\n    ''')\n\n    # Insert articles\n    for article in data['articles']:\n        conn.execute('''\n            INSERT INTO articles (title, author, date, line_count, content, source)\n            VALUES (?, ?, ?, ?, ?, ?)\n        ''', (\n            article['title'],\n            article['author'],\n            article['date'],\n            article['line_count'],\n            article['content'],\n            data['metadata']['parser']\n        ))\n\n    conn.commit()\n    conn.close()\n\n    print(f\"Saved {len(data['articles'])} articles to database: {db_file}\")\n\n# Save to database\nsave_to_database('news_data.json', 'news.db')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#export-for-excel-analysis","title":"Export for Excel Analysis","text":"<pre><code>def create_excel_report(articles_df, filename):\n    \"\"\"Create an Excel file with multiple sheets for analysis\"\"\"\n\n    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n        # Main data\n        articles_df.to_excel(writer, sheet_name='Articles', index=False)\n\n        # Summary statistics\n        summary_stats = pd.DataFrame({\n            'Metric': ['Total Articles', 'Unique Authors', 'Avg Length', 'Date Range'],\n            'Value': [\n                len(articles_df),\n                articles_df['author'].nunique(),\n                f\"{articles_df['line_count'].mean():.1f} lines\",\n                f\"{articles_df['date'].min()} to {articles_df['date'].max()}\"\n            ]\n        })\n        summary_stats.to_excel(writer, sheet_name='Summary', index=False)\n\n        # Top authors\n        top_authors = articles_df['author'].value_counts().head(10).reset_index()\n        top_authors.columns = ['Author', 'Article Count']\n        top_authors.to_excel(writer, sheet_name='Top Authors', index=False)\n\n        # Articles by date\n        daily_counts = articles_df['date'].value_counts().sort_index().reset_index()\n        daily_counts.columns = ['Date', 'Article Count']\n        daily_counts.to_excel(writer, sheet_name='Daily Counts', index=False)\n\n    print(f\"Excel report saved as: {filename}\")\n\n# Create Excel report\ncreate_excel_report(articles_df, 'news_analysis.xlsx')\n</code></pre>"},{"location":"user-guide/working-with-output-data/#next-steps","title":"Next Steps","text":"<p>Now that you know how to work with OPAL's output data:</p> <ol> <li>Automate Analysis: Create scripts that automatically analyze new data as you collect it</li> <li>Build Dashboards: Use tools like Streamlit or Dash to create interactive data dashboards</li> <li>Set Up Monitoring: Track trends over time by regularly collecting and analyzing data</li> <li>Share Insights: Export charts and summaries to share your findings</li> </ol> <p>For collecting more data, see Common Use Cases.</p> <p>For troubleshooting data issues, see Understanding Errors.</p>"}]}