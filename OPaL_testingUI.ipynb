{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to scrape websites and return json objects.\n",
    "\n",
    "Called OPaL: Opposing Positions and Lingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_news_urls(base_url, suffix:str, max_pages:int=None, output_file:str=None):\n",
    "    \"\"\"\n",
    "    First step in the web scraping process. This function will search for all URLs\n",
    "    that contain the suffix in the base URL. It will continue searching until it\n",
    "    reaches the maximum number of pages or until it reaches the end of the search.\n",
    "    \n",
    "    Includes improved error handling.\n",
    "\n",
    "    Args:\n",
    "    base_url: str: The base URL to start the search\n",
    "    suffix: str: The suffix to search for in the URLs\n",
    "    max_pages: int: The maximum number of pages to search\n",
    "    output_file: str: Optional file path to save the URLs\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of URLs found\n",
    "    \"\"\"\n",
    "    # Base variables to store the URLS, track the page number, and continue the search\n",
    "    news_urls = []\n",
    "    page = 1\n",
    "    continue_search = True\n",
    "    \n",
    "    # Headers for request to avoid being blocked\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "            AppleWebKit/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,\\\n",
    "            image/webp,*/*;q=0.8'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        while continue_search:\n",
    "            # Determine current URL based on page number\n",
    "            if page == 1:\n",
    "                current_url = base_url\n",
    "            else:\n",
    "                current_url = f\"{base_url}/page/{page}\"\n",
    "            \n",
    "            # Make the request with improved error handling\n",
    "            try:\n",
    "                response = requests.get(current_url, headers=headers, timeout=5)\n",
    "                response.raise_for_status()  # Raise exception for bad status codes\n",
    "            except requests.exceptions.RequestException as exception:\n",
    "                if hasattr(exception, 'response') and exception.response is not None:\n",
    "                    status_code = exception.response.status_code\n",
    "                    print(f\"HTTP Error: {status_code} occurred while fetching page {page}\")\n",
    "                else:\n",
    "                    print(f\"Non-Specific Request Error on page {page}: {str(exception)}\")\n",
    "                # Stop the search if we can't get this page\n",
    "                break\n",
    "            \n",
    "            # Parse the page to find URLs\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a')\n",
    "            \n",
    "            found_on_page = 0\n",
    "            \n",
    "            # Collect URLs from this page\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href:\n",
    "                    full_url = urljoin(current_url, href)\n",
    "                    if suffix in full_url:\n",
    "                        if full_url not in news_urls:\n",
    "                            news_urls.append(str(full_url))\n",
    "                            found_on_page += 1\n",
    "            \n",
    "            print(f\"Page {page}: Found {found_on_page} new URLs\")\n",
    "            \n",
    "            # Check conditions to continue or stop the search\n",
    "            if max_pages is not None and page >= max_pages:\n",
    "                print(f\"Reached maximum pages limit: {max_pages}\")\n",
    "                break\n",
    "                \n",
    "            if found_on_page == 0:\n",
    "                print(\"No new URLs found on this page\")\n",
    "                break\n",
    "                \n",
    "            # Increment page and add delay\n",
    "            page += 1\n",
    "            time.sleep(1)  # Polite delay between requests\n",
    "        \n",
    "        # Save URLs to file if output_file is provided\n",
    "        if output_file and news_urls:\n",
    "            try:\n",
    "                with open(output_file, 'w') as f:\n",
    "                    for url in news_urls:\n",
    "                        f.write(f\"{url}\\n\")\n",
    "                print(f\"Saved {len(news_urls)} URLs to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving URLs to file: {e}\")\n",
    "            \n",
    "        return news_urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in URL collection: {e}\")\n",
    "        # Return whatever URLs we've collected so far\n",
    "        return news_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the jupyter file this is a function. \n",
    "# In the python app it is a subclass\n",
    "def article_parser_1819(urls, output_file:str=None):\n",
    "    \"\"\"\n",
    "    Parser specific to 1819 News site format.\n",
    "    This function directly makes HTTP requests to the URLs and parses them,\n",
    "    without using a separate make_request function.\n",
    "    \n",
    "    Args:\n",
    "    urls (list): List of article URLs to parse\n",
    "    output_file: str: Optional file path to save the JSON data\n",
    "    \n",
    "    Returns:\n",
    "    str: JSON string with parsed article data\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "            AppleWebKit/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,\\\n",
    "            image/webp,*/*;q=0.8'\n",
    "    }\n",
    "\n",
    "    # Process each URL directly\n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"Fetching: {url}\")\n",
    "            response = requests.get(url, headers=headers, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the article content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            json_soup = {\n",
    "                'title': '',\n",
    "                'author': '',\n",
    "                'date': '',\n",
    "                'line_count': 0,\n",
    "                'line_content': {}\n",
    "            }\n",
    "\n",
    "            # Extract title\n",
    "            if soup.title:\n",
    "                title = soup.title.string\n",
    "                if title:\n",
    "                    title = title.strip()\n",
    "                    json_soup['title'] = title\n",
    "\n",
    "            # Extract author and date\n",
    "            author_date_div = soup.find('div', class_='author-date')\n",
    "            if author_date_div:\n",
    "                author_link = author_date_div.find('a')\n",
    "                if author_link:\n",
    "                    author_name = author_link.text\n",
    "                    json_soup['author'] = author_name\n",
    "                \n",
    "                # Extract date\n",
    "                text_parts = author_date_div.text.split('|')\n",
    "                if len(text_parts) > 1:\n",
    "                    date = text_parts[1].strip()\n",
    "                    json_soup['date'] = date\n",
    "\n",
    "            # Extract paragraphs and create content array\n",
    "            paragraphs = soup.find_all(['p'])\n",
    "            paragraph_texts = []\n",
    "\n",
    "            for p in paragraphs:\n",
    "                # Get the text and strip whitespace\n",
    "                text = p.get_text().strip()\n",
    "                # Split by line breaks that might be in the HTML\n",
    "                lines = text.split('\\n')\n",
    "                # Add each non-empty line\n",
    "                for line in lines:\n",
    "                    if line.strip():  # Only add non-empty lines\n",
    "                        paragraph_texts.append(line.strip())\n",
    "            \n",
    "            # Create JSON structure of paragraph content\n",
    "            json_soup['line_count'] = len(paragraph_texts)\n",
    "            for i, line in enumerate(paragraph_texts, 1):\n",
    "                json_soup['line_content'][f\"line {i}\"] = line\n",
    "            \n",
    "            all_articles.append(json_soup)\n",
    "            \n",
    "            # Add a small delay between requests\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except requests.exceptions.RequestException as exception:\n",
    "            if hasattr(exception, 'response') and exception.response is not None:\n",
    "                status_code = exception.response.status_code\n",
    "                print(f\"HTTP Error: {status_code} occurred while fetching {url}\")\n",
    "            else:\n",
    "                print(f\"Request Error for {url}: {str(exception)}\")\n",
    "            # Continue with next URL instead of failing completely\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to JSON string\n",
    "    json_data = json.dumps(all_articles, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    # Save JSON to file if output_file is provided\n",
    "    if output_file and all_articles:\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(json_data)\n",
    "            print(f\"Saved parsed data to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON to file: {e}\")\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_output_directory(directory):\n",
    "    \"\"\"Creates the output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: opal_output_20250305_124718\n",
      "Page 1: Found 27 new URLs\n",
      "Reached maximum pages limit: 1\n",
      "Saved 27 URLs to opal_output_20250305_124718/collected_urls.txt\n",
      "Found 27 articles to parse\n",
      "Fetching: https://1819news.com/news/item/gop-leaders-applaud-trumps-powerful-joint-session-speech-americas-golden-age-has-only-just-begun\n",
      "Fetching: https://1819news.com/news/item/hollis-towns-resigns-as-aldotcom-editor\n",
      "Fetching: https://1819news.com/news/item/recount-possible-in-2022-contested-conecuh-county-sheriffs-race-after-in-court-precinct-recount-places-doubt\n",
      "Fetching: https://1819news.com/news/item/col-john-eidsmoe-and-becky-gerritson-myths-and-realities-about-the-controversial-veterans-bill\n",
      "Fetching: https://1819news.com/news/item/elon-musks-doge-cancels-13-more-federal-office-leases-in-alabama\n",
      "Fetching: https://1819news.com/news/item/troy-carico-think-the-thinkable-and-call-for-iveys-resignation\n",
      "Fetching: https://1819news.com/news/item/theodore-man-left-journal-from-his-last-days-during-covid-pandemic-wife-heads-to-d-c-to-highlight-tragedy-of-covid-protocols-hospital-immunity\n",
      "Fetching: https://1819news.com/news/item/baldwin-county-court-to-decide-fate-of-68-camaro-at-center-of-stolen-car-vin-catastrophe\n",
      "Fetching: https://1819news.com/news/item/wilcox-county-employee-accused-of-stealing-437k\n",
      "Fetching: https://1819news.com/news/item/victims-group-opposes-bill-allowing-judges-to-split-30-year-prison-sentences\n",
      "Fetching: https://1819news.com/news/item/alabama-pharmacy-board-overhaul-advances-in-senate\n",
      "Fetching: https://1819news.com/news/item/house-handily-passes-bill-exempting-breastfeeding-mothers-from-jury-duty-after-jefferson-county-controversy\n",
      "Fetching: https://1819news.com/news/item/drama-in-the-house-as-lawmakers-push-back-against-bill-prohibiting-youthful-offender-status-for-those-over-16-charged-with-intentional-murder\n",
      "Fetching: https://1819news.com/news/item/taylor-hicks-march-14-at-song-theater-in-columbiana-al\n",
      "Fetching: https://1819news.com/news/item/tuberville-questions-pentagon-nominee-on-selection-criteria-in-determining-space-command-hq-as-push-for-huntsville-continues\n",
      "Fetching: https://1819news.com/news/item/gerrick-wilkins-trumps-tariffs-and-alabamas-golden-opportunity-for-growth\n",
      "Fetching: https://1819news.com/news/item/mark-tapson-is-christianity-making-a-comeback\n",
      "Fetching: https://1819news.com/news/item/justice-will-sellers-the-biblical-legacy-of-king-james-i-of-england\n",
      "Fetching: https://1819news.com/news/item/monika-hill-alabamas-outdated-licensing-laws-hold-makeup-artists-back\n",
      "Fetching: https://1819news.com/news/item/joey-clark-white-women-and-flip-floppers-or-how-lindsey-graham-is-perfectly-wretched\n",
      "Fetching: https://1819news.com/news/item/phillip-green-an-algop-win-for-veterans\n",
      "Fetching: https://1819news.com/news/item/tuberville-demands-clarification-on-ncaa-womens-athletics-eligibility\n",
      "Fetching: https://1819news.com/news/item/4-takeaways-from-no-1-auburns-83-72-loss-to-no-22-texas-a-m\n",
      "Fetching: https://1819news.com/news/item/no-1-auburn-aims-to-finish-strong\n",
      "Fetching: https://1819news.com/news/item/state-rep-colvin-bill-would-exclude-some-immigrant-students-from-counting-toward-schools-sport-classifications-its-not-an-accurate-reflection-of-our-athletic-pool\n",
      "Fetching: https://1819news.com/news/item/algop-chairman-vows-to-pass-resolution-calling-for-increased-tax-cuts\n",
      "Fetching: https://1819news.com/news/item/aderholt-proposes-bill-making-english-official-language-of-america-reinforcing-our-commitment-to-a-shared-national-identity\n",
      "Saved parsed data to opal_output_20250305_124718/parsed_articles.json\n",
      "Parsing complete. Results saved to opal_output_20250305_124718 directory.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    base_url = 'https://1819news.com/'\n",
    "    suffix = '/news/item'\n",
    "    max_pages = 1  # Adjust this number based on how many pages you want to scrape\n",
    "    \n",
    "    # Create timestamped output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"opal_output_{timestamp}\"\n",
    "    ensure_output_directory(output_dir)\n",
    "    \n",
    "    # Define output files\n",
    "    urls_file = os.path.join(output_dir, \"collected_urls.txt\")\n",
    "    json_file = os.path.join(output_dir, \"parsed_articles.json\")\n",
    "    \n",
    "    # Get the URLs using the improved URL collector and save to file\n",
    "    urls = get_all_news_urls(base_url, suffix, max_pages, urls_file)\n",
    "    print(f\"Found {len(urls)} articles to parse\")\n",
    "    \n",
    "    # Process those URLs directly with the article parser and save to file\n",
    "    result = article_parser_1819(urls, json_file)\n",
    "    print(f\"Parsing complete. Results saved to {output_dir} directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
